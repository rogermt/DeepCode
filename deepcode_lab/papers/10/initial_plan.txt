```yaml
complete_reproduction_plan:
  paper_info:
    title: "Neural Sinkhorn Gradient Flow (NSGF / NSGF++)"
    core_contribution: >
      A neural‑parameterised, time‑varying velocity field that follows the
      Wasserstein gradient flow w.r.t. the entropy‑regularised Sinkhorn
      divergence (NSGF).  NSGF++ adds a two‑phase pipeline: a short
      Sinkhorn‑gradient phase (≤5 Euler steps) followed by a neural straight‑
      line flow refined by a learned transition‑time predictor.

  # ----------------------------------------------------------------------
  # SECTION 1 – File Structure (≈ 900 chars)
  # ----------------------------------------------------------------------
  file_structure: |
    nsgf/
    ├─ configs/
    │   ├─ nsgf_2d.yaml                # low‑dim experiments
    │   ├─ nsgf_mnist.yaml
    │   └─ nsgfpp_cifar.yaml           # full NSGF++ on CIFAR‑10
    ├─ data/
    │   ├─ __init__.py
    │   ├─ synthetic_2d.py             # 8‑gaussians, moons, swiss‑roll, etc.
    │   └─ download_cifar10.py
    ├─ utils/
    │   ├─ sinkhorn.py                 # GeomLoss wrapper → potentials & grads
    │   ├─ ode_solvers.py              # explicit Euler (default), optional RK4
    │   ├─ logger.py                   # tqdm + TensorBoard/WandB hooks
    │   └─ math.py                     # norm helpers, Wasserstein distance
    ├─ models/
    │   ├─ __init__.py
    │   ├─ velocity_mlp.py             # 3‑layer MLP for 2‑D synthetic work
    │   ├─ unet.py                     # UNet backbone (image tasks)
    │   ├─ time_predictor_cnn.py       # 4‑layer CNN → τ∈[0,1]
    │   └─ straight_flow.py             # UNet with time‑embedding for NSF
    ├─ training/
    │   ├─ train_nsgf.py               # Alg. 1 – velocity‑field matching
    │   ├─ train_nsgfpp.py             # Alg. 3 – full NSGF++ training
    │   └─ build_trajectory_pool.py    # generate (x, v̂) pool, optional HDF5
    ├─ inference/
    │   ├─ sample_nsgf.py              # Alg. 2 – single‑phase sampling
    │   ├─ sample_nsgfpp.py            # Alg. 4 – two‑phase NSGF++ sampling
    │   └─ visualize.py                # 2‑D trajectory plots, image grids
    ├─ experiments/
    │   ├─ run_2d.sh
    │   ├─ run_mnist.sh
    │   └─ run_cifar.sh
    ├─ scripts/
    │   └─ evaluate_fid_is.py          # torch‑fid / IS utilities
    ├─ Dockerfile
    ├─ requirements.txt
    └─ README.md                      # created after code is functional
    # Total files ≈ 20 – respects the limit and follows the priority order:
    #   1️⃣ core algorithms, 2️⃣ supporting modules, 3️⃣ training/inference,
    #   4️⃣ config/data, 5️⃣ documentation (README & requirements).

  # ----------------------------------------------------------------------
  # SECTION 2 – Implementation Components (≈ 3450 chars)
  # ----------------------------------------------------------------------
  implementation_components: |
    # ------------------------------------------------------------------
    # 1. Sinkhorn Potential Solver (core algorithm)
    # ------------------------------------------------------------------
    • File: utils/sinkhorn.py
    • Purpose: For two minibatches X (source) and Y (target) compute the
      entropy‑regularised OT dual potentials f_X(·) and g_Y(·) and their
      gradients ∇_x f_X, ∇_x f_{X,Y}.  These gradients give the empirical
      velocity field  v̂(x) = ∇f_{X,Y}(x) – ∇f_{X,X}(x) (Eq. 13).
    • Implementation notes:
        – Use GeomLoss: SamplesLoss("sinkhorn", p=2, blur=β, scaling=ε,
          backend="tensorized").
        – Call with requires_grad=True on X; obtain grads via
          torch.autograd.grad(loss, X, create_graph=True).
        – Return both grads; final velocity = grad_cross – grad_self.
        – “blur” (β) ∈ {0.5,1.0,2.0} per dataset (Sec. E.2).

    # ------------------------------------------------------------------
    # 2. Velocity‑field Neural Network v_θ(x,t)
    # ------------------------------------------------------------------
    • File: models/unet.py  (image tasks) and models/velocity_mlp.py (2‑D).
    • Architecture:
        – UNet depth=2 for CIFAR‑10, depth=1 for MNIST.
        – Input: image tensor x (C×H×W) + scalar time t.
        – Time embedding: sinusoidal (dim =256) → linear projection,
          broadcasted and added to each residual block.
        – Output: velocity of same shape as x (linear layer, no activation).
        – Normalisation: GroupNorm (32 groups), similar to DDPM.
    • Loss: MSE with empirical velocity  v̂ from the Sinkhorn solver
      (Eq. 12).  Optimiser: Adam(lr=1e‑4, β₁=0.9, β₂=0.999).

    # ------------------------------------------------------------------
    # 3. Trajectory Pool Manager
    # ------------------------------------------------------------------
    • File: utils/trajectory_pool.py
    • Stores (x_t, v̂_t) pairs generated by Alg. 1.
    • Modes:
        – In‑memory (deque) for fast prototyping.
        – Disk‑backed HDF5 (optional) for large‑scale CIFAR‑10 pool
          (~45 GB).  Each entry holds a batch of particles and the
          corresponding velocities (float16 to reduce size).
    • API: add(x_batch, v_batch), sample(minibatch_size) → (x, v̂).

    # ------------------------------------------------------------------
    # 4. Velocity‑field Matching Trainer
    # ------------------------------------------------------------------
    • File: training/train_nsgf.py
    • Procedure (Alg. 1):
        1. Build the trajectory pool (≤10 steps for 2‑D, ≤5 for NSGF++).
        2. Loop over epochs:
            – Randomly draw a minibatch from the pool.
            – Forward v_θ(x,t); compute MSE loss vs. v̂.
            – Back‑prop and Adam step.
        3. Early‑stop when validation loss plateaus (patience = 10).
    • Checkpoint: v_θ.pth saved after each epoch.

    # ------------------------------------------------------------------
    # 5. Phase‑transition Time Predictor t_ϕ(x)
    # ------------------------------------------------------------------
    • File: models/time_predictor_cnn.py
    • Architecture:
        – 4‑conv blocks (32‑64‑128‑256 channels, 3×3, stride 1, padding 1),
          each followed by ReLU and a 2×2 AvgPool.
        – Flatten → Linear(256·H/4·W/4 → 1) → Sigmoid.
        – Input: image after NSGF phase; Output: τ∈(0,1).
    • Training (Alg. 3 – second stage):
        – Sample (x₀, y) from prior and target, draw t∼U(0,1).
        – Compute interpolated point x = t·y + (1‑t)·x₀.
        – MSE loss: (t – t_ϕ(x))².
        – Optimiser: Adam(lr=1e‑4), batch = 128, ~40 k updates.

    # ------------------------------------------------------------------
    # 6. Neural Straight Flow (NSF) Network u_δ(t,x)
    # ------------------------------------------------------------------
    • File: models/straight_flow.py (same UNet code as v_θ, re‑used).
    • Purpose: Learn the velocity of the straight‑line interpolation
      between x₀ and y, i.e. u(t,x) ≈ (y – x₀).
    • Training (Alg. 3 – third stage):
        – Same (x₀, y, t) triples as for the time predictor.
        – Target velocity = y – x₀ (independent of t, but conditioned on t).
        – MSE loss between u_δ(t,x) and the analytical straight‑line
          velocity.
        – Optimiser: Adam(lr=1e‑4), batch = 128, ~20 k updates.

    # ------------------------------------------------------------------
    # 7. NSGF Inference Engine (single‑phase)
    # ------------------------------------------------------------------
    • File: inference/sample_nsgf.py
    • Explicit Euler integration:
        for t in {0,…,T‑1}:
            x ← x + η·v_θ(x, t/T)
    • Parameters: η = 0.1 (2‑D) or 0.02 (images), T = 10 (synthetic) or ≤5 (NSGF++).

    # ------------------------------------------------------------------
    # 8. NSGF++ Inference Engine (two‑phase)
    # ------------------------------------------------------------------
    • File: inference/sample_nsgfpp.py
    • Phase‑1: Run the short NSGF (≤5 Euler steps) → intermediate x̂.
    • Phase‑2: τ ← t_ϕ(x̂) (clamped to [0,1]).
    • Phase‑3: Straight‑flow refinement:
        while s < 1:
            x̂ ← x̂ + ω·u_δ(s, x̂);  s ← s + ω
        where ω is chosen such that (1‑τ)/ω ≈ 5–10 steps.
    • Total NFE = (NSGF steps) + (1‑τ)/ω ≈ 60 for CIFAR‑10 (as reported).

    # ------------------------------------------------------------------
    # 9. Evaluation Suite
    # ------------------------------------------------------------------
    • File: scripts/evaluate_fid_is.py
    • Metrics:
        – 2‑Wasserstein distance (computed via large‑blur Sinkhorn loss).
        – FID (torch‑fid, Inception‑v3 features, 2048‑dim).
        – Inception Score (same Inception‑v3).
        – NFEs (count of velocity‑field evaluations).
    • Expected results (paper):
        * 2‑D synthetic: W₂ ≈ 0.02 after 10 steps, matching Table 1.
        * MNIST NSGF: FID ≈ 6.5 (single‑phase), NSGF++ ≈ 5.1.
        * CIFAR‑10 NSGF++: FID ≈ 5.55, IS ≈ 8.86, NFE ≈ 59.

    # ------------------------------------------------------------------
    # 10. Utilities & Logging
    # ------------------------------------------------------------------
    • logger.py – TensorBoard and optional WandB hooks.
    • math.py – functions for L₂ norm, pairwise distance matrices.
    • ode_solvers.py – generic explicit Euler; optional adaptive RK4 for
      research extensions.

  # ----------------------------------------------------------------------
  # SECTION 3 – Validation & Evaluation (≈ 2250 chars)
  # ----------------------------------------------------------------------
  validation_approach: |
    1. **Unit‑level sanity checks**
       - Verify `sinkhorn_potentials` on a tiny 2‑point batch by comparing
         analytical OT potentials (closed‑form for Gaussian) with the
         autograd‑computed gradients.
       - Confirm UNet forward works with time embedding (t = 0.0, 0.5, 1.0) by
         checking output shape and that gradients flow.

    2. **Synthetic 2‑D experiments**
       - Datasets: 8‑Gaussians, moons, swiss‑roll, checkerboard (generated
         on‑the‑fly via `synthetic_2d.py`).
       - Train only the MLP velocity field (Alg. 1) for 20 k steps.
       - Report final Wasserstein‑2 distance (via SciPy OT or large‑blur
         Sinkhorn) and compare to Table 1 values (≤ 0.03 error).
       - Plot particle trajectories (visualize.py) and ensure the flow
         aligns with the theoretical Sinkhorn gradient field.

    3. **Single‑phase NSGF on images**
       - MNIST: train `v_θ` with T = 10 steps, η = 0.02, batch = 256, blur = 0.5.
         Generate 10 k samples, compute FID and IS.
       - CIFAR‑10: same pipeline with T = 10, η = 0.02, batch = 128, blur = 0.8.
         Expected metrics: FID ≈ 9.2, IS ≈ 7.8 (baseline).
       - Verify NFEs = T (explicit Euler) and that generated samples visually
         resemble the target distribution.

    4. **Two‑phase NSGF++**
       - First train `v_θ` with T ≤ 5 (as above) → checkpoint A.
       - Train time‑predictor `t_ϕ` and NSF `u_δ` (Algorithm 3) using the same
         data splits; validate that MSE(t) < 0.01 and MSE(u) < 0.001.
       - Run `sample_nsgfpp.py` to produce 10 k CIFAR‑10 samples.
         Expected outcomes: FID ≈ 5.5 ± 0.3, IS ≈ 8.8 ± 0.2, average NFE ≈ 59.
       - Compare against the paper’s Table 2/3; a deviation > 5 % triggers an
         ablation run (e.g., increase pool size or reduce learning‑rate).

    5. **Ablation studies (optional)**
       - Vary trajectory‑pool size (10 %, 50 %, 100 %) and record impact on FID
         and training stability.
       - Change number of NSGF steps (3, 5, 7) while keeping total NFE constant.
       - Disable the time‑predictor (set τ = 0.5) and observe degradation.

    6. **Reproducibility**
       - Set fixed seeds (torch, numpy, random) to 42.
       - Log all hyper‑parameters in a `config.yaml` and dump them into the
         checkpoint directory.
       - Store training curves (losses, validation metrics) in TensorBoard
         and provide the exported CSVs in `experiments/results/`.

  # ----------------------------------------------------------------------
  # SECTION 4 – Environment & Dependencies (≈ 900 chars)
  # ----------------------------------------------------------------------
  environment_setup: |
    - **OS**: Ubuntu 20.04 LTS (or any modern Linux distribution).
    - **Python**: 3.10 (exact version recorded in `requirements.txt`).
    - **Core libraries**:
        * torch >= 2.0.0 (CUDA 11.8 compatible)
        * torchvision >= 0.15.0
        * geomloss >= 0.2.5   # Sinkhorn OT
        * numpy >= 1.24.0
        * tqdm, pandas, PyYAML
        * tensorboard >= 2.13.0
        * torch‑fid (for FID/IS)
        * h5py (optional pool storage)
    - **Hardware**: Minimum 1× NVIDIA RTX 3090 (24 GB VRAM) or equivalent.
        * For synthetic 2‑D experiments CPU‑only is sufficient.
    - **Installation steps**:
        1. `conda create -n nsgf python=3.10 && conda activate nsgf`
        2. `pip install -r requirements.txt`
        3. Verify GPU: `python -c "import torch; print(torch.cuda.is_available())"`
        4. Optional: build the Docker image (`docker build -t nsgf .`).

  # ----------------------------------------------------------------------
  # SECTION 5 – Implementation Strategy (≈ 1500 chars)
  # ----------------------------------------------------------------------
  implementation_strategy: |
    **Phase 0 – Preparation**
    • Clone the repository skeleton, create the virtual environment,
      and run a quick sanity check (`python utils/sinkhorn.py` on a
      tiny batch) to ensure GeomLoss works.

    **Phase 1 – Core utilities**
    • Implement `sinkhorn.py` and `ode_solvers.py`.  Write unit tests
      under `tests/` that compare analytic gradients for a 2‑point case.
    • Implement `trajectory_pool.py`; first test the in‑memory mode,
      then profile memory usage for the HDF5 variant on CIFAR‑10.

    **Phase 2 – Model primitives**
    • Code `velocity_mlp.py` (2‑D) and `unet.py` (image).  Verify the
      time‑embedding by feeding a constant tensor and checking that the
      embedding changes with t.
    • Add `time_predictor_cnn.py` and `straight_flow.py`; run a forward
      pass on a batch of CIFAR‑10 images to confirm output shape
      (scalar τ and velocity map).

    **Phase 3 – Single‑phase training**
    • Run `build_trajectory_pool.py` with T = 10, η = 0.1 for the 2‑D case.
    • Execute `train_nsgf.py` – monitor loss via TensorBoard; stop when
      validation MSE ≤ 1e‑3.
    • After convergence, call `sample_nsgf.py` and compute the 2‑W distance.
      These steps should reproduce the synthetic results in Table 1.

    **Phase 4 – Full NSGF++ pipeline**
    • Using the checkpoint from Phase 3 (or retrain with T ≤ 5), run
      `train_nsgfpp.py`:
        1. Load the short‑step pool and fine‑tune `v_θ`.
        2. Train the time predictor `t_ϕ` (≈ 40 k updates).
        3. Train the NSF network `u_δ` (≈ 20 k updates).
    • Save three checkpoints (`v_theta.pth`, `phi.pth`, `u_delta.pth`).

    **Phase 5 – Evaluation**
    • Run `sample_nsgfpp.py` to generate 10 k CIFAR‑10 samples.
    • Execute `evaluate_fid_is.py` to obtain FID, IS, and NFEs.
    • Compare against the target numbers; if any metric deviates > 5 %,
      iterate by adjusting:
        – learning‑rate schedule,
        – pool size,
        – number of NSGF steps,
        – blur hyper‑parameter.
    • Produce final plots (trajectory visualisations, image grids)
      and store them in `experiments/results/`.

    **Phase 6 – Packaging**
    • Write `README.md` with step‑by‑step commands, list of hyper‑parameters,
      and expected runtime per experiment.
    • Add a `Dockerfile` that installs all dependencies and sets the
      entrypoint to `/bin/bash`.
    • Final sanity run: `docker build . && docker run --gpus all nsgf ./run_cifar.sh`
      should complete end‑to‑end within ~3 h on a RTX‑3090.

    Throughout the development, maintain a `git` history with clear
    commit messages (e.g., “Add Sinkhorn solver”, “Implement velocity‑field
    UNet”, “Train NSGF++ – time predictor”).  Continuous integration
    (GitHub Actions) can run the 2‑D unit tests on each push to guarantee
    reproducibility.  This systematic, phased approach guarantees that
    every component—from low‑level Sinkhorn gradients to the final two‑phase
    sampler—is built, validated, and documented, enabling an independent
    researcher to reproduce *all* results of the NSGF / NSGF++ paper
    without consulting the original manuscript.