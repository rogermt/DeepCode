```yaml
complete_reproduction_plan:
  paper_info:
    title: "Neural Sinkhorn Gradient Flow"
    core_contribution: "A neural network that learns the timeâ€‘varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence (NSGF) and a twoâ€‘phase variant (NSGF++) that first follows the Sinkhorn flow for â‰¤5 steps and then refines samples with a learned straightâ€‘flow."

  # SECTION 1: File Structure
  file_structure: |
    project_root/
    â”œâ”€ data/
    â”‚   â”œâ”€ synthetic/          # 2â€‘D toy generators (gaussians, moons, checkerboard)
    â”‚   â”œâ”€ mnist/              # torchvision download script
    â”‚   â””â”€ cifar10/            # torchvision download script
    â”œâ”€ src/
    â”‚   â”œâ”€ __init__.py
    â”‚   â”œâ”€ models/
    â”‚   â”‚   â”œâ”€ __init__.py
    â”‚   â”‚   â”œâ”€ unet.py         # timeâ€‘conditioned UNet (vÎ¸ & NSF)
    â”‚   â”‚   â”œâ”€ mlp.py          # small MLP for 2â€‘D experiments
    â”‚   â”‚   â””â”€ time_predictor.py # 4â€‘conv CNN â†’ scalar Ï„
    â”‚   â”œâ”€ utils/
    â”‚   â”‚   â”œâ”€ __init__.py
    â”‚   â”‚   â”œâ”€ sinkhorn.py     # wrapper around GeomLoss, returns potentials & grads
    â”‚   â”‚   â”œâ”€ replay_buffer.py# circular trajectory pool
    â”‚   â”‚   â”œâ”€ ode_solver.py   # Euler / RK4 integrators (torchâ€‘compatible)
    â”‚   â”‚   â””â”€ metrics.py      # FID, IS, 2â€‘Wasserstein
    â”‚   â”œâ”€ training/
    â”‚   â”‚   â”œâ”€ __init__.py
    â”‚   â”‚   â”œâ”€ train_nsgf.py   # Algorithmâ€¯1 â€“ velocityâ€‘field matching
    â”‚   â”‚   â”œâ”€ train_nsgfpp.py # Algorithmâ€¯3 â€“ threeâ€‘stage NSGF++ training
    â”‚   â”‚   â””â”€ train_utils.py  # LR schedulers, logger, checkpointing
    â”‚   â”œâ”€ inference/
    â”‚   â”‚   â”œâ”€ __init__.py
    â”‚   â”‚   â”œâ”€ infer_nsgf.py   # Algorithmâ€¯2 â€“ plain NSGF sampling
    â”‚   â”‚   â””â”€ infer_nsgfpp.py # Algorithmâ€¯4 â€“ NSGF++ sampling
    â”‚   â””â”€ configs/
    â”‚       â”œâ”€ nsgf.yaml
    â”‚       â”œâ”€ nsgfpp_mnist.yaml
    â”‚       â””â”€ nsgfpp_cifar.yaml
    â”œâ”€ scripts/
    â”‚   â”œâ”€ download_data.sh
    â”‚   â”œâ”€ run_nsgf.sh
    â”‚   â”œâ”€ run_nsgfpp.sh
    â”‚   â””â”€ eval_metrics.sh
    â”œâ”€ logs/                    # TensorBoard / CSV logs
    â”œâ”€ checkpoints/             # saved weights: vÎ¸, Ï†, Î´
    â”œâ”€ requirements.txt
    â””â”€ README.md                # written after code is functional

  # SECTION 2: Implementation Components
  implementation_components: |
    ## Core Components and Their Mapping
    - **Sinkhorn Potential Solver (utils/sinkhorn.py)**
      *Purpose*: Compute entropyâ€‘regularized OT potentials f_{Âµ,Î½}, g_{Âµ,Î½} and their gradients.
      *Implementation*: Thin wrapper around `geomloss.SinkhornLoss(eps=Îµ, blur=blur, scaling=scaling)`. 
      Returns tensors f, g and `grad_f = torch.autograd.grad(f.sum(), X, create_graph=True)[0]`.
      *Parameters*: Îµ âˆˆ {0.5,1.0,2.0}; blur = Îµ; scaling âˆˆ {0.80,0.85,0.95} (datasetâ€‘dependent).

    - **Trajectory Pool Manager (utils/replay_buffer.py)**
      *Purpose*: Store (X_t, vÌ‚_t) pairs generated by the Sinkhorn flow for experienceâ€‘replay.
      *Design*: Circular buffer (`max_len=1500` for MNIST, `2500` for CIFARâ€‘10). Supports `add(pair)` and `sample(batch)` returning batches of positions and empirical velocities.

    - **Velocityâ€‘Field Network vÎ¸ (models/unet.py / models/mlp.py)**
      *Purpose*: Approximate the continuous field v_Î¸(x,t) that matches the empirical velocity vÌ‚.
      *Architecture*:
        â€¢ 2â€‘D: 3â€‘layer MLP, 256 hidden units, ReLU.
        â€¢ Image data: UNet with time embedding (sinusoidal) concatenated to channel dimension.
      *Input*: Tensor X (batch, C, H, W) + scalar t (broadcasted), output same shape as X.

    - **Phaseâ€‘Transition Predictor tÏ† (models/time_predictor.py)**
      *Purpose*: Predict optimal switching time Ï„âˆˆ[0,1] from the intermediate NSGF state.
      *Architecture*: 4â€‘conv CNN (32,64,128,256 filters, 3Ã—3, strideâ€¯1, 2Ã—2 AvgPool after each conv) â†’ Flatten â†’ FC â†’ sigmoid output.
      *Loss*: L_Ï† = E[(Ï„â€‘t_Ï†(X_t))Â²] (Eq.â€¯23).

    - **Neural Straight Flow uÎ´ (models/unet.py)**
      *Purpose*: Learn the straightâ€‘line velocity of the linear interpolation flow.
      *Architecture*: Same UNet as vÎ¸ (weights not shared). Trained on pairs (t, X_t) â†’ u = Yâ€‘Xâ‚€ (Eq.â€¯24).

    - **ODE Integration Engine (utils/ode_solver.py)**
      *Functions*: `euler_step(fn, x, dt)`, `rk4_step(fn, x, dt)`, `integrate(fn, x0, t0, t1, dt)`. All operations are torchâ€‘tensor based for GPU speed.

    - **Training Scripts**
      â€¢ `train_nsgf.py` implements Algorithmâ€¯1 â€“ builds trajectory pool, then regresses vÎ¸ via L(Î¸)=E[â€–vÎ¸â€‘vÌ‚â€–Â²].
      â€¢ `train_nsgfpp.py` runs three stages: (i) NSGF training (vÎ¸), (ii) phaseâ€‘predictor training (tÏ†), (iii) NSF training (uÎ´). Checkpoints saved after each stage.
      â€¢ Common utilities (`train_utils.py`) provide cosine LR schedule with linear warmâ€‘up (5â€¯% of total steps), Adam optimizer (Î²â‚=0.9, Î²â‚‚=0.999), gradient clipping (normâ€¯=â€¯1.0), and deterministic seeding.

    - **Inference Scripts**
      â€¢ `infer_nsgf.py` â€“ explicit Euler integration of vÎ¸ for T steps (Tâ‰¤5 for NSGF++).
      â€¢ `infer_nsgfpp.py` â€“ NSGF phase â†’ Ï„ = tÏ†(X_T) â†’ NSF refinement from Ï„ to 1 using the chosen ODE scheme.

    ## Formulas and Losses
    - Sinkhorn divergence: S_Îµ(Âµ,Î½)=W_{2,Îµ}(Âµ,Î½)-Â½[W_{2,Îµ}(Âµ,Âµ)+W_{2,Îµ}(Î½,Î½)] (Eq.â€¯5).
    - First variation: Î´F_Îµ/Î´Âµ = f_{Âµ,Î½} - f_{Âµ,Âµ} (Eq.â€¯9).
    - Velocity field: v_{F,Îµ}^{Âµ_t}=âˆ‡f_{Âµ_t,Î½} - âˆ‡f_{Âµ_t,Âµ_t} (Eq.â€¯10).
    - Empirical velocity (discrete): vÌ‚_t = âˆ‡f_{Âµ_t,Âµ_t}(X_t) - âˆ‡f_{Âµ_t,Î½}(X_t) (Eq.â€¯13).
    - Particle dynamics: dX_t/dt = vÌ‚_t (Eq.â€¯14).
    - L(Î¸) = E_{t,Xâˆ¼Âµ_t}[â€–v_Î¸(X,t) - vÌ‚_tâ€–Â²] (Algâ€¯1 loss).
    - Phaseâ€‘predictor loss: L_Ï† = E_{t,X_t}[ (t - t_Ï†(X_t))Â² ].
    - NSF loss: L_Î´ = E_{t,X_t}[ â€–u_Î´(t,X_t) - (Y-Xâ‚€)â€–Â² ].

    ## Hyperâ€‘parameters (as in the paper)
    - Batch sizes: 256 (2â€‘D), 128 (MNIST/CIFARâ€‘10).
    - NSGF steps T: â‰¤5 for NSGF++, â‰¤10 for baseline NSGF.
    - Euler step Î·: 0.2 (empirically tuned); NSF step Ï‰: 0.3.
    - Adam LR: 1eâ€‘4 for all networks, cosine decay after warmâ€‘up.
    - Weight decay: 0.
    - Gradient clipping: 1.0.
    - Random seed: 0 (torch, numpy, random).

  # SECTION 3: Validation & Evaluation
  validation_approach: |
    ### Quantitative Benchmarks
    1. **2â€‘D Synthetic Datasets** (8â€‘gaussians, moons, checkerboard)
       - Metric: Exact 2â€‘Wasserstein distance (computed via `geomloss.WassersteinDistance`).
       - Success criterion: â‰¤0.285 on 8â€‘gaussians after â‰¤5 NSGF steps (Tableâ€¯1). Plot trajectories and compare against Flowâ€‘Matching, OTâ€‘CFM baselines.
    2. **MNIST (28Ã—28)**
       - Generate 10â€¯k samples with NSGF++ (T=5, Î·=0.2, Ï‰=0.3).
       - Compute FID (â‰¤4.0) and Inception Score (â‰¥9.5) matching Tableâ€¯3.
       - Record Number of Function Evaluations (NFE = T + (1â€‘Ï„)/Ï‰) and confirm â‰¤60 NFEs.
    3. **CIFARâ€‘10 (32Ã—32)**
       - Generate 10â€¯k samples; target FID â‰¤9.0, IS â‰¥9.2 (Tableâ€¯2).
       - NFEs should stay â‰¤60, with average Ï„ predicted by t_Ï† around 0.4â€“0.6.
    4. **Ablation Study**
       - Remove phaseâ€‘predictor (fixed Ï„=0.5) â†’ expect FID increase â‰ˆ10â€¯% and higher NFE.
       - Replace NSF with linear interpolation â†’ degradation in sample quality.
       - Vary NSGF steps T âˆˆ {3,5,7} â†’ plot FID vs. NFE curve (should reproduce Figureâ€¯5â€‘6).
    5. **Speed & Memory**
       - Measure wallâ€‘clock time on a single RTXâ€¯3090 for each dataset (â‰ˆ1â€¯h for 2â€‘D, â‰ˆ30â€¯min for CIFARâ€‘10).
       - Verify GPU memory stays <12â€¯GB with halfâ€‘precision trajectory pool if needed.

    ### Qualitative Checks
    - Visualise particle trajectories for 2â€‘D experiments (Fig.â€¯1â€‘4 style) by plotting X_t at each Euler step.
    - Produce uncurated sample grids for MNIST and CIFARâ€‘10 (Figuresâ€¯8â€‘11) and compare sideâ€‘byâ€‘side with the paper.
    - Plot learned Ï„ predictions versus groundâ€‘truth linear interpolation times to demonstrate predictor accuracy (RÂ²â€¯>â€¯0.95).

    ### Verification Procedure
    - All random seeds fixed; run each experiment three times and report meanâ€¯Â±â€¯std.
    - Use TensorBoard to track losses L(Î¸), L_Ï†, L_Î´, and NFE per epoch.
    - Provide a script (`scripts/eval_metrics.sh`) that automatically loads a checkpoint, generates samples, computes FID/IS/2â€‘W, and prints a summary table.

  # SECTION 4: Environment & Dependencies
  environment_setup: |
    - **Python**: â‰¥3.9 (tested with 3.10)
    - **PyTorch**: â‰¥2.0, CUDA 11.7 (or later)
    - **GeomLoss**: â‰¥0.2.4 (provides SinkhornLoss and WassersteinDistance)
    - **TorchVision**: â‰¥0.15 (MNIST, CIFARâ€‘10 loaders)
    - **TorchMetrics**: for FID/IS computation
    - **Einops**, **tqdm**, **numpy**, **scipy**, **hydraâ€‘core** (optional config handling)
    - **Optional**: `torchdiffeq` for higherâ€‘order ODE solvers
    - **Hardware**: Single GPU with â‰¥10â€¯GB VRAM (RTXâ€¯3090 used in the paper). CPUâ€‘only is possible for 2â€‘D experiments but will be slower.
    - **Installation** (conda):
      ```bash
      conda create -n nsgf python=3.10
      conda activate nsgf
      pip install -r requirements.txt
      ```
    - Ensure deterministic behavior:
      ```python
      torch.manual_seed(0)
      np.random.seed(0)
      random.seed(0)
      torch.backends.cudnn.deterministic = True
      torch.backends.cudnn.benchmark = False
      ```

  # SECTION 5: Implementation Strategy
  implementation_strategy: |
    ## Phaseâ€¯0 â€“ Boilerplate
    1. Clone repository skeleton, create the directory layout from Sectionâ€¯1.
    2. Write `requirements.txt` and run the conda environment script.
    3. Add a deterministic seed module (`utils/seed.py`) imported by every script.

    ## Phaseâ€¯1 â€“ Core Utilities
    - Implement `utils/sinkhorn.py`:
        * Wrap GeomLoss, expose `compute_potentials(src, tgt, eps, blur, scaling)`.
        * Provide `gradient(f, X)` returning âˆ‡_X f.
    - Write unit tests verifying that for two Gaussian clouds the analytical gradient matches the autograd result.
    - Implement `utils/replay_buffer.py` with add/sample methods; test circular eviction logic.

    ## Phaseâ€¯2 â€“ Model Foundations
    - Build the MLP (2â€‘D) and UNet (image) according to the architectures in the paper.
    - Incorporate sinusoidal time embedding: `t_emb = [sin(2^kÏ€t), cos(2^kÏ€t)]` concatenated to input channels.
    - Validate forward passes (output shape, gradient flow) with dummy tensors.

    ## Phaseâ€¯3 â€“ Algorithmâ€¯1 (NSGF) Training
    - Assemble `train_nsgf.py`:
        * Loop: generate trajectory pool (â‰¤5 Sinkhorn steps) â†’ store (X_t, vÌ‚_t).
        * Optimise vÎ¸ using L(Î¸). Use Adam, cosine LR schedule with 5â€¯% warmâ€‘up.
    - Run a short sanity check on a 2â€‘D toy problem (batchâ€¯=â€¯256, 5000 iterations) and ensure loss drops below 1eâ€‘3.
    - Save checkpoint `vtheta.pth`.

    ## Phaseâ€¯4 â€“ Phaseâ€‘Predictor & NSF (NSGF++)
    - Implement `models/time_predictor.py` and `train_nsgfpp.py` stageâ€¯2:
        * Sample (Xâ‚€, Y) pairs, construct linear interpolation X_t, train tÏ† with MSE on t.
    - Implement UNetâ€‘based NSF (stageâ€¯3) using the same loss pattern as vÎ¸ but with target velocity `Y - Xâ‚€`.
    - Verify that tÏ† predicts Ï„ accurately (RÂ²â€¯>â€¯0.95) and NSF reproduces straightâ€‘line velocity (L_Î´â€¯<â€¯1eâ€‘4).

    ## Phaseâ€¯5 â€“ Full NSGF++ Pipeline
    - Write `infer_nsgfpp.py`:
        * Load vÎ¸, tÏ†, uÎ´.
        * Run explicit Euler for T NSGF steps.
        * Query Ï„ = tÏ†(X_T) per batch.
        * Integrate NSF from Ï„ to 1 using Euler (or RK4 if `torchdiffeq` is installed).
    - Generate 10â€¯k samples on MNIST and CIFARâ€‘10; compute FID/IS, compare against target metrics.

    ## Phaseâ€¯6 â€“ Evaluation & Ablations
    - Create `scripts/eval_metrics.sh` to:
        * Compute 2â€‘W distance on 2â€‘D data.
        * Compute FID/IS using TorchMetrics.
        * Log NFE (T + (1â€‘Ï„)/Ï‰) per sample.
    - Run the ablation experiments listed in Sectionâ€¯3, store results in `logs/ablation/`.
    - Produce plots (FID vs. NFE, Ï„ distribution, trajectory visualisations) using Matplotlib/Seaborn.

    ## Phaseâ€¯7 â€“ Documentation & Release
    - Populate `README.md` with a quickâ€‘start guide, commandâ€‘line examples, and expected results.
    - Upload pretrained checkpoints to a public bucket (e.g., HuggingFace) and link them from the README.
    - Tag the repository version (v1.0) and provide a `CITATION.cff` file.

    ## Timeline (approximate)
    - Daysâ€¯1â€‘2: Environment, utilities, unit tests.
    - Daysâ€¯3â€‘5: Model code and simple sanity runs.
    - Daysâ€¯6â€‘9: Full NSGF training on 2â€‘D + MNIST.
    - Daysâ€¯10â€‘13: NSGF++ threeâ€‘stage training on CIFARâ€‘10.
    - Daysâ€¯14â€‘16: Inference, metric evaluation, ablations.
    - Daysâ€¯17â€‘18: Documentation, final testing, repository cleanup.

    By following this staged plan, a developer can reproduce every experiment, figure, and table reported in *Neural Sinkhorn Gradient Flow* without consulting the original manuscript. ğŸš€
```