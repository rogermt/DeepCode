# Extracted from 1.pdf

*Total pages: 17*

---

## Page 1

Neural Sinkhorn Gradient Flow
Huminhao Zhu1,Fangyikang Wang1,Chao Zhang1,Hanbin Zhao1and Hui Qian1
1College of Computer Science and Technology, Zhejiang University
{zhuhuminhao, wangfangyikang, zczju, zhaohanbin, qianhui }@zju.edu.cn
Abstract
Wasserstein Gradient Flows (WGF) with respect to
specific functionals have been widely used in the
machine learning literature. Recently, neural net-
works have been adopted to approximate certain in-
tractable parts of the underlying Wasserstein gradi-
ent flow and result in efficient inference procedures.
In this paper, we introduce the Neural Sinkhorn
Gradient Flow (NSGF) model, which parametrizes
the time-varying velocity field of the Wasserstein
gradient flow w.r.t. the Sinkhorn divergence to the
target distribution starting a given source distribu-
tion. We utilize the velocity field matching train-
ing scheme in NSGF, which only requires samples
from the source and target distribution to compute
an empirical velocity field approximation. Our the-
oretical analyses show that as the sample size in-
creases to infinity, the mean-field limit of the empir-
ical approximation converges to the true underlying
velocity field. To further enhance model efficiency
on high-dimensional tasks, a two-phase NSGF++
model is devised, which first follows the Sinkhorn
flow to approach the image manifold quickly ( ≤5
NFEs) and then refines the samples along a sim-
ple straight flow. Numerical experiments with syn-
thetic and real-world benchmark datasets support
our theoretical results and demonstrate the effec-
tiveness of the proposed methods.
1 Introdution
The Wasserstein Gradient Flow (WGF) with respect to cer-
tain specific functional objective F(denoted as FWasser-
stein gradient flow) is a powerful tool for solving optimiza-
tion problems over the Wasserstein probability space. Since
the seminal work of [Jordan et al. , 1998 ]which shows that
the Fokker-Plank equation is the Wasserstein gradient flow
with respect to the free energy, Wasserstein gradient flow
w.r.t. different functionals have been widely used in various
machine learning tasks such as Bayesian inference [Zhang et
al., 2021a ], reinforcement learning [Zhang et al. , 2021b ], and
mean-field games [Zhang and Katsoulakis, 2023 ].
One recent trend in the Wasserstein gradient flow liter-
ature is to develop efficient generative modeling methods[Gao et al. , 2019; Gao et al. , 2022; Ansari et al. , 2021;
Mokrov et al. , 2021; Alvarez-Melis et al. , 2022; Bunne et
al., 2022; Fan et al. , 2022 ]. In general, these methods mimic
the Wasserstein gradient flow with respect to a specific dis-
tribution metric, driving a source distribution towards a tar-
get distribution. Neural networks are typically employed to
approximate the computationally challenging components of
the underlying Wasserstein gradient flow such as the time-
dependent transport maps. During the training process of
these methods, it is common to require samples from the tar-
get distribution. After the training process, an inference pro-
cedure is often employed to generate new samples from the
target distribution This procedure involves iteratively trans-
porting samples from the source distribution with the assis-
tance of the trained neural network. Based on the chosen
metric, these methods can be categorized into two main types.
Divergences Between Distributions With Exact Same
Supports. The first class of widely used metrics is the f-
divergence, such as the Kullback-Leibler divergence and the
Jensen-Shannon divergence. These divergences are defined
based on the density ratio between two distributions and are
only well-defined when dealing with distributions that have
exactly the same support. Within the scope of f-divergence
Wasserstein gradient flow generative models, neural networks
are commonly utilized to formulate density-ratio estimators,
as demonstrated by [Gao et al. , 2019; Ansari et al. , 2021 ]and
[Heng et al. , 2022 ]. However, as one can only access finite
samples from target distributions in the training process, the
support shift between the sample collections from the com-
pared distributions may cause significant approximation error
in the density-ratio estimators [Choi et al. , 2022 ]. An alter-
native approach, proposed by [Fanet al. , 2022 ], circumvents
these limitations by employing a dual variational formulation
of the f-divergence. In this framework, two networks are em-
ployed to approximate the optimal variational function and
the transport maps. These two components are optimized al-
ternately. It’s imperative to highlight that the non-convex and
non-concave characteristics of their min-max objective can
render the training inherently unstable [Hsieh et al. , 2021 ].
Divergences Between Distributions With Possible Dif-
ferent Supports. Another type of generative Wasserstein
gradient flow model employs divergences that are well-
defined for distributions with possible different supports.
This includes free energy fuctionals [Mokrov et al. , 2021;arXiv:2401.14069v1  [cs.LG]  25 Jan 2024

## Page 2

FM NSGF++
t = 0 t = 1 t = 0.5Figure 1: Tajectories comparison between the Flow matching and the NSGF++ model in CIFAR-10 task. we can see NSGF++ model quickly
recovers the target structure and progressively optimizes the details in subsequent steps
Bunne et al. , 2022 ], the kernel-based metrics such as the
Maximum-Mean/Sobolev Discrepancy [Mroueh et al. , 2019;
Mroueh and Rigotti, 2020 ]and sliced-Wasserstein distance
[Liutkus et al. , 2019; Du et al. , 2023 ]. As these divergences
can be efficiently approximated with samples, neural net-
works are typically used to directly model the transport maps
used in the inference procedure. In Wasserstein gradient flow
methods, input convex neural networks (ICNNs, [Amos et al. ,
2017 ]) are commonly used to approximate the transport map.
However, recently, several works [Korotin et al. , 2021 ]dis-
cuss the poor expressiveness of ICNNs architecture and show
that it would result in poor performance in high-dimension
applications. Besides, the Maximum-Mean/Sobolev discrep-
ancy Wasserstein gradient flow models are usually hard to
train and are easy to trapped in poor local optima in prac-
tice[Arbel et al. , 2019 ], since the kernel-based divergences
are highly sensitive to the parameters of the kernel function
[Liet al. , 2017; Wang et al. , 2018 ].[Liutkus et al. , 2019;
Duet al. , 2023 ]consider sliced-Wasserstein WGF to build
nonparametric generative Models which do not achieve high
generation quality, it is an interesting work on how to com-
bine sliced-Wasserstein WGF and neural network methods.
Contribution. In this paper, we investigate the Wasser-
stein gradient flow with respect to the Sinkhorn divergence,
which is categorized under the second type of divergence
and does not necessitate any kernel functions. We introduce
theNeural Sinkhorn Gradient Flow (NSGF) model, which
parametrizes the time-varying velocity field of the Sinkhorn
Wasserstein gradient flow from a specified source distribu-
tion. The NSGF employs a velocity field matching scheme
that demands only samples from the target distribution to
calculate empirical velocity field approximations. Our the-
oretical analyses show that as the sample size approaches
infinity, the mean-field limit of the empirical approxima-
tion converges to the true velocity field of the Sinkhorn
Wasserstein gradient flow. Given distinct source and tar-
get data samples, our NSGF can be harnessed across a wide
range of machine learning applications, including uncondi-
tional/conditional image generation, style transfer, and audio-
text translation. To further enhance model efficiency on high-
dimensional image datasets, a two-phase NSGF++ model is
devised, which first follows the Sinkhorn flow to approach the
image manifold quickly ( ≤5NFEs) and then refine the sam-
ples along a simple straight flow. A novel phase-transition
time predictor is proposed to transfer between the two phases.We empirically validate NSGF on low-dimensional 2D data
and NSGF++ on benchmark images (MNIST, CIFAR-10).
Our findings indicate that our models can be trained to yield
commendable results in terms of generation cost and sample
quality, surpassing the performance of the neural Wasserstein
gradient flow methods previously tested on CIFAR-10, to the
best of our knowledge.
2 Related Works
Sinkhorn Divergence in Machine Learning. Originally in-
troduced in the domain of optimal transport, the Sinkhorn
divergence emerged as a more computationally tractable al-
ternative to the classical Wasserstein distance [Cuturi, 2013;
Peyr´eet al. , 2017; Feydy et al. , 2019 ]. Since its inception,
Sinkhorn divergence has found applications across a range of
machine learning tasks, including domain adaptation [Alaya
et al. , 2019; Komatsu et al. , 2021 ], Sinkhorn barycenter
[Luise et al. , 2019; Shen et al. , 2020 ]and color transfer [Pai
et al. , 2021 ]. Indeed, it has already been extended to single-
step generative modeling methods, such as the Sinkhorn
GAN and V AE [Genevay et al. , 2018; Deja et al. , 2020;
Patrini et al. , 2020 ]. However, to the best of our knowledge,
it has yet to be employed in developing efficient generative
Wasserstein gradient flow models.
Neural ODE/SDE Based Diffusion Models. Recently,
diffusion models, as a class of Neural ODE/SDE Based
generative methods have achieved unprecedented success,
which also transforms a simple density to the target distri-
bution, iteratively [Song and Ermon, 2019; Ho et al. , 2020;
Song et al. , 2021 ]. Typically, each step of diffusion models
only progresses a little by denoising a simple Gaussian noise,
while each step in WGF models follows the most informative
direction (in a certain sense). Hence, diffusion models usu-
ally have a long inference trajectory. In recent research under-
takings, there has been a growing interest in exploring more
informative steps within diffusion models. Specifically, flow
matching methods [Lipman et al. , 2023; Liu et al. , 2023; Al-
bergo and Vanden-Eijnden, 2023 ]establish correspondence
between the source and target via optimal transport, subse-
quently crafting a probability path by directly linking data
points from both ends. Notably, when the source and target
are both Gaussians, their path is actually a Wasserstein gra-
dient flow. However, this property does not consistently hold
for general data probabilities. Moreover, [Tong et al. , 2023;
Pooladian et al. , 2023 ]consider calculating the minibatch op-

## Page 3

timal transport map to guide data points connecting. Besides,
[Daset al. , 2023 ]consider the shortest forward diffusion path
for the Fisher metric and [Shaul et al. , 2023 ]explore the con-
ditional Gaussian probability path based on the principle of
minimizing the Kinetic Energy. Nonetheless, a commonality
among many of these methods is their reliance on Gaussian
paths for theoretical substantiation, thereby constraining the
broader applicability of these techniques within real-world
generative modeling.
3 Preliminaries
3.1 Notations
We denote x= (x1,···, xd)∈RdandX ⊂Rdas a vec-
tor and a compact ground set in Rd, respectively. For a given
pointx∈ X,∥x∥p:= (P
ixp
i)1
pdenotes the p-norm on eu-
clidean space, and δxstands for the Dirac (unit mass) distri-
bution at point x∈ X.P2(X)denotes the set of probability
measures on Xwith finite second moment and C(X)denotes
the space of continuous functions on X. For a given func-
tionalF(·) :P2(X)→R,δF(µt)
δµ(·) :Rd→Rdenotes its
first variation at µ=µt. Besides, we use ∇and∇ ·()to
denote the gradient and the divergence operator, respectively.
3.2 Wasserstein distance and Sinkhorn divergence
We first introduce the background of Wasserstein distance.
Given two probability measures µ, ν∈ P 2(X), the p-
Wasserstein distance Wp(µ, ν) :P2(X)× P 2(X)→R+
is defined as:
Wp(µ, ν) = inf
π∈Π(µ,ν)Z
X×X∥x−y∥pdπ(x,y)1
p
,(1)
where Π(µ, ν)denotes the set of all probability couplings π
with marginals µandν. TheWpdistance aims to find a cou-
pling πso as to minimize the cost function ∥x−y∥pof mov-
ing a probability mass from µtoν. It has been demonstrated
that the p-Wasserstein distance is a valid metric on P2(X),
and(P2(X),Wp)is referred to as the Wasserstein probabil-
ity space [Villani and others, 2009 ].
Note that directly calculating Wpis computationally ex-
pensive, especially for high dimensional problems [San-
tambrogio, 2015 ]. Consequently, the entropy-regularized
Wasserstein distance [Cuturi, 2013 ]is proposed to approxi-
mate equation equation 1 by regularizing the original problem
with an entropy term:
Definition 1. The entropy-regularized Wasserstein distance
is formally defined as:
Wp,ε(µ, ν) =
inf
π∈Π(µ,ν)"Z
X×X∥x−y∥pdπ(x,y)1
p
+εKL(π|µ⊗ν)#
,
(2)
where ε > 0is a regularization coefficient, µ⊗νdenotes
the product measure, i.e., µ⊗ν(x,y) = µ(x)ν(y), and
KL(π|µ⊗ν)denotes the KL-divergence between πandµ⊗ν.
Generally, the computational cost of Wp,εis much lower
thanWp, and can be efficiently calculated with Sinkhorn al-
gorithms [Cuturi, 2013 ]. Without loss of generality, we fixp= 2 and abbreviate W2,ε:=Wεfor ease of notion in
the whole paper. According to Fenchel-Rockafellar theorem,
the entropy-regularized Wasserstein problem Wεequation 2
has an equivalent dual formulation, which is given as follows
[Peyr´eet al. , 2017 ]:
Wε(µ, ν) = max
f,g∈C(X)⟨µ, f⟩+⟨ν, g⟩
−ε
µ⊗ν,exp1
ε(f⊕g−C)
−1
,(3)
where Cis the cost function in equation 2 and f⊕gis the
tensor sum: (x, y)∈ X27→f(x) +g(y). The maximizers
fµ,νandgµ,νof equation 3 are called the Wε-potentials of
Wε(µ, ν). The following lemma states the optimality condi-
tion for the Wε-potentials:
Lemma 1. (Optimality [Cuturi, 2013 ]) TheWε-potentials
(fµ,ν, gµ,ν)exist and are unique (µ, ν)−a.e.up to an additive
constant (i.e. ∀K∈R,(fµ,ν+K, g µ,ν−K)is optimal).
Moreover,
Wε(µ, ν) =⟨µ, fµ,ν⟩+⟨ν, gµ,ν⟩. (4)
We describe such the method in Appendix A for complete-
ness. Note that, although computationally more efficient than
theWpdistance, the Wεdistance is not a true metric, as there
exists µ∈ P 2(X)such that Wε(µ, µ)̸= 0 when ε̸= 0,
which restricts the applicability of Wε. As a result, the fol-
lowing Sinkhorn divergence Sε(µ, ν) :P2(X)×P2(X)→R
is proposed [Peyr´eet al. , 2017 ]:
Definition 2. Sinkhorn divergence:
Sε(µ, ν) =Wε(µ, ν)−1
2(Wε(µ, µ) +Wε(ν, ν)).(5)
Sε(µ, ν)is nonnegative, bi-convex thus a valid metric
onP2(X)and metricize the convergence in law. Actually
Sε(µ, ν)interpolates the Wasserstein distance ( ϵ→0) and
the Maximum Mean Discrepancy ( ϵ→ ∞ )[Feydy et al. ,
2019 ].
3.3 Gradient flows
Consider an optimization problem over P2(X):
min
µ∈P2(X)F(µ) :=D(µ|µ∗). (6)
where µ∗is the target distribution, Dis the divergence we
choose. We consider now the problem of transporting mass
from an initial distribution µ0to a target distribution µ∗,
by finding a continuous probability path µtstarting from
µ0=µthat converges to µ∗while decreasing F(µt). To
solve this optimization problem, one can consider a de-
scent flow of F(µ)in the Wasserstein space, which trans-
ports any initial distribution µ0towards the target distribu-
tionµ∗. Specifically, the descent flow of F(µ)is described
by the following continuity equation [Ambrosio et al. , 2005;
Villani and others, 2009; Santambrogio, 2017 ]:
∂µt(x)
∂t=−∇ · (µt(x)vt(x)). (7)
where vµt:X → X is a velocity field that defines the direc-
tion of position transportation. To ensure a descent of F(µt)

## Page 4

over time t, the velocity field vµtshould satisfy the following
inequality ( [Ambrosio et al. , 2005 ]):
dF(µt)
dt=Z
⟨∇δF(µt)
δµ,vt⟩dµt≤0. (8)
A straightforward choice of vtisvt=−∇δF(µt)
δµ, which
is actually the steepest descent direction of F(µt). When
we select this vt, we refer to the aforementioned continuous
equation as the Wasserstein gradient flow ofF. We give the
definition of the first variation in the appendix for the sake of
completeness of the article.
4 Methodology
In this section, we first introduce the Sinkhorn Wasser-
stein gradient flow and investigate its convergence properties.
Then, we develop our Neural Sinkhorn Gradient Flow model,
which consists of a velocity field matching training procedure
and a velocity field guided inference procedure. Moreover,
we theoretically show that the mean-field limit of the empir-
ical approximation used in the training procedure converges
to the true velocity field of the Sinkhorn Wasserstein gradient
flow.
4.1 Sinkhorn Wasserstein gradient flow
Based on the definition of the Sinkhorn divergence, we con-
struct our Sinkhorn objective Fε(·) =Sε(·, µ∗), where µ∗
denotes the target distribution. The following theorem gives
the first variation of the Sinkhorn objective.
Theorem 1. (First variation of the Sinkhorn objective [Luise
et al. , 2019 ]) Let ε > 0. Let (fµ,µ∗, gµ,µ∗)be the
Wε-potentials of Wε(µ, µ∗)and(fµ,µ, gµ,µ)be the Wε-
potentials of Wε(µ, µ). The first variation of the Sinkhorn
objective Fεis
δFε
δµ=fµ,µ∗−fµ,µ. (9)
According to Theorem 1, we can construct the Sinkhorn
Wasserstein gradient flow by setting the velocity field vtin
the continuity equation equation 7 as vFεµt=−∇δFε(µt)
δµt=
∇fµt,µt− ∇fµt,µ∗.
Proposition 1. Consider the Sinkhorn Wasserstein gradient
flow described by the following continuity equation:
∂µt(x)
∂t=−∇ · (µt(x)(∇fµt,µt(x)− ∇fµt,µ∗(x))).(10)
The following local descending property of Fεholds:
dFε(µt)
dt=−Z
∥∇fµt,µt(x)− ∇fµt,µ∗(x)∥2dµt,(11)
where the r.h.s. equals 0 if and only if µt=µ∗.
4.2 Velocity-fields Matching
We now present our NSGF method, the core of which lies
in training a neural network to approximate the time-varying
velocity field vFεµtinduced by Sinkhorn Wasserstein gradient
flow. Given a target probability density path µt(x)and it’scorresponding velocity field vSεµt, which generates µt(x), we
define the velocity field matching objective as follows:
min
θEt∼[0,T],x∼µthvθ(x, t)−vSε
µt(x)2i
. (12)
To construct our algorithm, we utilize independently and
identically distributed ( i.i.d) samples denoted as {Yi}n
i=1∈
Rd, which are drawn from an unknown target distribution µ∗
a common practice in the field of generative modeling. Given
the current set of samples {˜Xt
i}n
i=1∼µt, our method cal-
culates the velocity field using the Wε-potentials (Lemma 1)
f˜µt,˜µ∗andf˜µt,˜µtbased on samples. Here, ˜µtand˜µ∗repre-
sent discrete Dirac distributions.
Remark 1. In the discrete case, Wε-potentials equation 1
can be computed by a standard method in [Genevay et al. ,
2016 ]. In practice, we use the efficient implementation of the
Sinkhorn algorithm with GPU acceleration from the Geom-
Loss package [Feydy et al. , 2019 ].
The corresponding finite sample velocity field approxima-
tion can be computed as follows:
ˆvFε
˜µt(˜Xt
i) =∇˜Xt
if˜µt,˜µt(˜Xt
i)− ∇ ˜Xt
if˜µt,˜µ∗(˜Xt
i). (13)
Subsequently, we derive the particle formulation correspond-
ing to the flow formulation equation 10.
d˜Xt
i=ˆvFϵ
˜µt
˜Xt
i
dt, i= 1,2,···n. (14)
In the following proposition, we investigate the mean-field
limit of the particle set {˜Xt
i}i=1,···,M.
Theorem 2. (Mean-field limits.) Suppose the empirical dis-
tribution ˜µ0ofMparticles weakly converges to a distribu-
tionµ0when M→ ∞ . Then, the path of equation equa-
tion 14 starting from ˜µ0weakly converges to a solution of the
following partial differential equation starting from µ0when
M→ ∞ :
∂µt(x)
∂t=−∇ · (µt(x)∇δFε(µt)
δµt). (15)
which is actually the gradient flow of Sinkhorn divergence Fε
in the Wasserstein space.
The following proposition shows that the goal of the ve-
locity field matching objective equation 12 can be regarded as
approximating the steepest local descent direction with neural
networks.
Proposition 2. (Steepest local descent direction.) Consider
the infinitesimal transport T(x) = x+λϕ. The Fr ´echet
derivative under this particular perturbation,
d
dλFε(T#µ)|λ=0= lim
λ→0Fε(T#µ)− Fε(µ)
λ
=Z
X∇fµ,µ∗(x)ϕ(x)dµ−Z
X∇fµ,µ(x)ϕ(x)dµ,(16)
and the steepest local descent direction is ϕ=
∇fµ,µ∗(x)−fµ,µ(x)
∥∇fµ,µ∗(x)−fµ,µ(x)∥.

## Page 5

Algorithm 1: Velocity field matching training
Input : number of time steps T, batch size n, gradient flow
step size η >0, empirical or samplable distribution
µ0andµ∗, neural network parameters θ, optimizer
step size γ >0
/*Build trajectory pool */
while Building do
/*Sample batches of size n i.i.d. from
the datasets */
˜X0
i∼µ0,˜Yi∼µ∗, i= 1,2,···n.
fort= 0,1,···Tdo
calculate f˜µt,˜µt
˜Xt
i
, f˜µt,˜µ∗
˜Xt
i
.
ˆvFϵµt
˜Xt
i
=∇f˜µt,˜µt
˜Xt
i
− ∇f˜µt,˜µ∗
˜Xt
i
.
˜Xt+1
i=˜Xt
i+ηˆvFϵ
t
˜Xt
i
.
store all
˜Xt
i,ˆvFϵ
t
˜Xt
i
pair into the pool , i =
1,2,···n.
/*velocity field matching */
while Not convergence do
from trajectory pool sample pair
˜Xt
i,ˆvFϵ
t
˜Xt
i
.
L(θ) =vθ(˜Xt
i, t)−ˆvFεµt
˜Xt
i2
,
θ←θ−γ∇θL(θ).
Output: θparameterize the time-varying velocity field
Algorithm 2: Inference via velocity field
Input : number of time steps T, inference step size η, time-
varying velocity field vθ, prior samples ˜X0
i∼˜µ0
fort= 0,1,···Tdo
˜Xt+1
i=˜Xt
i+ηvθ
˜Xt
i, t
, i= 1,2,···n.
Output: ˜XT
ias the results.
4.3 Minibatch Sinkhorn Gradient Flow and
Experience Replay
According to Theorem 2, we construct our NSGF method
based on minibatches. We utilize a set of discrete targets,
denoted as {Yi}n
i=1, where nrepresents the batch size, to
construct the Sinkhorn Gradient Flow starting from random
Gaussian noise or other initial distributions. As indicated
by Theorem 2, the mean-field limit converges to the true
Sinkhorn Gradient flow when the batch size approaches ∞.
Note that in practice, we only use a moderate batch size for
computation efficiency, and the experimental results demon-
strate that this works well for practical generative tasks.
Considering the balance between expensive training costs
and training quality, we opted to first build a trajectory pool of
Sinkhorn gradient flow and then sample from it to construct
the velocity field matching algorithm. Our method draws in-
spiration from experience replay, a common technique in re-
inforcement learning, adapting it to enhance our model’s ef-
fectiveness [Mnih et al. , 2013; Silver et al. , 2016 ]. Once we
calculate the time-varying velocity field ˆvs
µt(˜Xt
i), we can pa-
rameterize the velocity field using a straightforward regres-
sion method. The velocity field matching training procedure
is outlined in Algorithm 1.
NSGF( NFE)Unet
…
Phase T ransition
Time Predictor
NSF refine
NN Blocks
Data Samples
Figure 2: NSGF++ framework
Once obtained a feasible velocity field approximation vθ,
one can generate new samples by iteratively employing the
explicit Euler discretization of the Equation equation 14 to
drive the samples to the target. Note that various other numer-
ical schemes, such as the implicit Euler method [Platen and
Bruti-Liberati, 2010 ]and Runge-Kutta methods [Butcher,
1964 ], can be employed. In this study, we opt for the first-
order explicit Euler discretization method [S¨uli and Mayers,
2003 ]due to its simplicity and ease of implementation. We
leave the exploration of higher-order algorithms for future re-
search.
4.4 NSGF++
In this subsection, we propose an approach to enhance the
performance of NSGF on high-dimensional datasets. As a
trajectory pool should be first constructed in the velocity field
matching training procedure of NSGF, the storage and com-
putation costs would greatly hinter the usage of NSGF in
large-scale tasks. To tackle this problem, we propose a two-
phase NSGF++ algorithm, which first follows the Sinkhorn
gradient flow to approach the image manifold quickly and
then refine the samples along a simple straight flow. Specif-
ically, our NSGF++ model consists of three components, (1)
a NSGF model trained on T≤5time steps, (2) a Neural
Straight Flow (NSF) model trained via velocity field match-
ing on a straight flow Xt∼(1−t)P0+tP1, t∈[0,1],
which has also been used in existing FM models, (3) a phase-
transition time predictor to transfer from NSGF to the NSF.
Here, we train the time predictor tϕ:X → [0,1]with the
following regression objective:
L(ϕ) =Et∈U(0,1),Xt∼Pt||t−tϕ(Xt)||2.
Note that the training of the straight NSF model and the time
predictor is simulated free and need no extra storage. As a
result, the training cost of NSFG++ is similar to existing FM
models, since the computation cost of NSF is nearly the same
as FM, and the 5-step NSGF and the time predictor is easy to
train.
In the inference of NSGF++, we first follow the NSGF with
less than 5 NFEs form X0∼P0to obtain ˜Xt, then trans-
fer it with the time predictor tϕ, and obtain our final output
by refining the transferred sample with the NSF model from
tϕ(˜Xt), as shown in Figure 5.

## Page 6

Algorithm2-Wasserstein distance (10 steps) 2-Wasserstein distance (100 steps)
8gaussians 8gaussians-moons moons scurve checkerboard 8gaussians 8gaussians-moons moons scurve checkerboard
NSGF (ours) 0.285 0.144 0.077 0.117 0.252 0.278 0.144 0.067 0.110 0.147
JKO-Flow 0.290 0.177 0.085 0.135 0.269 0.274 0.167 0.085 0.123 0.160
EPT 0.295 0.180 0.082 0.138 0.277 0.289 0.176 0.080 0.118 0.163
OT-CFM 0.289 0.173 0.088 0.149 0.253 0.269 0.165 0.078 0.127 0.159
1-RF 0.427 0.294 0.107 0.169 0.396 0.415 0.293 0.099 0.136 0.166
2-RF 0.428 0.311 0.125 0.171 0.421 0.430 0.311 0.121 0.136 0.170
3-RF 0.421 0.298 0.110 0.170 0.413 0.414 0.297 0.103 0.140 0.170
SI 0.435 0.324 0.134 0.187 0.427 0.411 0.294 0.096 0.139 0.166
FM 0.423 0.292 0.111 0.171 0.417 0.415 0.290 0.097 0.135 0.165
Table 1: Comparison of neural gradient-flow-based methods and neural ODE-based diffusion models over five data sets with 10/100 Euler
steps. The principle of steps in JKO-flow means backward Eulerian method steps (JKO steps).
(a) NSGF
 (b) EPT
 (c) FM
 (d) SI
Figure 3: Visualization results for 2D generated paths. We show different methods that drive the particle from the prior distribution (black)
to the target distribution (blue). The color change of the flow shows the different number of steps (from blue to red means from 0toT).NSGF
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 EPT
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 FM
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 SI
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
Figure 4: 2-Wasserstein Distance of the generated process utilizing
neural ODE-based diffusion models and NSGF. The FM/SI methods
reduce noise roughly linearly, while NSGF quickly recovers the tar-
get structure and progressively optimizes the details in subsequent
steps.
5 Experiments
We conduct an empirical investigation of the NSGF-based
generative models (the standard NSGF and its two-phase vari-
ant NSGF++) across a range of experiments. Initially, we
demonstrate how NSGF guides the evolution and conver-
gence of particles from the initial distribution toward the tar-
get distribution in 2D simulation experiments. Subsequently,
our attention turns to real-world image benchmarks, such
as MNIST and CIFAR-10. To improve the efficiency in
those high-dimensional tasks, we adopt the two-phase variant
NSGF++ instead of the standard NSGF. Our method’s adapt-ability to high-dimensional spaces is exemplified through ex-
periments conducted on these datasets.
5.1 2D simulation data
We assess the performance of various generative modeling
models in low dimensions. Specifically, we conduct a com-
parative analysis between our method, NSGF, and several
neural ODE-based diffusion models, including Flow Match-
ing (FM; [Lipman et al. , 2023 ]), Rectified Flow (1,2,3-RF;
[Liuet al. , 2023 ]), Optimal Transport Condition Flow Match-
ing (OT-CFM; [Tong et al. , 2023; Pooladian et al. , 2023 ]),
Stochastic Interpolant (SI; [Albergo and Vanden-Eijnden,
2023 ]), and neural gradient-flow-based models such as JKO-
Flow [Fan et al. , 2022 ]and EPT [Gao et al. , 2022 ]. Our
evaluation involves learning 2D distributions adapted from
[Grathwohl et al. , 2018 ], which include multiple modes.
Table 1 provides a comprehensive overview of our 2D ex-
perimental results, clearly illustrating the generalization ca-
pabilities of NSGF. Even when employing fewer steps. It is
evident that neural gradient-flow-based models consistently
outperform neural ODE-based diffusion models, particularly
in low-step settings. This observation suggests that neu-
ral gradient-flow-based models generate more informative
paths, enabling effective generation with a reduced number
of steps. Furthermore, our results showcase the best perfor-
mances among neural gradient-flow-based models, indicating
that we have successfully introduced a lower error in approx-
imating Wasserstein gradient flows. More complete details of
the experiment can be found in the appendix E. In the absence
of specific additional assertions, we adopted Euler steps as the
inference steps.

## Page 7

AlgorithmCIFAR 10
IS(↑) FID( ↓) NFE( ↓)
NSGF++ (ours) 8.86 5.55 59
EPT[2022 ] / 46.63 10k
JKO-Flow [2022 ]7.48 23.7 >150
DGGF [2022 ] / 28.12 110
OT-CFM [2023 ] / 11.14 100
FM[2023 ] / 6.35 142
RF[2023 ] 9.20 4.88 100
SI[2023 ] / 10.27 /
Table 2: Comparison of Neural Wasserstein gradient flow methods
and Neural ODE-based diffusion models over CIFAR-10
We present additional comparisons between neural ODE-
based diffusion models and neural gradient-flow-based mod-
els, represented by NSGF and EPT, in Figure 3, 4, which il-
lustrates the flow at different steps from 0toT. Our observa-
tions reveal that the velocity field induced by NSGF exhibits
notably high-speed values right from the outset. This is at-
tributed to the fact that NSGF follows the steepest descent di-
rection within the probability space. In contrast, neural ODE-
based diffusion models, particularly those based on stochastic
interpolation, do not follow the steepest descent path in 2D
experiments. Even with the proposed rectified flow method
by[Liuet al. , 2023 ]to straighten the path, these methods still
necessitate more steps to reach the desired outcome.
5.2 Image benchmark data
In this section, we illustrate the scalability of our algorithm
to the high-dimensional setting by applying our methods to
real image datasets. Notably, we leverage the two-phase vari-
ant (NSGF++) instead of the standard NSGF to enhance ef-
ficiency in high-dimensional spaces. It is worth mention-
ing that we achieve this improvement by constructing a sig-
nificantly smaller training pool compared with the standard
NSGF pool (10% of the usual size), thus requiring only 5%
of the typical training duration. We evaluate NSGF++ on
MNIST and CIFAR10 to show our generating ability. Due
to the limit of the space, we defer the generative images and
comparison results of MNIST in appendix 3.
We report sample quality using the standard Fr ´echet Incep-
tion Distance (FID) [Heusel et al. , 2017 ], Inception Score (IS)
[Salimans et al. , 2016 ]and compute cost using the number
of function evaluations (NFE). These are all standard metrics
throughout the literature.
Table 2 presents the results, including the Fr ´echet Incep-
tion Distance (FID), Inception Score (IS), and the number
of function evaluations (NFE), comparing the empirical dis-
tribution generated by each algorithm with the target distri-
bution. While our current implementation may not yet ri-
val state-of-the-art methods, it demonstrates promising out-
comes, particularly in terms of generating quality (FID), out-
performing neural gradient-flow-based models (EPT, [Gao et
al., 2022 ]; JKO-Flow, [Fanet al. , 2022 ]; DGGF,(LSIF- X2)
[Heng et al. , 2022 ]) with fewer steps. It’s essential to empha-
size that this work represents an initial exploration of this par-
ticular model category and has not undergone optimization
Figure 5: The inference result of our NSGF++ model. The first row
shows the result after 5 NSGF steps and the second row shows the
final results.
using common training techniques found in recent diffusion-
based approaches. Such techniques include the use of ex-
ponential moving averages, truncations, learning rate warm-
ups, and similar strategies. Furthermore, it’s worth noting
that training neural gradient-flow-based models like NSGF in
high-dimensional spaces can be challenging. Balancing the
optimization of per-step information with the limitations of
the neural network’s expressive power presents an intriguing
research avenue that warrants further investigation.
6 Conclusion
This paper delves into the realm of Wasserstein gradient flow
w.r.t. the Sinkhorn divergence as an alternative to kernel
methods. Our main investigation revolves around the Neu-
ral Sinkhorn Gradient Flow (NSGF) model, which introduces
a parameterized velocity field that evolves over time in the
Sinkhorn gradient flow. One noteworthy aspect of the NSGF
is its efficient velocity field matching, which relies solely on
samples from the target distribution for empirical approxima-
tions. The combination of rigorous theoretical foundations
and empirical observations demonstrates that our approxima-
tions of the velocity field converge toward their true coun-
terparts as the sample sizes grow. To further enhance model
efficiency on high-dimensional tasks, a two-phase NSGF++
model is devised, which first follows the Sinkhorn flow to
approach the image manifold quickly and then refine the
samples along a simple straight flow. Through extensive
empirical experiments on well-known datasets like MNIST
and CIFAR-10, we validate the effectiveness of the proposed
methods.

## Page 8

References
[Alaya et al. , 2019 ]Mokhtar Z Alaya, Maxime Berar, Gilles Gasso,
and Alain Rakotomamonjy. Screening sinkhorn algorithm for
regularized optimal transport. NeurIPS , 32, 2019.
[Albergo and Vanden-Eijnden, 2023 ]Michael Samuel Albergo and
Eric Vanden-Eijnden. Building normalizing flows with stochastic
interpolants. In The Eleventh ICLR , 2023.
[Alvarez-Melis et al. , 2022 ]David Alvarez-Melis, Yair Schiff, and
Youssef Mroueh. Optimizing functionals on the space of proba-
bilities with input convex neural networks. Transactions on Ma-
chine Learning Research , 2022.
[Ambrosio et al. , 2005 ]Luigi Ambrosio, Nicola Gigli, and
Giuseppe Savar ´e.Gradient flows: in metric spaces and in the
space of probability measures . Springer Science & Business
Media, 2005.
[Amos et al. , 2017 ]Brandon Amos, Lei Xu, and J Zico Kolter. In-
put convex neural networks. In ICML , pages 146–155. PMLR,
2017.
[Ansari et al. , 2021 ]Abdul Fatir Ansari, Ming Liang Ang, and
Harold Soh. Refining deep generative models via discriminator
gradient flow. In ICLR , 2021.
[Arbel et al. , 2019 ]Michael Arbel, Anna Korba, Adil Salim, and
Arthur Gretton. Maximum mean discrepancy gradient flow.
NeurIPS , 32, 2019.
[Bunne et al. , 2022 ]Charlotte Bunne, Laetitia Papaxanthos, An-
dreas Krause, and Marco Cuturi. Proximal optimal transport
modeling of population dynamics. In International Confer-
ence on Artificial Intelligence and Statistics , pages 6511–6528.
PMLR, 2022.
[Butcher, 1964 ]John C Butcher. Implicit runge-kutta processes.
Mathematics of computation , 18(85):50–64, 1964.
[Choi et al. , 2022 ]Kristy Choi, Chenlin Meng, Yang Song, and
Stefano Ermon. Density ratio estimation via infinitesimal clas-
sification. In International Conference on Artificial Intelligence
and Statistics , pages 2552–2573. PMLR, 2022.
[Cuturi, 2013 ]Marco Cuturi. Sinkhorn distances: Lightspeed com-
putation of optimal transport. NeurIPS , 26, 2013.
[Dai and Seljak, 2020 ]Biwei Dai and Uros Seljak. Sliced iterative
normalizing flows. arXiv preprint arXiv:2007.00674 , 2020.
[Damodaran et al. , 2018 ]Bharath Bhushan Damodaran, Benjamin
Kellenberger, R ´emi Flamary, Devis Tuia, and Nicolas Courty.
Deepjdot: Deep joint distribution optimal transport for unsuper-
vised domain adaptation. In Proceedings of the European con-
ference on computer vision (ECCV) , pages 447–463, 2018.
[Daset al. , 2023 ]Ayan Das, Stathi Fotiadis, Anil Batra, Farhang
Nabiei, FengTing Liao, Sattar Vakili, Da-shan Shiu, and Alberto
Bernacchia. Image generation with shortest path diffusion. arXiv
preprint arXiv:2306.00501 , 2023.
[Deja et al. , 2020 ]Kamil Deja, Jan Dubi ´nski, Piotr Nowak, San-
dro Wenzel, Przemysław Spurek, and Tomasz Trzcinski. End-
to-end sinkhorn autoencoder with noise generator. IEEE Access ,
9:7211–7219, 2020.
[Dhariwal and Nichol, 2021 ]Prafulla Dhariwal and Alexander
Nichol. Diffusion models beat gans on image synthesis. Ad-
vances in neural information processing systems , 34:8780–8794,
2021.
[Duet al. , 2023 ]Chao Du, Tianbo Li, Tianyu Pang, YAN
Shuicheng, and Min Lin. Nonparametric generative modeling
with conditional sliced-wasserstein flows. 2023.[Fanet al. , 2022 ]Jiaojiao Fan, Qinsheng Zhang, Amirhossein
Taghvaei, and Yongxin Chen. Variational wasserstein gradient
flow. In ICML , pages 6185–6215. PMLR, 2022.
[Fatras et al. , 2019 ]Kilian Fatras, Younes Zine, R ´emi Flamary,
R´emi Gribonval, and Nicolas Courty. Learning with minibatch
wasserstein: asymptotic and gradient properties. arXiv preprint
arXiv:1910.04091 , 2019.
[Fatras et al. , 2021a ]Kilian Fatras, Thibault S ´ejourn ´e, R´emi Fla-
mary, and Nicolas Courty. Unbalanced minibatch optimal trans-
port; applications to domain adaptation. In ICML , pages 3186–
3197. PMLR, 2021.
[Fatras et al. , 2021b ]Kilian Fatras, Younes Zine, Szymon Majew-
ski, R ´emi Flamary, R ´emi Gribonval, and Nicolas Courty. Mini-
batch optimal transport distances; analysis and applications.
arXiv preprint arXiv:2101.01792 , 2021.
[Feydy et al. , 2019 ]Jean Feydy, Thibault S ´ejourn ´e, Franc ¸ois-
Xavier Vialard, Shun-ichi Amari, Alain Trouv ´e, and Gabriel
Peyr´e. Interpolating between optimal transport and mmd using
sinkhorn divergences. In The 22nd International Conference on
Artificial Intelligence and Statistics , pages 2681–2690. PMLR,
2019.
[Folland, 1999 ]Gerald B Folland. Real analysis: modern tech-
niques and their applications , volume 40. John Wiley & Sons,
1999.
[Gao et al. , 2019 ]Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang,
Can Yang, and Shunkang Zhang. Deep generative learning via
variational gradient flow. In ICML , pages 2093–2101. PMLR,
2019.
[Gao et al. , 2022 ]Yuan Gao, Jian Huang, Yuling Jiao, Jin Liu, Xil-
iang Lu, and Zhijian Yang. Deep generative learning via eu-
ler particle transport. In Mathematical and Scientific Machine
Learning , pages 336–368. PMLR, 2022.
[Genevay et al. , 2016 ]Aude Genevay, Marco Cuturi, Gabriel
Peyr´e, and Francis Bach. Stochastic optimization for large-scale
optimal transport. NeurIPS , 29, 2016.
[Genevay et al. , 2018 ]Aude Genevay, Gabriel Peyr ´e, and Marco
Cuturi. Learning generative models with sinkhorn divergences.
InInternational Conference on Artificial Intelligence and Statis-
tics, pages 1608–1617. PMLR, 2018.
[Goodfellow et al. , 2014 ]Ian Goodfellow, Jean Pouget-Abadie,
Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial
nets. NeurIPS , 27, 2014.
[Grathwohl et al. , 2018 ]Will Grathwohl, Ricky TQ Chen, Jesse
Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-
form continuous dynamics for scalable reversible generative
models. arXiv preprint arXiv:1810.01367 , 2018.
[Heng et al. , 2022 ]Alvin Heng, Abdul Fatir Ansari, and Harold
Soh. Deep generative wasserstein gradient flows. 2022.
[Heusel et al. , 2017 ]Martin Heusel, Hubert Ramsauer, Thomas
Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash
equilibrium. NeurIPS , 30, 2017.
[Hoet al. , 2020 ]Jonathan Ho, Ajay Jain, and Pieter Abbeel. De-
noising diffusion probabilistic models. NeurIPS , 33:6840–6851,
2020.
[Hsieh et al. , 2021 ]Ya-Ping Hsieh, Panayotis Mertikopoulos, and
V olkan Cevher. The limits of min-max optimization algorithms:
Convergence to spurious non-critical sets. In ICML , pages 4337–
4348. PMLR, 2021.

## Page 9

[Jordan et al. , 1998 ]Richard Jordan, David Kinderlehrer, and Felix
Otto. The variational formulation of the fokker–planck equation.
SIAM journal on mathematical analysis , 29(1):1–17, 1998.
[Kalantari, 2007 ]Iraj Kalantari. Induction over the Continuum ,
pages 145–154. Springer Netherlands, Dordrecht, 2007.
[Komatsu et al. , 2021 ]Tatsuya Komatsu, Tomoko Matsui, and Jun-
bin Gao. Multi-source domain adaptation with sinkhorn barycen-
ter. In 2021 29th European Signal Processing Conference (EU-
SIPCO) , pages 1371–1375. IEEE, 2021.
[Korotin et al. , 2021 ]Alexander Korotin, Lingxiao Li, Aude
Genevay, Justin M Solomon, Alexander Filippov, and Evgeny
Burnaev. Do neural optimal transport solvers work? a continu-
ous wasserstein-2 benchmark. NeurIPS , 34:14593–14605, 2021.
[Liet al. , 2017 ]Chun-Liang Li, Wei-Cheng Chang, Yu Cheng,
Yiming Yang, and Barnab ´as P´oczos. Mmd gan: Towards deeper
understanding of moment matching network. NeurIPS , 30, 2017.
[Lipman et al. , 2023 ]Yaron Lipman, Ricky T. Q. Chen, Heli Ben-
Hamu, Maximilian Nickel, and Matthew Le. Flow matching for
generative modeling. In The Eleventh ICLR , 2023.
[Liuet al. , 2023 ]Xingchao Liu, Chengyue Gong, and qiang liu.
Flow straight and fast: Learning to generate and transfer data
with rectified flow. In The Eleventh ICLR , 2023.
[Liutkus et al. , 2019 ]Antoine Liutkus, Umut Simsekli, Szymon
Majewski, Alain Durmus, and Fabian-Robert St ¨oter. Sliced-
wasserstein flows: Nonparametric generative modeling via opti-
mal transport and diffusions. In ICML , pages 4104–4113. PMLR,
2019.
[Luise et al. , 2019 ]Giulia Luise, Saverio Salzo, Massimiliano Pon-
til, and Carlo Ciliberto. Sinkhorn barycenters with free support
via frank-wolfe algorithm. NeurIPS , 32, 2019.
[Mnih et al. , 2013 ]V olodymyr Mnih, Koray Kavukcuoglu, David
Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learn-
ing. arXiv preprint arXiv:1312.5602 , 2013.
[Mokrov et al. , 2021 ]Petr Mokrov, Alexander Korotin, Lingxiao
Li, Aude Genevay, Justin M Solomon, and Evgeny Burnaev.
Large-scale wasserstein gradient flows. NeurIPS , 34:15243–
15256, 2021.
[Mroueh and Rigotti, 2020 ]Youssef Mroueh and Mattia Rigotti.
Unbalanced sobolev descent. NeurIPS , 33:17034–17043, 2020.
[Mroueh et al. , 2019 ]Youssef Mroueh, Tom Sercu, and Anant Raj.
Sobolev descent. In The 22nd International Conference on Arti-
ficial Intelligence and Statistics , pages 2976–2985. PMLR, 2019.
[Paiet al. , 2021 ]Gautam Pai, Jing Ren, Simone Melzi, Peter
Wonka, and Maks Ovsjanikov. Fast sinkhorn filters: Using ma-
trix scaling for non-rigid shape correspondence with functional
maps. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 384–393, 2021.
[Patrini et al. , 2020 ]Giorgio Patrini, Rianne Van den Berg, Patrick
Forre, Marcello Carioni, Samarth Bhargav, Max Welling, Tim
Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Un-
certainty in Artificial Intelligence , pages 733–743. PMLR, 2020.
[Peyr´eet al. , 2017 ]Gabriel Peyr ´e, Marco Cuturi, et al. Computa-
tional optimal transport. Center for Research in Economics and
Statistics Working Papers , (2017-86), 2017.
[Platen and Bruti-Liberati, 2010 ]Eckhard Platen and Nicola Bruti-
Liberati. Numerical solution of stochastic differential equations
with jumps in finance , volume 64. Springer Science & Business
Media, 2010.[Pooladian et al. , 2023 ]Aram-Alexandre Pooladian, Heli Ben-
Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lip-
man, and Ricky Chen. Multisample flow matching: Straight-
ening flows with minibatch couplings. arXiv preprint
arXiv:2304.14772 , 2023.
[Salimans et al. , 2016 ]Tim Salimans, Ian Goodfellow, Wojciech
Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. NeurIPS , 29, 2016.
[Santambrogio, 2015 ]Filippo Santambrogio. Optimal transport for
applied mathematicians. Birk¨auser, NY , 55(58-63):94, 2015.
[Santambrogio, 2017 ]Filippo Santambrogio. {Euclidean, metric,
and Wasserstein }gradient flows: an overview. Bulletin of Math-
ematical Sciences , 7:87–154, 2017.
[Shaul et al. , 2023 ]Neta Shaul, Ricky TQ Chen, Maximilian
Nickel, Matthew Le, and Yaron Lipman. On kinetic optimal
probability paths for generative models. In ICML , pages 30883–
30907. PMLR, 2023.
[Shen et al. , 2020 ]Zebang Shen, Zhenfu Wang, Alejandro Ribeiro,
and Hamed Hassani. Sinkhorn barycenter via functional gradient
descent. NeurIPS , 33:986–996, 2020.
[Silver et al. , 2016 ]David Silver, Aja Huang, Chris J Maddison,
Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc
Lanctot, et al. Mastering the game of go with deep neural net-
works and tree search. nature , 529(7587):484–489, 2016.
[Song and Ermon, 2019 ]Yang Song and Stefano Ermon. Gener-
ative modeling by estimating gradients of the data distribution.
NeurIPS , 32, 2019.
[Song et al. , 2021 ]Yang Song, Jascha Sohl-Dickstein, Diederik P
Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential
equations. In ICLR , 2021.
[S¨uli and Mayers, 2003 ]Endre S ¨uli and David F Mayers. An intro-
duction to numerical analysis . Cambridge university press, 2003.
[Tong et al. , 2023 ]Alexander Tong, Nikolay Malkin, Guillaume
Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian FATRAS,
Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-
based generative models with minibatch optimal transport. In
ICML Workshop on New Frontiers in Learning, Control, and Dy-
namical Systems , 2023.
[Villani and others, 2009 ]C´edric Villani et al. Optimal transport:
old and new , volume 338. Springer, 2009.
[Wang et al. , 2018 ]Wei Wang, Yuan Sun, and Saman Halgamuge.
Improving mmd-gan training with repulsive loss function. arXiv
preprint arXiv:1812.09916 , 2018.
[Zhang and Katsoulakis, 2023 ]Benjamin J Zhang and Markos A
Katsoulakis. A mean-field games laboratory for generative mod-
eling. arXiv preprint arXiv:2304.13534 , 2023.
[Zhang et al. , 2021a ]Chao Zhang, Zhijian Li, Hui Qian, and Xin
Du. Dpvi: A dynamic-weight particle-based variational inference
framework. arXiv preprint arXiv:2112.00945 , 2021.
[Zhang et al. , 2021b ]Yufeng Zhang, Siyu Chen, Zhuoran Yang,
Michael Jordan, and Zhaoran Wang. Wasserstein flow meets
replicator dynamics: A mean-field analysis of representation
learning in actor-critic. NeurIPS , 34:15993–16006, 2021.

## Page 10

A Computation of Wε-potentials
TheWε-potentials is the cornerstone to conduct NSGF.
Hence, a key component of our method is to efficiently com-
pute this quantity. [Genevay et al. , 2016 ]provided an efficient
method when both µandνare discrete measures so that we
can calculate Wε-potential in terms of samples. In particular,
when µis discrete, f can be simply represented by a finite-
dimensional vector since only its values on supp(µ)matter.
To more clearly explain the relationship between the cal-
culation of Wε-protentials and the composition of our algo-
rithm, we provide the following explanation: In practice, we
actually calculate the Wε-potentials for the empirical distri-
bution of discrete minibatches and construct Sinkhorn WGF
based on this. Therefore, in fact, the µandνin the subse-
quent text refer to (˜Xt
i)n
i=1and(˜Yt
i)n
i=1in the Algorithm 1.
We first introduce another property of the entropy-regularized
optimal transport problem.
Lemma 2. Define the Sinkhorn mapping: A:C(X)×
M+
1(X)→ C(X)
A(f, µ)(y) =−εlogZ
Xexp(( f(x)−c(x,y))/ε)dµ(x).
(17)
The pair (fµ,ν, gµ,ν)are the Wε-potentials of the entropy-
regularized optimal transport problem 2 if they satisfy:
fµ,ν=A(gµ,ν, ν), µ−a.e. and
gµ,ν=A(fµ,ν, µ), ν−a.e.,(18)
or equivalentlyZ
Xh(x,y)dν(y) = 1 , µ−a.e.,
Z
Xh(x,y)dµ(x) = 1 , ν−a.e.,(19)
where h(x,y) := exp1
γ(f(x) +g(x)−c(x,y)).
To be more precise, by plugging in the optimality condition
ongµ,νin 1, the dual problem 2 becomes:
OTε(µ, ν) = max
f∈C⟨f, µ⟩+⟨A(f, µ), ν⟩ (20)
Viewing the discrete measure µas a weight vector wµon
supp(µ), we have:
OTε(µ, ν) = max
f∈Rd
F(f) :=f⊤wµ+Ey∼ν[A(f, µ)(y)]	
,
(21)
that is, we get a standard concave stochastic optimization
problem, where the randomness of the problem comes from
ν[Genevay et al. , 2016 ]. Hence, the problem can be solved
using stochastic gradient descent (SGD). In our methods, we
can treat the computation of Wε-potentials as a Blackbox. In
practice, we use the efficient implementation of the Sinkhorn
algorithm with GPU acceleration from the GeomLoss pack-
age[Feydy et al. , 2019 ].
B Theory of Sinkhorn Wasserstein gradient
flow
Definition 3. (First variation of Functionals over Probabil-
ity). Given a functional F:P(X)→R+, we shell perturbmeasure µwith a perturbation χso that µ+tχbelongs to
P(X)for small t(R
dχ= 0). We treat F(µ), as a functional
over probability in its second argument and compute its first
variation as follows:
d
dtF(µ+tχ)
t=0= lim
t→0F(µ+tχ)− F(µ)
t:=ZδF
δµ(µ)dχ.
(22)
B.1 Proof of Theorem 1
Proof. According to definition 3, given Fε(·) =Sε(·, µ∗)
andtin a neighborhood of 0, we define µt=µ+tδµ
lim
t→01
t(Fε(µt)− Fε(µ)) = lim
t→01
t(Wε(µt, µ∗)− W ε(µ, µ∗))
| {z }
∆first part
t
−lim
t→01
2t(Wε(µt, µt)− W ε(µ, µ))
| {z }
∆second part
t
We first analysis ∆first part
t :=
limt→01
t(Wε(µt, µ∗)− W ε(µ, µ∗)). First, let us re-
mark that (f, g)is the a suboptimal pair of dual potentials
Wε,µ∗(µ)for short. Recall 3,
Wε≥ ⟨µt, f⟩+⟨µ∗, g⟩ −ε
µt⊗µ∗,exp1
ε(f⊕g−C)
−1
,
and thus, since
Wε≥ ⟨µ, f⟩+⟨µ∗, g⟩ −ε
µ⊗µ∗,exp1
ε(f⊕g−C)
−1
,
one has
∆first part
t≥ ⟨δµ, f⟩ −ε
δµ⊗µ∗,exp1
ε(f⊕g−C)
+o(1)
≥ ⟨δµ, f−ε⟩+o(1)
Conversely, let us denote by (ft, gt)the optimal pair of
potentials for Wε(µt, µ∗)satisfying gt(xo) = 0 for some
arbitrary anchor point xo∈ X . As (ft, gt)are suboptimal
potentials for Wε(µ, µ∗)we get that
Wε≥ ⟨µ, ft⟩+⟨µ∗, gt⟩ −ε
µt⊗µ∗,exp1
ε(f⊕g−C)
−1
,
and thus, since
Wε≥ ⟨µt, ft⟩+⟨µ∗, gt⟩ −ε
µt⊗µ∗,exp1
ε(ft⊕g−C)
−1
,
one has
∆first part
t≥ ⟨δµ, f t⟩ −ε
δµ⊗µ∗
t,exp1
ε(ft⊕g−C)
+o(1)
≥ ⟨δµ, f t−ε⟩+o(1)
Now, let us remark that as tgoes to 0,µ+tδµ ⇀ µ .ftandgt
converge uniformly towards fandgaccording to Proposition
13[Feydy et al. , 2019 ]. we get
∆first part
t =⟨δµ, f⟩

## Page 11

Simular to analysis
∆first part
t :=limt→01
t(Wε(µt, µ∗)− W ε(µ, µ∗))
we define
∆second part
t :=limt→01
2t(Wε(µt, µt)− W ε(µ, µ))
, we have:
∆second part
t =⟨δµ, f′⟩
to be more clearly, we denote f=fµ,µ∗andf′=fµ,µthus,
lim
t→01
t(Fε(µt)− Fε(µ)) =⟨δµ, f µ,µ∗−fµ,µ⟩.
So the first variation of Fεis:
δFε
δµ=fµ,µ∗−fµ,µ.
B.2 Proof of Proposition 2
Following the lines of our proof in Theorem 1, we give the
following proof.
Lemma 3. (Fr´echet derivative of entropy-regularized
Wasserstein distance) Let ε >0. We shall fix in the following
a measure µ∗and let (fµ,µ∗, gµ,µ∗)be the Wεpotentials of
Wε(µ, µ∗)according to lemma 1. Consider the infinitesimal
transport T(x) =x+λϕ. We have the Fr ´echet derivative
under this particular perturbation:
d
dλWε(T#µ, µ∗)|λ=0= lim
λ→0Wε(T#µ, µ∗)− W ε(µ, µ∗)
λ
=Z
X∇fµ,µ∗(x)ϕ(x)dµ(x).
(23)
Proof. letf=fµ,µ∗andg=gµ,µ∗be the Wε-potentials 1
toWε(µ, µ∗)for short. By 3 and the optimality of (f, g), we
have follows:
Wε(µ, µ∗) =⟨f, µ⟩+⟨g, µ∗⟩.
However, (f, g)are not necessarily the optimal dual variables
forWε(T#µ, µ∗), recall the lemma 2:
Wε(T#µ, µ∗)≥ ⟨f, T#µ⟩+⟨g, µ∗⟩ −ε⟨h−1, T#µ⊗µ∗⟩,
whereR
Xh(x,y)dµ∗(y) = 1 and hence ⟨h−1, T#µ⊗µ∗⟩=
0. Thus:
Wε(T#µ, µ∗)− W ε(µ, µ∗)≥ ⟨f, T#µ−µ⟩.
Use the change-of-variables formula of the push-forward
measure to obtain:
1
λ⟨f, T#µ−µ⟩=1
λZ
X((f◦T)(x)−f(x))dµ(x)
=Z
X∇f(x+λ′ϕ(x))ϕ(x)dµ(x),
where λ′∈[0, λ]is from the mean value theorem. Here we
assume ∇fis Lipschitz continuous follow Proposition 12 in[Feydy et al. , 2019 ]and Lemma A.4 form [Shen et al. , 2020 ].
We have:
lim
λ→01
λ⟨f, T#µ−µ⟩=Z
X∇f(x)ϕ(x)dµ(x).
Hence:
lim
λ→01
λ(Wε(T#µ, µ∗)− W ε(µ, µ∗))≥Z
X∇f(x)ϕ(x)dµ(x).
Similarly, let f′andg′be theWεpotentials to Wε(T#µ, µ∗),
we have:
Wε(µ, µ∗)≥ ⟨f′, µ⟩+⟨g, µ∗⟩ −ε⟨h−1, µ⊗µ∗⟩,
whereR
Xh(x,y)dµ∗(y) = 1 and hence ⟨h−1, µ⊗µ∗⟩= 0.
Thus:
Wε(T#µ, µ∗)− W ε(µ, µ∗)≤ ⟨f′, T#µ−µ⟩.
Same as above, use the change-of-variables formula and the
mean value theorem:
1
λ⟨f′, T#µ−µ∗⟩=Z
X∇f′(x+λ′ϕ(x))ϕ(x)dµ(x),
Thus:
lim
λ→01
λ(Wε(T#µ, µ∗)− W ε(µ, µ∗))≤
Z
Xlim
λ→0∇f′(x+λ′ϕ(x))ϕ(x)dµ(x).
Assume that ∇f′is Lipschitz continuous and f′→fas
λ→0. Consequently we have limλ→0∇f′(x+λ′ϕ(x))
and hence:
lim
λ→01
λ(Wε(T#µ, µ∗)− W ε(µ, µ∗)) =Z
X∇f(x)ϕ(x)dµ(x).
According to lemma 3, we have:
d
dλFε(T#µ)
λ=0=Z
X∇fµ,µ∗(x)ϕ(x)dµ(x)
−1
2·Z
X2∇fµ,µ(x)ϕ(x)dµ(x)
=Z
X∇fµ,µ∗(x)ϕ(x)dµ−Z
X∇fµ,µ(x)ϕ(x)dµ.
B.3 Proof of theorem 2
Proof. First, we define Ψ(µ) =R
hdµwhere h:Rd→Ris
an arbitrary bounded and continuous function andδΨ(µ)
δµ(x)
denotes the first variation of functional Ψatµsatisfying:
ZδΨ(µ)
δµ(x)ξ(x)dx= lim
ϵ→0Ψ(µ+ϵξ)−Ψ(µ)
ϵ
for all signed measureR
ξ(x)dx= 0. We also have the fol-
lowing:
δΨ(µ)
δµ(·) =δR
hdµ
δµ(·) =h(·)

## Page 12

Assume µtis a flow satisfies the following:
∂tΨ[µt] = (LΨ)[µt],
where,
LΨ[µt] =−Z
⟨∇δFε(µt)
δµ(x),∇xδΨ(µt)
δµ(x)⟩µt(x)dx
(24)
Notably, µtis a solution of equation 2.
Next, let ˜µM
tbe the distribution produced by the equation
14 at time t. Under mild assumption of ˜µM
0⇀ µ 0, we want
to show that the mean-field limit of ˜µM
tasM→ ∞ isµtby
showing that limM→∞Ψ(µM
t) = Ψ( µt)[Folland, 1999 ].
For the measure valued flow ˜µM
tequation 14, the infinites-
imal generator of Ψw.r.t. ˜µM
tis defined as follows:
(LΨ)[˜µM
t] := lim
ϵ→0+Ψ(˜µM
t+ϵ)−Ψ(˜µM
t)
ϵ,
According to the definition of first variation, it can be calcu-
lated that
(LΨ)[˜µM
t] = lim
ϵ→0+Ψ[PM
i=11
Mδxi
t+ϵ]−Ψ(PM
i=11
Mδxi
t)
ϵ
=ZδΨ(˜µM
t)
δµ(x)MX
i=11
M∂tρ(xi
t)dx
Then we adopt the Induction over the Continuum to prove
limn→∞Ψ(˜µM
t) = Ψ( µt)for all t >0. Here t∈R+satisfy
the requirement of well ordering and the existence of a great-
est lower bound for non-empty subsets, so Induction over the
Continuum is reasonable [Kalantari, 2007 ].
1. As for t= 0, our assumption of ˜µM
0⇀ µ 0suffice.
2. For the case of t=t∗, we first hypothesis that for t < t∗,
˜µM
t⇀ µ tasM→ ∞ . Then for t < t∗we have:
lim
M→∞(LΨ)[˜µM
t]
= lim
M→∞ZδΨ(˜µM
t)
δµ(x)MX
i=11
M∂tρ(xi
t)dx
=−lim
M→∞Z
⟨∇δFε(µt)
δµ(x),∇xδΨ(˜µM
t)
δµ(x)⟩˜µM
t(x)dx
=−Z
⟨∇δFε(µt)
δµ(x),∇xδΨ(µt)
δµ(x)⟩µt(x)dx.
Because limM→∞Ψ(˜µM
0) = Ψ( µ0)att= 0 and
limM→∞(∂tΨ)[˜µM
t] = ( ∂tΨ)[µt]for all t < t∗, we have
limM→∞Ψ(˜µM
t∗) = Ψ( µt∗).
Combining (1) and (2), we can reach to the conclusion
thatlimM→∞Ψ(µM
t) = Ψ( µt)for all t. which indicates
that˜µM
t⇀ µ tif˜µM
0⇀ µ 0. Since µtsolves the partial
differential equation 10, we conclude that the path of equa-
tion 14 starting from ˜µM
0weakly converges to a solution of
the partial differential equation equation 10 starting from µ0
asM→ ∞ .B.4 Descending property
Proposition 3. Consider the Sinkhorn gradient flow 10, the
differentiation of Fε(µt)with respect to the time tsatisfies:
dFε(µt)
dt=−Z∇δFε(µt)
δµt2
dµt≤0 (25)
Proof. By substituting Ψ(·) =Fε(·)in equation 24, we di-
rectly reach to the above equality.
C Minibatch Optimal Transport
For large datasets, the computation and storage of the opti-
mal transport plan can be challenging due to OT’s cubic time
and quadratic memory complexities relative to the number
of samples [Cuturi, 2013; Genevay et al. , 2016; Peyr ´eet al. ,
2017 ]. The minibatch approximation offers a viable solution
for enhancing calculation efficiency. Theoretical analysis of
using the minibatch approximation for transportation plans is
provided by [Fatras et al. , 2019; Fatras et al. , 2021b ]. Al-
though minibatch OT introduces some errors compared to
the exact OT solution, its efficiency in computing approxi-
mate OT is clear, and it has seen successful applications in
domains like domain adaptation [Damodaran et al. , 2018;
Fatras et al. , 2021a ]and generative modeling [Genevay et al. ,
2018 ].
More recently, [Pooladian et al. , 2023; Tong et al. , 2023 ]
introduced OT-CFM and empirically demonstrated that us-
ing minibatch approximation of optimal transport in flow
matching methods [Liuet al. , 2023; Lipman et al. , 2023 ]can
straighten the flow’s trajectory and yield more consistent sam-
ples. OT-CFM specifically focuses on minibatch initial and
target samples, continuing to use random linear interpolation
paths. In contrast, NSGF leverages minibatch Wε-potentials
to construct Sinkhorn gradient flows in minibatches. Our
method also involves performing velocity field matching on
the flow’s discretized form, marking a separate and innovative
direction in the field.
D NSGF++
We introduce a two-phase NSGF++ algorithm that initially
employs the Sinkhorn gradient flow for rapid approximation
to the image manifold, followed by sample refinement using a
straightforward straight flow. The NSGF++ model comprises
three key components:
• An NSGF model trained for T≤5time steps.
• A phase-transition time predictor, denoted as tϕ:X →
[0,1], which facilitates the transition from NSGF to NSF.
• A Neural Straight Flow (NSF) model, trained through
velocity field matching on a linear interpolation straight
flowXt∼(1−t)P0+tP1, t∈[0,1].
the detailed algorithm is outlined in 3.
In the inference process of NSGF++, we initially apply the
NSGF with fewer than 5 NFE, starting from X0∼P0, to ob-
tain an intermediate output ˜XT. This output is then processed
using the time predictor tϕ. The final output is achieved by
refining this intermediate result with the NSF model, starting

## Page 13

Algorithm 3: NSGF++ Training
Input : number of time steps T, batch size n, gradient flow step size η >0, empirical or samplable distribution µ0andµ∗,
neural network parameters θ, optimizer step size γ >0
/*NSGF model */
/*Build trajectory pool */
while Building do
/*Sample batches of size n i.i.d. from the datasets */
˜X0
i∼µ0,˜Yi∼µ∗, i= 1,2,···n.
fort= 0,1,···Tdo
calculate f˜µt,˜µt
˜Xt
i
, f˜µt,˜µ∗
˜Xt
i
.
ˆvFϵµt
˜Xt
i
=∇f˜µt,˜µt
˜Xt
i
− ∇f˜µt,˜µ∗
˜Xt
i
.
˜Xt+1
i=˜Xt
i+ηˆvFϵ
t
˜Xt
i
.
store all
˜Xt
i,ˆvFϵ
t
˜Xt
i
pair into the pool , i= 1,2,···n.
/*velocity field matching */
while Not convergence do
from trajectory pool sample pair
˜Xt
i,ˆvFϵ
t
˜Xt
i
.
L(θ) =vθ(˜Xt
i, t)−ˆvFεµt
˜Xt
i2
,
θ←θ−γ∇θL(θ).
/*phase trainsition time predictor */
while Training do
˜X0
i∼µ0,˜Yi∼µ∗, i= 1,2,···n.
t∼ U(0,1).
Xt= t˜Yi+ (1−t)˜X0
i
L(ϕ) =Et∈U(0,1),Xt∼Pt||t−tϕ(Xt)||2.
ϕ←ϕ−γ′∇ϕL(ϕ).
/*NSF model */
while Training do
˜X0
i∼µ0,˜Yi∼µ∗, i= 1,2,···n.
t∼ U(0,1).
Xt= t˜Yi+ (1−t)˜X0
i.
LNSF(δ)←uδ(t, Xt)−
˜Yi−˜X0
i2
.
δ←δ−γ′′∇δLNSF(δ).
Output: vθparameterize the time-varying velocity field of NSGF, tϕparameterize the phase trainsition time predictor, uδ
parameterize the NSF model
from the state tϕ(˜Xt). The detailed algorithm is outlined in
4.
E Experimrnts
E.1 2D simulated data
For the 2D experiments, we closely follow [Tong
et al. , 2023 ]and the released code at https:
//github.com/atong01/conditional-flow-matching (code
released under MIT license), and use the same synthetic
datasets and the 2-Wasserstein distance between the test
set and samples simulated using NSGF as the evaluation
metric. We use 1024 samples in the test set since we find
the We use a simple MLP with 3 hidden layers and 256
hidden units to parameterize the velocity matching networks.
We use batch size 256 and 10/100 steps with a uniform
schedule at sampling time. For both Nerual gradient-flow-based models and Nerual ODE-based Models, we train for
20000 steps in total. Note that FM cannot be used for the
8gaussians-moons task since it requires a Gaussian source,
but we still conducted experiments with the algorithm and
found competitive experimental results. We believe that
this is because FM is essentially very close to 1-RF in its
algorithmic design, and that the Gaussian source condition
can be meaningfully relaxed in practice, as confirmed in
[Tong et al. , 2023 ]. The experiments are run using one 3090
GPU and take approximately less than 60 minutes (for both
training and testing).
For the neural gradient-flow-based models, we solely im-
plemented the EPT without the outer loop, as the outer loop
can be likened to a GAN-like distillation approach [Goodfel-
lowet al. , 2014 ]. Notably, the original EPT [Gao et al. , 2022 ]
recommends iterating for 20,000rounds with an exceedingly

## Page 14

Algorithm 4: NSGF++ Inference
Input : number of NSGF time steps T≤5, NSGF++
inference step size η, NSGF velocity field vθ,
phase trainsition time predictor tϕ, NSF inference
step size ω, NSF model uδ, prior samples ˜X0
i∼˜µ0,
ODEsolver (X, model, starttime, endtime, steps )
/*NSGF phase */
fort= 0,1,···Tdo
˜Xt+1
i=˜Xt
i+ηvθ
˜Xt
i, t
, i= 1,2,···n.
/*phase trainsition time predict */
ˆt=tϕ(˜XT
i).
/*NSF refine phase */
ˆT= (1−ˆt)/ω.
˜X=ODEsolver (˜XT
i,uδ,ˆt,1,ˆT).
Output: ˜Xas the results.
small step size; however, to ensure a fair comparison, we em-
ployed the same number of steps as the other methods while
adapting the step size accordingly. It’s worth mentioning that
for the JKO-Flow, we used the recommended parameter set-
ting of 10steps, as suggested in [Fanet al. , 2022 ], but we
also provide results for 100steps for comparative purposes.
All the results for Neural Gradient flow-based models were
trained and sampled following the standard procedures out-
lined in their respective papers.
E.2 Image benchmark data
For the MNIST/CIFAR-10 experiments, we summarize the
setup of our NSGF++ model here, where the exact param-
eter choices can be seen in the source code. For the cal-
culation of Wε-potentials, we use the GeomLoss package
[Feydy et al. , 2019 ]with blur = 0.5,1.0or2.0, scaling =
0.80,0.85or0.95depends on learning rate of Sinkhorn gra-
dient flow. We find using an incremental lr scheme will im-
prove training performance. More detailed experiments we
will leave for future work. We used the Adam optimizer with
β1= 0.9, β2= 0.999and no weight decay. Here we list
different part of our NSGF++ model separately. First, we use
the UNet architecture from [Dhariwal and Nichol, 2021 ]. For
MNIST, we use channels = 32, depth = 1, channels multiple
= [1, 2, 2], heads = 1, attention resolution = 16, dropout = 0.0.
For CIFAR-10, we use channels = 128, depth = 2, channels
multiple = [1, 2, 2, 2], heads = 4, heads channels = 64, atten-
tion resolution = 16, dropout = 0.0. We use the same UNet
architecture in our neural straight flow model.
For the phase transition time predictor, we employed an
efficiently designed convolutional neural network (CNN) ca-
pable of achieving satisfactory results while optimizing train-
ing time. The CNN used in our study consists of a struc-
tured architecture featuring four convolutional layers with fil-
ter depths of 32, 64, 128, and 256. Each layer uses a 3x3 ker-
nel, a stride of 1, and padding of 1, coupled with ReLU activa-
tion and 2x2 average pooling for effective feature downsam-
pling. The network culminates in a fully connected layer that
transforms the flattened features into a single value, further
(a) NSGF
 (b) CFM
 (c) OT-CFM
(d) 1-RF
 (e) 2-RF
 (f) SI
Figure 6: Visualization results for 2D generated paths. We show
different methods that drive the particle from the prior distribution
(black) to the target distribution (blue). The color change of the flow
shows the different number of steps (from blue to red means from 0
toT). We can see NSGF using fewer steps than OT-CFM
refined through a sigmoid activation function for regression
tasks targeting time value outputs. This architecture is tai-
lored for processing image inputs to predict continuous time
values within a specific range. The training parameters are as
follows: Batch size: 128Learning rate: 10−4. For sampling,
a 5-step Euler integration is applied in the NSGF phase on
MNIST and CIFAR-10 datasets. Training the phase transi-
tion time predictor is efficient and methodically streamlined.
Utilizing a well-structured CNN as its backbone, the model
reaches peak performance in merely 20 minutes, covering
40,000 iterations. This training efficiency is a significant ad-
vantage, especially for applications that demand rapid model
adaptation.
For the MNIST/CIFAR-10 experiments, a considerable
amount of storage space is required when establishing the
trajectory pool during the first phase of the algorithm. For
MNIST dataset, setting the batch size to 256 and saving 1500
batches 5-step minibatch Sinkhorn gradient flow trajectories
requires less than 20GB of storage space and with CIFAR-10,
setting the batch size to 128 and saving 2500 batches 5-step
minibatch Sinkhorn gradient flow trajectories requires about
45GB of storage space. In situations where storage space is
limited, we suggest dynamically adding and removing trajec-
tories in the trajectory pool to meet the training requirements.
Identifying a more effective trade-off between training time
and storage space utilization is a direction for future improve-
ment.
E.3 Supplementary experimental results
2D simulated data
In our supplementary materials, we present additional results
from 2D simulated data to demonstrate the efficiency of the
NSGF++ model in Figure 6 and Figure 7. These results in-
dicate that NSGF++ achieves competitive performance with
a more direct path and fewer steps compared to other neural
Wasserstein gradient flow and flow-matching methods, high-
lighting its effectiveness and computational efficiency.

## Page 15

NSGF
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 EPT
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 FM
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 SI
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 OT-CFM
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 RF-1
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246 RF-2
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
6
 4
 2
 0 2 4 66
4
2
0246
Figure 7: 2-Wasserstein Distance of the generated process utilizing
neural ODE-based diffusion models and NSGF. The FM/SI methods
reduce noise roughly linearly, while NSGF quickly recovers the tar-
get structure and progressively optimizes the details in subsequent
steps.
MNIST
In our study, we include results from the MNIST dataset to
showcase the efficiency of the NSGF++ model. As detailed
in Table 3, NSGF++ achieves competitive Fr ´echet Inception
Distances (FIDs) while utilizing only 60% of the Number
of Function Evaluations (NFEs) typically required. This un-
derscores the model’s effectiveness in balancing performance
with computational efficiency. To evaluate our results, we use
the Fr ´echet inception distance (FID) between 10K generated
samples and the test dataset. Here, a smaller FID value indi-
cates a higher similarity between generated and test samples.
CIFAR-10
In our work, we present further results of the NSGF++ model
on the CIFAR-10 dataset, illustrated in Figures 9 and 11.
These experimental findings demonstrate that NSGF++ at-
tains competitive performance in generation tasks, highlight-
ing its efficacy.AlgorithmMNIST
FID(↓) NFE( ↓)
NSGF++(ours) 3.8 60
SWGF [2019 ] 225.1 500
SIG[2020 ] 4.5 /
FM[2023 ] 3.4 100
OT-CFM [2023 ] 3.3 100
RF[2023 ] 3.1 100
Training set 2.27 /
Table 3: Comparison of NSGF++ and other methods over MNIST,
The last row states statistics of the FID scores between 10k training
examples and 10k test examples
Figure 8: Uncurated samples on MNIST and L2-nearest neighbors
from the training set (top: Samples, bottom: real)We observe that
they are significantly different. Hence, our method generates really
new samples and is not just reproducing the samples from the train-
ing set
Figure 9: Uncurated samples on CIFAR-10 and L2-nearest neigh-
bors from the training set (top: Samples, bottom: real)

## Page 16

Figure 10: The extensive inference result of our NSGF++ model on MNIST. The first row shows the result after 5 NSGF steps and the second
row shows the final results

## Page 17

Figure 11: The extensive inference result of our NSGF++ model on CIFAR-10. The first row shows the result after 5 NSGF steps and the
second row shows the final results

