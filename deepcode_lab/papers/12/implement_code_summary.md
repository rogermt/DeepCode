# Code Implementation Progress Summary
*Accumulated implementation progress for all files*


================================================================================
## IMPLEMENTATION File utils/sinkhorn.py; ROUND 0 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:20:53
**File Implemented**: utils/sinkhorn.py

**Core Purpose**  
Provides a thin wrapper around the GeomLoss Sinkhorn divergence to compute dual potentials and their gradients, enabling the calculation of empirical velocity fields required for the Neural Sinkhorn Gradient Flow algorithms.

**Public Interface**  
- **Class `SinkhornEngine`** – Engine for computing Sinkhorn dual potentials.  
  - **Constructor parameters**: `epsilon: float = 0.1`, `blur: float = 1.0`, `scaling: float = 0.9`, `device: str = "cpu"`  
  - **Key methods**:  
    - `compute_potentials(x_src: torch.Tensor, x_tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`  
      Returns the four dual potentials *(f_src, f_tgt, g_src, g_tgt)* for the self‑ and cross‑potential respectively.  
    - `empirical_velocity(x: torch.Tensor, x_target: torch.Tensor) -> torch.Tensor`  
      Computes the empirical velocity field `∇f_self(x) − ∇f_cross(x)` using autograd on the potentials.

**Internal Dependencies**  
- `torch` – tensor operations and autograd.  
- `dataclasses.dataclass` – simple data‑class definition.  
- `typing.Tuple` – type hint for return values.  
- `geomloss.SamplesLoss` – core Sinkhorn loss/potential computation (wrapped in `self.loss_fn`).

**External Dependencies** (files that will import this module)  
- Expected to be imported by training scripts: `nsfg/train/train_nsfg.py`, `nsfg/train/train_nsfgpp.py`.  
- May also be used in evaluation utilities such as `nsfg/eval/inference_nsfg.py` for velocity‑field checks.  
- The main exported symbol is the `SinkhornEngine` class.

**Implementation Notes**  
- The engine creates a `SamplesLoss("sinkhorn", …)` object once during `__post_init__` for efficiency.  
- `compute_potentials` calls `self.loss_fn.potential` twice: first with identical point clouds to obtain the self‑potential, then with source and target clouds for the cross‑potential.  
- Potentials are squeezed to 1‑D tensors so that `torch.autograd.grad` can be applied directly.  
- `empirical_velocity` forces the input `x` to require gradients, computes the sum of each potential, and extracts per‑sample gradients via `torch.autograd.grad`.  
- A small `__main__` block demonstrates usage and can serve as a quick unit‑test.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File nsfg/utils/replay_buffer.py; ROUND 1 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:20:56
**File Implemented**: nsfg/utils/replay_buffer.py

**Core Purpose**  
Provides a lightweight circular replay buffer for storing trajectory batches — particle positions, associated times, and empirical velocity fields — and sampling random minibatches for training the velocity‑field network.

**Public Interface**  
- **Class `ReplayBuffer`** – container for trajectory data.  
  - **Constructor** `__init__(capacity: int, device: torch.device | str = "cpu")` – sets maximum number of stored batches and target device for sampled tensors.  
  - **Methods**  
    - `add(x: torch.Tensor, t: torch.Tensor, v_hat: torch.Tensor) -> None` – inserts a new batch; discards oldest when capacity is exceeded.  
    - `sample(batch_size: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]` – returns a random minibatch `(x_batch, t_batch, v_batch)` concatenated along dimension 0; moves tensors to the configured device.  
    - `clear() -> None` – empties the buffer.  
    - `__len__() -> int` – current number of stored batches.  
    - `__repr__() -> str` – printable summary.

**Internal Dependencies**  
- `from __future__ import annotations` – future‑type‑hint support.  
- `import random` – random sampling of indices.  
- `from typing import List, Tuple` – type annotations for stored data.  
- `import torch` – tensor handling, device management, and cloning/detaching.

**External Dependencies**  
- **Expected consumers** (not yet implemented but will use the buffer):  
  - `nsfg/train/train_nsfg.py` – builds the trajectory pool and samples minibatches for velocity‑field matching.  
  - `nsfg/train/train_nsfgpp.py` – may reuse the same buffer for the NSGF++ pipeline.  
  - `nsfg/eval/inference_nsfg.py` and `nsfg/eval/inference_nsfgpp.py` – could sample trajectories for evaluation or debugging.

**Implementation Notes**  
- **Architecture choices**:  
  - Stores data as a Python list of `(x, t, v_hat)` tuples for simplicity; each tensor is detached and cloned to avoid retaining computation graphs.  
  - FIFO eviction is performed by `pop(0)` when capacity is exceeded.  
  - `sample` concatenates selected batches, moving the result to the user‑specified device, enabling seamless GPU training.  
- **Cross‑file relationships**: The buffer acts as a shared data source between the training scripts (which generate trajectories via the Sinkhorn engine) and the model training loops that consume minibatches for gradient updates.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File nsfg/utils/ode_solver.py; ROUND 2 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:21:00
**File Implemented**: nsfg/utils/ode_solver.py

**Core Purpose**  
Provides lightweight ODE integration utilities (explicit Euler and classical 4‑stage Runge‑Kutta) for advancing particle states under a learned velocity field. Includes a generic `integrate` routine that repeatedly applies the chosen step method and returns the final state together with the full trajectory.

---

**Public Interface**  

- **Function `euler_step(model, x, t, dt)`**  
  - *Purpose*: Perform a single explicit Euler update `x ← x + dt·model(x, t)`.  
  - *Parameters*  
    - `model: Callable[[Tensor, Tensor], Tensor]` – callable returning the velocity field `dx/dt`.  
    - `x: Tensor` – current particle tensor of shape `[batch, …]`.  
    - `t: Tensor` – current time (scalar tensor or `[batch]`).  
    - `dt: float` – step size.  
  - *Returns*: `Tensor` – updated particle tensor.  

- **Function `rk4_step(model, x, t, dt)`**  
  - *Purpose*: Perform a single classical Runge‑Kutta (RK4) update.  
  - *Parameters*: Same as `euler_step`.  
  - *Returns*: `Tensor` – updated particle tensor after one RK4 step.  

- **Function `integrate(model, x0, t0, t1, dt, method="euler")`**  
  - *Purpose*: Integrate the ODE `dx/dt = model(x, t)` from `t0` to `t1` using the specified method.  
  - *Parameters*  
    - `model: Callable[[Tensor, Tensor], Tensor]` – velocity field.  
    - `x0: Tensor` – initial particle state.  
    - `t0: float` – start time.  
    - `t1: float` – end time.  
    - `dt: float` – integration step size.  
    - `method: str` – `"euler"` or `"rk4"` (case‑insensitive).  
  - *Returns*: `Tuple[Tensor, List[Tensor]]` – final state `x_T` and a list `trajectory` containing the initial state and all intermediate states.  

- **Exports (`__all__`)**  
  - `"euler_step"`  
  - `"rk4_step"`  
  - `"integrate"`  

---

**Internal Dependencies**  

- `from __future__ import annotations` – enables postponed evaluation of type hints.  
- `import torch` – core tensor operations, device handling, and no‑grad context.  
- `from typing import Callable, Iterable, List, Tuple` – type annotations for clarity.  

---

**External Dependencies**  

- **Expected to be imported by**  
  - `nsfg/eval/inference_nsfg.py` – NSGF inference pipeline.  
  - `nsfg/eval/inference_nsfgpp.py` – NSGF++ inference pipeline.  
  - Potential future training scripts that may need to simulate trajectories (e.g., for debugging).  

- **Key exports used elsewhere**  
  - `euler_step` and `rk4_step` for single‑step updates.  
  - `integrate` for full‑trajectory generation during inference.  

---

**Implementation Notes**  

- **Design choices**  
  - All integration functions run under `torch.no_grad()` because they are used during inference where gradients are not required, reducing memory overhead.  
  - `integrate` builds a per‑sample time tensor `t` matching the batch size, enabling both scalar and per‑sample time handling.  
  - The step size `dt` is converted to a tensor on the same device/dtype as `x` for numerical consistency.  
  - Input validation checks for positive `dt` and a positive number of steps, raising informative `ValueError`s.  

- **Cross‑file relationships**  
  - The `model` callable is expected to be any neural network that maps `(x, t)` → velocity, e.g., `v_θ` or `u_δ` from the `nsfg/models` package.  
  - By keeping the solver generic, it can be reused for both the NSGF and NSGF++ phases without modification.  

---

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File nsfg/eval/inference_nsfg.py; ROUND 3 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:21:03
**File Implemented**: nsfg/eval/inference_nsfg.py

**Core Purpose**  
Implements the NSGF inference pipeline (Algorithm 2).  Provides a function that iteratively integrates a learned velocity‑field network over a fixed number of explicit Euler steps to transform a prior distribution into samples from the target distribution, and a small CLI for running the inference and saving the generated samples.

---

**Public Interface**  

- **Function `run_nsfg`**  
  ```python
  run_nsfg(
      v_theta: torch.nn.Module,
      X0: torch.Tensor,
      eta: float = 0.1,
      T: int = 10,
      device: torch.device | str | None = None,
  ) -> torch.Tensor
  ```
  *Purpose*: Executes NSGF inference by repeatedly calling `v_theta(x, t)` and updating the particles with an explicit Euler step `x ← x + η·v`. Returns the final particle positions after `T` steps.  

- **CLI entry point `main()`** (executed when the module is run as a script)  
  - Parses arguments (`--checkpoint`, `--batch-size`, `--dim`, `--steps`, `--eta`, `--device`, `--output`).  
  - Loads a saved `torch.nn.Module` checkpoint, creates a Gaussian prior `X0`, calls `run_nsfg`, and saves the resulting samples to a NumPy ``.npz`` file.

---

**Internal Dependencies**  

- `argparse` – command‑line argument parsing.  
- `os` – file‑system checks.  
- `torch` – core tensor operations, model handling, device management.  
- `nsfg.utils.ode_solver.euler_step` – imported but **not used** in the current implementation (the Euler update is performed inline).  

---

**External Dependencies**  

- **Expected to be imported by**:  
  - Training scripts that need to run inference on a trained `v_theta` (e.g., evaluation notebooks, benchmark scripts).  
  - Higher‑level evaluation pipelines (e.g., `nsfg/eval/inference_nsfgpp.py`) that may call `run_nsfg` as a sub‑routine for the first phase of NSGF++.  

- **Key exports used elsewhere**:  
  - `run_nsfg` – the primary function other modules will call to generate samples from a learned velocity field.  

---

**Implementation Notes**  

- **Architecture decisions**:  
  - The function is deliberately simple: it performs the Euler integration loop inside a `torch.no_grad()` block, moving the model to the requested device and setting it to evaluation mode.  
  - Time is represented as a tensor of shape `[batch]` filled with the current integer step, matching the expected input signature of `v_theta`.  
  - The CLI saves samples in a compact ``.npz`` format for easy downstream loading.  

- **Cross‑File Relationships**:  
  - Relies on the contract that `v_theta` accepts `(x, t)` where `t` is a 1‑D tensor of scalars.  
  - Although `euler_step` is imported, the integration is performed manually; this keeps the function self‑contained and avoids an extra function call overhead.  

---

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File nsfg/eval/inference_nsfgpp.py; ROUND 4 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:21:07
**File Implemented**: nsfg/eval/inference_nsfgpp.py

**Core Purpose**  
Implements the inference pipeline for the NSGF++ algorithm (Algorithm 4). It runs the two‑phase generation: a few Euler steps of the learned velocity‑field network `v_θ`, predicts the phase‑transition time τ with a small CNN `predictor_φ`, and then integrates the straight‑flow network `u_δ` from τ to 1 using either explicit Euler or RK4. The module also provides a CLI for end‑to‑end sample generation and checkpoint loading.

---

**Public Interface**  

- **Function `run_nsfgpp`**  
  ```python
  run_nsfgpp(
      v_theta: torch.nn.Module,
      u_delta: torch.nn.Module,
      predictor_phi: torch.nn.Module,
      X0: torch.Tensor,
      eta: float = 0.1,
      omega: float = 0.1,
      T: int = 5,
      device: torch.device | str | None = None,
      method: str = "euler",
  ) -> Tuple[torch.Tensor, torch.Tensor]
  ```
  *Purpose*: Executes the full NSGF++ inference pipeline on a batch of prior particles `X0`.  
  *Returns*:  
  - `X_T`: final generated samples after both phases (shape `[batch, …]`).  
  - `tau`: predicted transition times (shape `[batch, 1]`).

- **Function `_parse_args`** (internal) – builds an `argparse.Namespace` for the CLI.

- **Function `main`** (internal) – CLI entry point: loads checkpoints, creates a Gaussian prior, calls `run_nsfgpp`, and saves the generated samples (and τ) to a `.npz`/torch file.

- **Exports (`__all__`)**  
  ```python
  __all__ = ["run_nsfgpp"]
  ```

---

**Internal Dependencies**  

- **Standard library**  
  - `argparse`, `os`, `typing.Tuple`

- **Third‑party**  
  - `torch` – tensor operations, model handling, device management.

- **Local project modules**  
  - `nsfg.utils.ode_solver` – imports `euler_step`, `rk4_step`, `integrate` (currently only `euler_step`/`rk4_step` are referenced for future extensions).  
  - `nsfg.models.predictor_cnn.PredictorCNN` – CNN that predicts τ from intermediate particles.  
  - `nsfg.models.unet.UNet` – UNet architecture used for both `v_θ` and `u_δ`.

---

**External Dependencies**  

- **Expected importers**  
  - Scripts or notebooks that need to generate samples with NSGF++ (e.g., `nsfg/scripts/run_*.sh`, evaluation notebooks).  
  - Potential higher‑level orchestration code in `nsfg/train/` that may call the inference routine after training.

- **Key exports used elsewhere**  
  - `run_nsfgpp` – the primary function other modules will call to obtain generated data.  

---

**Implementation Notes**  

- **Device handling** – Automatically selects CUDA if available; otherwise falls back to CPU. All models are moved to the chosen device and set to `eval()` mode.  
- **Phase 1** – Performs `T` Euler updates with `v_theta`. The time tensor `t` is incremented by 1 each step (used only for conditioning).  
- **Phase 2** – Predicts τ with `predictor_phi`; clamps to `(1e‑6, 1‑1e‑6)` to avoid division‑by‑zero.  
- **Phase 3** – Computes per‑sample step counts `N_per = ceil((1‑τ)/ω)`, then uses the maximum across the batch (`N`) to keep the loop simple (all samples advance together). Each iteration updates `X` with `u_delta` and increments the scalar time `t_sf` by `ω`.  
- **Method selection** – Parameter `method` currently defaults to `"euler"`; the stub for `"rk4"` is present for future extension via `rk4_step`.  
- **CLI** – Loads model checkpoints assuming they contain only `state_dict`s, constructs default UNet and PredictorCNN architectures, generates a standard Gaussian prior of the requested dimensionality, runs inference, and saves results with `torch.save`.  

---

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File nsfg/eval/metrics.py; ROUND 5 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:21:12
**File Implemented**: nsfg/eval/metrics.py

**Core Purpose**  
Provides a collection of evaluation utilities for generated samples, including 2‑Wasserstein distance, Frechet Inception Distance (FID), and Inception Score (IS). It also supplies an `InceptionFeatureExtractor` wrapper around a pretrained Inception‑v3 model for feature and logit extraction.

---

**Public Interface**  

- **Class `InceptionFeatureExtractor`** – Wraps a pretrained Inception‑v3 model and returns both pool‑3 features (2048‑D) and class logits.  
  - **Constructor** `__init__(device: torch.device | str = "cpu")` – loads the model on the given device and prepares a normalization transform.  
  - **Key methods**:  
    - `_preprocess(x: torch.Tensor) -> torch.Tensor` – resizes to 299×299 and normalises.  
    - `forward(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]` – returns `(features, logits)` for a batch of images.  

- **Function `wasserstein2_distance(x: torch.Tensor, y: torch.Tensor, device: torch.device | str = "cpu") -> float`** – Computes the squared 2‑Wasserstein distance between two point clouds using GeomLoss `SamplesLoss("energy")`.  

- **Function `compute_fid(generated: torch.Tensor, real: torch.Tensor, device: torch.device | str = "cpu", batch_size: int = 64) -> float`** – Calculates Frechet Inception Distance between two image sets.  

- **Function `compute_inception_score(generated: torch.Tensor, device: torch.device | str = "cpu", batch_size: int = 64, splits: int = 10) -> float`** – Computes the Inception Score (mean over `splits`).  

- **Function `evaluate_generated(generated: torch.Tensor, real: torch.Tensor, device: torch.device | str = "cpu", batch_size: int = 64) -> Dict[str, float]`** – Convenience wrapper returning a dict with keys `wasserstein2`, `fid`, and `inception_score`.  

- **`__all__`** – Exposes the four public functions and the `InceptionFeatureExtractor` class.

---

**Internal Dependencies**  

- **torch** – tensors, neural‑network modules, functional utilities, data loaders.  
- **torch.nn** – base class for `InceptionFeatureExtractor`.  
- **torch.nn.functional** – image resizing (`F.interpolate`).  
- **torch.utils.data** – `DataLoader`, `TensorDataset` for batched feature extraction.  
- **torchvision.models** – pretrained Inception‑v3 model.  
- **torchvision.transforms** – normalization transform for Inception input.  
- **numpy** – array handling for statistics and IS computation.  
- **scipy.linalg** – matrix square‑root (`sqrtm`) for FID.  
- **geomloss.SamplesLoss** – energy loss used to compute the 2‑Wasserstein distance.  

---

**External Dependencies** (who imports this file)  

- **nsfg/eval/inference_nsfg.py** – may import `evaluate_generated` for quick sanity checks.  
- **nsfg/eval/inference_nsfgpp.py** – similar usage.  
- **nsfg/scripts/** – CLI scripts that generate samples and call the evaluation helpers.  
- **Any downstream benchmarking notebook or script** that needs quantitative metrics.

---

**Implementation Notes**  

- **Inception preprocessing**: Images are resized to 299×299 and channel‑wise normalized using ImageNet statistics before feeding the model.  
- **Feature extraction**: The model’s final pooling layer is kept; the fully‑connected head is used only for logits.  
- **Wasserstein computation**: Uses GeomLoss `SamplesLoss("energy")` with zero blur/scaling/epsilon, which directly yields the squared 2‑Wasserstein distance.  
- **Batch handling**: Both FID and IS compute activations/logits in configurable batch sizes to avoid OOM.  
- **Numerical stability**: Small epsilon added in IS KL computation; complex‑valued covariance roots are handled by discarding the imaginary part.  

---

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File nsfg/eval/__init__.py; ROUND 6 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:21:15
**File Implemented**: nsfg/eval/__init__.py

**Core Purpose**  
Provides a convenient shortcut for importing the most‑used evaluation utilities. It re‑exports selected functions and classes from `nsfg.eval.metrics` so that users can access them directly via `nsfg.eval`.

**Public Interface**  
- **Function** `wasserstein2_distance(... )`: computes the 2‑Wasserstein distance between two point clouds. → `float` (distance).  
- **Function** `compute_fid(... )`: calculates the Fréchet Inception Distance for generated samples. → `float`.  
- **Function** `compute_inception_score(... )`: returns the Inception Score of a set of images. → `float`.  
- **Function** `evaluate_generated(... )`: runs a full evaluation pipeline (FID, IS, Wasserstein) on generated data. → `dict` (metrics).  
- **Class** `InceptionFeatureExtractor`: wrapper around a pretrained Inception‑v3 model for feature extraction. → methods include `__call__(images)` etc.  

All of the above are imported from `nsfg.eval.metrics` and listed in `__all__`.

**Internal Dependencies**  
- `from .metrics import ( wasserstein2_distance, compute_fid, compute_inception_score, evaluate_generated, InceptionFeatureExtractor )` – pulls the concrete implementations from the sibling `metrics.py` module.

**External Dependencies** (files that depend on this module)  
- Any user code or scripts that need evaluation utilities, e.g., training scripts, notebooks, or CLI tools that import `nsfg.eval` to call `wasserstein2_distance`, `compute_fid`, etc.

**Implementation Notes**  
- The module simply re‑exports symbols; no lazy‑loading logic is required beyond the import statements.  
- `__all__` is defined to control what gets imported when using `from nsfg.eval import *`.  
- This design keeps the public API stable even if `metrics.py` changes internally.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File nsfg/models/mlp.py; ROUND 7 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 11:21:18
**File Implemented**: nsfg/models/mlp.py

**Core Purpose**  
Defines a lightweight multilayer perceptron (`MLPVelocity`) that maps a 2‑D spatial coordinate and a scalar time to a 2‑D velocity vector. This model is used for the synthetic 2‑D experiments described in the paper.

---

**Public Interface**  

| Symbol | Type | Purpose | Key Methods / Signature | Constructor Params |
|--------|------|---------|--------------------------|--------------------|
| **Class** `MLPVelocity` | `torch.nn.Module` | Predicts a 2‑D velocity field from `(x, t)` inputs. | `forward(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor`<br>`_init_weights()` (private)<br>`__repr__()` | `hidden_dim: int = 256` – size of each hidden layer.<br>`num_hidden: int = 3` – number of hidden layers. |
| **Function** `forward` | method of `MLPVelocity` | Concatenates `x` (shape `[B,2]`) and `t` (shape `[B]` or `[B,1]`), passes through hidden layers with ReLU (except final layer) and returns velocity (shape `[B,2]`). | `forward(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor` | – |
| **Function** `_init_weights` | private method | Applies Xavier‑uniform initialization to all linear layers and zeros the biases. | `_init_weights() -> None` | – |
| **Function** `__repr__` | method | Human‑readable representation of the network configuration. | `__repr__() -> str` | – |

*No free functions or constants are exported.*

---

**Internal Dependencies**  

- `torch` – core tensor operations.  
- `torch.nn` – base class `nn.Module` and `nn.Linear`.  
- `torch.nn.functional as F` – ReLU activation.

---

**External Dependencies**  

- **Expected importers**: training and inference scripts that need a velocity model, e.g. `nsfg/train/train_nsfg.py`, `nsfg/eval/inference_nsfg.py`.  
- **Key exports used elsewhere**: the `MLPVelocity` class (instantiated with default or custom hidden dimensions).

---

**Implementation Notes**  

- **Architecture**:  
  - Input dimension = 3 (2‑D coordinate + scalar time).  
  - Three hidden layers of equal size (`hidden_dim`, default 256).  
  - ReLU activation after each hidden layer; linear output layer maps to 2‑D velocity.  
- **Weight Initialization**: Xavier uniform for weights, zero for biases, applied to every `nn.Linear` in the module.  
- **Time handling**: Accepts `t` as a 1‑D or 2‑D tensor; automatically unsqueezes to `[B,1]` for concatenation.  
- **Modularity**: The network is deliberately simple to keep the synthetic 2‑D experiments fast and memory‑efficient.

---

---
*Auto-generated by Memory Agent*


