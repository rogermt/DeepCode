```yaml
complete_reproduction_plan:
  paper_info:
    title: "Neural Sinkhorn Gradient Flow"
    core_contribution: >
      A neural network learns the time‑varying velocity field of the
      Wasserstein gradient flow defined by the Sinkhorn divergence.
      A two‑phase variant (NSGF++) first follows the learned Sinkhorn flow
      for ≤5 steps, then hands over to a learned straight‑line flow
      whose start time is predicted by a small CNN.  The approach needs
      only source and target samples, enjoys a mean‑field convergence
      guarantee, and dramatically reduces the number of function
      evaluations (NFEs) on high‑dimensional image data.

  # --------------------------------------------------------------------
  # SECTION 1: File Structure (≈850 chars)
  # --------------------------------------------------------------------
  file_structure: |
    nsfg/
    ├── data/                     # dataset loaders (MNIST, CIFAR‑10, synthetic 2‑D)
    │   ├── __init__.py
    │   ├── mnist.py
    │   ├── cifar.py
    │   └── synthetic.py
    ├── models/
    │   ├── __init__.py
    │   ├── unet.py              # UNet backbone for v_θ and u_δ
    │   ├── mlp.py               # simple MLP for 2‑D experiments
    │   └── predictor_cnn.py      # 4‑layer CNN for t_ϕ
    ├── utils/
    │   ├── __init__.py
    │   ├── sinkhorn.py          # wrapper around GeomLoss Sinkhorn potentials
    │   ├── ode_solver.py        # explicit Euler / RK4 helper
    │   └── replay_buffer.py     # trajectory‑pool implementation
    ├── train/
    │   ├── __init__.py
    │   ├── train_nsfg.py        # Algorithm 1 (velocity‑field matching)
    │   ├── train_nsfgpp.py      # Algorithm 3 (NSGF++ training)
    │   └── train_predictor.py   # training of t_ϕ
    ├── eval/
    │   ├── __init__.py
    │   ├── inference_nsfg.py    # Algorithm 2 (NSGF inference)
    │   ├── inference_nsfgpp.py  # Algorithm 4 (NSGF++ inference)
    │   └── metrics.py           # 2‑W distance, FID, IS calculators
    ├── scripts/
    │   ├── run_2d_experiment.sh
    │   ├── run_mnist.sh
    │   └── run_cifar.sh
    ├── configs/
    │   ├── nsfg_2d.yaml
    │   ├── nsfgpp_mnist.yaml
    │   └── nsfgpp_cifar.yaml
    ├── README.md                 # documentation – created last
    ├── requirements.txt
    └── setup.py                  # installs the package

  # --------------------------------------------------------------------
  # SECTION 2: Implementation Components (≈3400 chars)
  # --------------------------------------------------------------------
  implementation_components: |
    1. **Sinkhorn Potential Engine (utils/sinkhorn.py)**
       • Uses GeomLoss `SamplesLoss('sinkhorn', blur=β, scaling=α, epsilon=ε)`.
       • Provides `compute_potentials(x_src, x_tgt)` returning vectors `f_src,tgt`,
         `g_src,tgt`.  Gradients of `f` w.r.t. particle positions give
         ∇f needed for empirical velocity (Eq. 13).
       • Parameters:
         - ε (entropy regulariser, default 0.1)
         - blur ∈ {0.5,1.0,2.0} (chosen per dataset)
         - scaling ∈ {0.80,0.85,0.95}
       • Batch‑wise computation; supports autograd for back‑propagation.

    2. **Trajectory Pool (utils/replay_buffer.py)**
       • Implements a circular buffer storing tuples
         `(x, t, v̂)` for every step of the Sinkhorn flow during pool
         construction (Algorithm 1, line “store all … pair into the pool”).
       • Capacity configurable (e.g. 1500 batches for MNIST, 2500 for CIFAR‑10).
       • Provides `sample(batch_size)` for random minibatches during
         velocity‑field matching.

    3. **Velocity‑field Network v_θ (models/unet.py & models/mlp.py)**
       • **UNet variant** (image experiments):
         - Encoder/decoder depth 1 (MNIST) or 2 (CIFAR‑10) with channel
           multipliers `[1,2,2]` (MNIST) or `[1,2,2,2]` (CIFAR‑10).
         - 4 heads, head‑channel 64, attention at resolution 16×16.
         - GroupNorm + SiLU activation.
         - Time “t” injected by FiLM: a linear layer maps scalar t to
           per‑channel scale/bias, added after each residual block.
         - Final 1×1 conv outputs a vector field of same spatial shape
           as input (2‑D flow for point clouds, 3‑channel flow for images).
       • **MLP variant** (2‑D synthetic):
         - 3 hidden layers, 256 units each, ReLU.
         - Input: concatenated `(x, t)` (2‑D coordinate + scalar).
         - Output: 2‑D velocity.
       • Weight initialization: Xavier uniform; bias zero.

    4. **Phase‑transition Time Predictor t_ϕ (models/predictor_cnn.py)**
       • Small CNN:
         Conv(3→32,3×3, stride 1, pad 1) → ReLU → AvgPool2d(2)
         Conv(32→64) → ReLU → AvgPool2d(2)
         Conv(64→128) → ReLU → AvgPool2d(2)
         Conv(128→256) → ReLU → AvgPool2d(2)
         Flatten → Linear(256×H/16×W/16 → 1) → Sigmoid.
       • Trained on random linear interpolations:
         `X = t·Y + (1‑t)·X₀`, loss `L_ϕ = (t − t_ϕ(X))²` (Eq. L(ϕ) in paper).
       • Optimizer: Adam, lr = 1e‑4, batch = 128, ~40 k iterations.

    5. **Neural Straight Flow u_δ (models/unet.py re‑used)**
       • Same UNet architecture as v_θ.
       • Input: concatenation of scalar time `t` (broadcast) and interpolated
         sample `X_t`.
       • Target: displacement `Y − X₀`.
       • Loss `L_NSF = ‖u_δ(t,X_t) – (Y‑X₀)‖²` (Eq. LNSF).
       • Optimizer: Adam, lr = 1e‑4, similar schedule as v_θ.

    6. **Training Loop – NSGF (train/train_nsfg.py)**
       • **Pool construction**:
         for each batch:
           - Sample `X₀ ∼ µ₀` (Gaussian or noise) and `Y ∼ µ*` (training data).
           - For t = 0 … T:
               * Compute self‑potential `f_{µ_t,µ_t}` and cross‑potential
                 `f_{µ_t,µ*}` via `sinkhorn.compute_potentials`.
               * Empirical velocity `v̂ = ∇f_{µ_t,µ_t}(X_t) − ∇f_{µ_t,µ*}(X_t)`.
               * Euler step `X_{t+1} = X_t + η·v̂` (η default 0.1).
               * Store `(X_t, t, v̂)` in replay buffer.
       • **Velocity‑field matching**:
         repeatedly sample minibatches from the buffer,
         compute prediction `v_θ(X, t)`, MSE loss (Eq. 12),
         back‑propagate, update θ with Adam (γ = 1e‑4).
         Stop when validation loss plateaus (≈10‑20 k gradient steps).

    7. **Training Loop – NSGF++ (train/train_nsfgpp.py)**
       • Phase 1: reuse the NSGF training code with `T ≤ 5`.
       • Phase 2: train `t_ϕ` as described in component 4.
       • Phase 3: train `u_δ` on straight‑line interpolations (component 5).
       • All three models share the same data loader; training can be
         interleaved or sequential (paper uses sequential).

    8. **Inference – NSGF (eval/inference_nsfg.py)**
       • Input: prior batch `X₀`, learned `v_θ`, step size η, number of steps T.
       • Loop `X_{t+1} = X_t + η·v_θ(X_t, t)`.
       • Return final particles `X_T`.

    9. **Inference – NSGF++ (eval/inference_nsfgpp.py)**
       • Run NSGF phase (≤5 steps) → intermediate `X̂`.
       • Predict transition time `τ = t_ϕ(X̂)`.
       • Compute number of refinement steps `N = ceil((1‑τ)/ω)` (ω default 0.1).
       • Integrate `u_δ` from `τ` to 1 using explicit Euler (or RK4) with N steps.
       • Output refined samples.

    10. **Metrics (eval/metrics.py)**
        • 2‑Wasserstein distance: use `geomloss.SamplesLoss('energy', blur=0)`.
        • FID: pre‑trained Inception‑v3 from `torchvision.models`.
        • IS: same Inception model, compute KL between conditional class
          distribution and marginal.
        • NFEs: count of forward passes through each network during inference.

    11. **Configuration Files (configs/*.yaml)**
        • Store hyper‑parameters: batch sizes, learning rates, ε, blur, scaling,
          η, ω, T, number of trajectory batches, random seeds.

  # --------------------------------------------------------------------
  # SECTION 3: Validation & Evaluation (≈2200 chars)
  # --------------------------------------------------------------------
  validation_approach: |
    **Synthetic 2‑D Experiments**
    • Datasets: 8‑gaussians, moons, checkerboard, S‑curve (1024 test points).
    • Protocol: Build a trajectory pool with `T = 10` (for the 10‑step baseline) and
      `T = 100` (for the high‑step baseline).  Use batch size 256, η = 0.1.
    • Metrics: 2‑Wasserstein distance between generated samples and the true target.
      Target values taken from Table 1 (e.g. 0.285 ± 0.02 for 8‑gaussians at 10 steps).
    • Success criterion: reproduced distances lie within ±0.02 of reported numbers.

    **MNIST**
    • Train NSGF++ (T ≤ 5) on the MNIST training split; prior is standard Gaussian.
    • Hyper‑parameters follow `nsfgpp_mnist.yaml` (blur = 0.5, scaling = 0.85,
      η = 0.1, ω = 0.1, learning rate = 1e‑4 for all three networks).
    • Evaluate on 10 k generated samples:
      - FID ≤ 3.8 (paper reports 3.8)
      - IS ≥ 5.5 (paper reports 5.55)
      - NFEs ≈ 60 (≈5 NSGF + ≈55 NSF steps derived from τ predictions).
    • Visual sanity check: compute L2 nearest‑neighbour distances to training set;
      ensure generated images are not memorised (as in Fig 8).

    **CIFAR‑10**
    • Same training regimen, but UNet depth = 2, channels = 128, heads = 4.
    • Use `blur = 1.0`, `scaling = 0.90`, larger trajectory pool (2500 batches,
      ≈45 GB).
    • Metrics on 10 k samples:
      - FID ≤ 8.9 (paper 8.86)
      - IS ≥ 5.5 (paper 5.55)
      - NFEs ≈ 59 (5 NSGF steps + ≈54 NSF steps).
    • Compare side‑by‑side with baselines (JKO‑Flow, EPT, FM, OT‑CFM) using
      the same FID/IS/NFE reporting format.

    **Ablation Studies**
    • Remove phase‑transition predictor: run only NSGF (5 steps) → expect FID > 10.
    • Remove NSF: after NSGF, directly output `X̂` → FID degrades by ~2‑3.
    • Vary T∈{1,3,5} to observe trade‑off between NFEs and sample quality.
    • Random seed control: set `torch.manual_seed(0)` and `np.random.seed(0)` for
      reproducibility; log seeds in each run.

    **Stability Checks**
    • Plot loss curves for `L(θ)`, `L_ϕ`, and `L_NSF`; ensure monotonic descent.
    • Verify that the empirical velocity field stored in the replay buffer
      matches the analytic Sinkhorn velocity by computing the norm of the
      residual `‖v_θ − v̂‖` on a held‑out batch (should be < 1e‑3 after training).

  # --------------------------------------------------------------------
  # SECTION 4: Environment & Dependencies (≈900 chars)
  # --------------------------------------------------------------------
  environment_setup: |
    - **Operating System**: Linux (Ubuntu 20.04) or macOS 12+.
    - **Python**: 3.9‑3.10 (tested on 3.9).
    - **Core libraries**:
        * torch==1.12.1 (CUDA 11.7) – for GPU acceleration.
        * geomloss==0.2.4 – Sinkhorn potentials (GPU support).
        * torchvision==0.13.1 – Inception‑v3 for FID/IS.
        * tqdm, numpy, scipy, matplotlib – utilities.
    - **Hardware**: Single NVIDIA RTX‑3090 (24 GB). 2‑D experiments run on CPU,
      but GPU greatly speeds up Sinkhorn.
    - **Installation**:
        ```bash
        conda create -n nsfg python=3.9
        conda activate nsfg
        pip install -r requirements.txt
        ```
    - **Optional**: `torchdiffeq` if you wish to experiment with higher‑order
      ODE solvers; not required for the baseline implementation.

  # --------------------------------------------------------------------
  # SECTION 5: Implementation Strategy (≈1700 chars)
  # --------------------------------------------------------------------
  implementation_strategy: |
    **Phase 0 – Project Bootstrap**
    1. Clone the repo skeleton, run `pip install -e .` to expose modules.
    2. Verify GPU visibility: `torch.cuda.is_available()`.
    3. Pull the MNIST and CIFAR‑10 datasets via torchvision; generate the synthetic
       2‑D clouds with `data/synthetic.py`.

    **Phase 1 – Core Sinkhorn Engine & Replay Buffer**
    • Implement `utils/sinkhorn.py` wrapping GeomLoss.  Write unit tests that
      for a small batch (n = 4) the gradients of `f` match finite‑difference
      estimates of `∇f`.
    • Implement `utils/replay_buffer.py` with `add` and `sample` methods; test
      that after `capacity` inserts older entries are overwritten.

    **Phase 2 – Velocity‑field Network & Training Loop**
    • Code `models/mlp.py` (2‑D) and `models/unet.py` (image) with FiLM
      time conditioning.
    • Write `train/train_nsfg.py`:
        - Build pool once (outer loop) using the sinkhorn engine.
        - Store `(x, t, v̂)` tensors on GPU for fast sampling.
        - Training: mini‑batch MSE loss, Adam (lr = 1e‑4), gradient clipping = 1.0
          (optional).
        - Early‑stop when validation loss does not improve for 500 steps.
    • Validate by reproducing the 2‑D 10‑step result (W2 distance ∼0.28).

    **Phase 3 – NSGF++ Sub‑components**
    • **Time predictor**: implement `models/predictor_cnn.py`; train via
      `train/train_predictor.py`.  Use uniform sampling of `t∈U(0,1)`.
    • **Straight‑flow network**: reuse UNet from Phase 2; train with
      `train/train_nsfgpp.py`’s third block (`L_NSF`).
    • Integrate all three blocks in the same script; after each sub‑model
      reaches convergence, save checkpoints (`ckpt_vθ.pt`, `ckpt_phi.pt`,
      `ckpt_u.pt`).

    **Phase 4 – Inference Pipelines**
    • Implement `eval/inference_nsfg.py` (Algorithm 2) – expose a CLI
      `python -m eval.inference_nsfg --steps 10 --eta 0.1`.
    • Implement `eval/inference_nsfgpp.py` (Algorithm 4).  The ODE solver
      loops over `N = ceil((1‑τ)/ω)` steps, applying `u_δ` each iteration.
    • Add a wrapper script `scripts/run_cifar.sh` that loads checkpoints,
      generates 10 k samples, computes FID/IS/NFE, and writes a JSON report.

    **Phase 5 – Reproducibility & Reporting**
    • Fix random seeds at the beginning of every script (`torch.manual_seed`,
      `np.random.seed`, `random.seed`).
    • Log hyper‑parameters, training curves (TensorBoard), and final metrics.
    • Cross‑check storage usage: ensure replay buffer does not exceed the
      advertised 20 GB / 45 GB limits; if exceeded, enable on‑the‑fly
      deletion of oldest batches.
    • Compare results against the validation checklist (Section 3).  If any
      metric deviates beyond tolerance, iteratively adjust learning rates,
      ε/blur parameters, or increase pool size.

    **Phase 6 – Optional Extensions**
    • Replace explicit Euler with RK4 in `utils/ode_solver.py` to test higher‑order
      integration (should not affect reported numbers).
    • Experiment with different entropy regularisation ε (e.g., 0.05, 0.2) to see
      sensitivity on image quality.
    • Publish the final code with a README containing full usage examples,
      GPU requirements, and command‑line flags for reproducing every table
      in the paper.

```