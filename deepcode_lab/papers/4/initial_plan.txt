```yaml
complete_reproduction_plan:
  paper_info:
    title: "Neural Sinkhorn Gradient Flow (NSGF) & NSGF++"
    core_contribution: |
      A neural‑parameterized, time‑varying velocity field that learns the Sinkhorn‑regularized Wasserstein gradient flow (NSGF). 
      NSGF++ augments NSGF with a learned phase‑transition time predictor and a Neural Straight Flow (NSF) that refines samples after a few Sinkhorn steps, drastically reducing the number of function evaluations (NFEs) while preserving image quality.

  # ----------------------------------------------------------------------
  # SECTION 1 – File Structure (≈850 chars)
  # ----------------------------------------------------------------------
  file_structure: |
    nsfg/
    ├─ data/
    │   ├─ __init__.py
    │   ├─ synthetic.py          # 2‑D toy distributions
    │   ├─ mnist.py
    │   └─ cifar10.py
    ├─ models/
    │   ├─ __init__.py
    │   ├─ unet.py               # conditional UNet (velocity & NSF)
    │   ├─ mlp.py                # low‑dim MLP alternative
    │   ├─ phase_cnn.py          # tφ predictor
    │   └─ sinkhorn_engine.py    # GeomLoss wrapper for potentials & grads
    ├─ training/
    │   ├─ __init__.py
    │   ├─ trainer_nsgf.py       # Alg.1 – trajectory pool + velocity matching
    │   ├─ trainer_nsgfpp.py     # Alg.3 – three‑stage NSGF++ training
    │   └─ utils.py              # checkpointing, logger, seed handling
    ├─ inference/
    │   ├─ __init__.py
    │   ├─ infer_nsgf.py         # Alg.2 – single‑phase sampling
    │   ├─ infer_nsgfpp.py       # Alg.4 – two‑phase NSGF++ sampling
    │   └─ ode_solver.py         # Euler (default) & RK4 optional
    ├─ experiments/
    │   ├─ configs/
    │   │   ├─ nsgf_2d.yaml
    │   │   ├─ nsgfpp_mnist.yaml
    │   │   └─ nsgfpp_cifar.yaml
    │   ├─ scripts/
    │   │   ├─ run_2d.sh
    │   │   ├─ run_mnist.sh
    │   │   └─ run_cifar.sh
    │   └─ notebooks/
    │       └─ analysis.ipynb
    ├─ metrics/
    │   ├─ __init__.py
    │   ├─ frechet_inception.py   # FID
    │   ├─ inception_score.py
    │   └─ wasserstein2.py        # 2‑W distance for synthetic data
    ├─ requirements.txt
    ├─ README.md
    └─ setup.py
    # Note: README.md and requirements.txt are created after core code is functional.

  # ----------------------------------------------------------------------
  # SECTION 2 – Implementation Components (≈3600 chars)
  # ----------------------------------------------------------------------
  implementation_components: |
    ### 2.1 Sinkhorn Engine (utils/sinkhorn_engine.py)
    - Wraps `geomloss.SamplesLoss("sinkhorn", p=2, blur=..., scaling=..., epsilon=...)`.
    - `compute_potentials(src, tgt)` returns gradients ∇f_src and ∇f_tgt using autograd.
    - Parameters: `blur ∈ {0.5,1.0,2.0}`, `scaling ∈ {0.80,0.85,0.95}`, `eps` = entropy regularisation (defaults to 0.01).
    - Provides `empirical_velocity(x_src, x_tgt)` implementing Eq.(13): `v̂ = ∇f_self - ∇f_cross`.

    ### 2.2 Velocity‑Field Network (models/unet.py & models/mlp.py)
    - **UNet** (for image data) conditioned on time `t`. Time is broadcast as an extra channel concatenated to the input before the first conv layer.
    - Configurable via YAML: `channels`, `depth`, `channel_mult`, `attention_res`, `dropout`.
    - **MLP** (for 2‑D synthetic data): 3 hidden layers, 256 units each, ReLU, final linear projection to `d`‑dim vector field.
    - Forward signature: `forward(x, t) → v_θ(x,t)`.

    ### 2.3 Phase‑Transition Predictor (models/phase_cnn.py)
    - Small 4‑layer CNN with progressive average‑pooling.
    - Architecture: Conv(3→32)‑ReLU‑Pool → Conv(32→64)‑ReLU‑Pool → Conv(64→128)‑ReLU‑Pool → Conv(128→256)‑ReLU → Flatten → Linear → Sigmoid.
    - Outputs scalar `t̂ ∈ [0,1]`.
    - Trained with MSE loss: `L(φ)=E[(t - t̂)^2]`.

    ### 2.4 Neural Straight Flow Network (models/unet.py reused)
    - Same UNet backbone as velocity net but learns `u_δ(t,x)`.
    - Training target: the *direction vector* `y - x` for straight‑line interpolation.
    - Loss: `L_NSF = E[||u_δ(t,x) - (y - x)||²]`.

    ### 2.5 Replay Buffer (utils/replay_buffer.py)
    - Circular buffer storing tuples `(x, t, v̂)`.
    - Capacity defined in config (default 1500 batches → ~20 GB for MNIST, ~45 GB for CIFAR‑10).
    - `sample(batch_size)` returns random minibatch for the velocity‑field loss.

    ### 2.6 Trainer – NSGF (training/trainer_nsgf.py) – Algorithm 1
    1. **Trajectory‑pool construction**  
       ```python
       X = sample_prior(n)
       Y = sample_target(n)
       for step in range(T+1):
           f_self, _ = sinkhorn.compute(X, X)
           f_cross, _ = sinkhorn.compute(X, Y)
           v_hat = f_self - f_cross
           buffer.add(X, step/T, v_hat)   # store (x, normalized t, v̂)
           X = X + η * v_hat               # explicit Euler update
       ```
    2. **Velocity‑field matching**  
       ```python
       for iter in range(max_iters):
           x_batch, t_batch, v_batch = buffer.sample(n)
           v_pred = velocity_net(x_batch, t_batch)
           loss = ((v_pred - v_batch)**2).mean()
           optimizer.zero_grad(); loss.backward(); optimizer.step()
       ```
    - Optimizer: Adam(lr=1e‑4), β=(0.9,0.999), optional grad‑clip ‖∇‖≤1.

    ### 2.7 Trainer – NSGF++ (training/trainer_nsgfpp.py) – Algorithm 3
    *Stage 1 (NSGF, ≤5 NFEs):* reuse `trainer_nsgf` with `T≤5`.  
    *Stage 2 (Phase predictor):*  
       ```python
       x0 = sample_prior(n); y = sample_target(n)
       t = torch.rand(n,1)                         # U(0,1)
       xt = t*y + (1-t)*x0                          # straight interpolation
       t_pred = phase_cnn(xt)
       loss = ((t - t_pred)**2).mean()
       ```
    *Stage 3 (Neural Straight Flow):* same as velocity net but with target `y - x0`.  
    - Each stage has its own optimizer (Adam, lr=1e‑4). Training epochs ≈40 k iterations per stage (≈20 min on RTX‑3090).

    ### 2.8 Inference – NSGF (inference/infer_nsgf.py) – Algorithm 2
    ```python
    x = sample_prior(N)
    for step in range(T):
        t = step / T
        v = velocity_net(x, t)
        x = x + η * v
    return x
    ```
    - `η = 1/T` (default) or user‑specified.

    ### 2.9 Inference – NSGF++ (inference/infer_nsgfpp.py) – Algorithm 4
    1. **NSGF phase:** run ≤5 Euler steps as above.  
    2. **Predict transition time:** `t_hat = phase_cnn(x)` (clamped to [0,1]).  
    3. **NSF refinement:** integrate from `t_hat` to 1 using step size `ω` (default 0.02).  
       ```python
       s = t_hat.clone()
       while (s < 1.0).any():
           u = nsf_net(x, s)
           x = x + ω * u
           s = s + ω
       ```
    - Solver can be swapped for `torchdiffeq.odeint` for higher‑order integration.

    ### 2.10 Metrics (metrics/*)
    - **FID / IS:** wrappers around `torch-fidelity` (`compute_fid(real, fake)`, `compute_is(fake)`).  
    - **2‑Wasserstein for synthetic data:** `SamplesLoss("sinkhorn", p=2, blur=1.0)(real, fake)`.  
    - All metrics logged via TensorBoard / Weights & Biases.

    ### 2.11 Utilities (utils/*)
    - `logger.py`: sets up TensorBoard, optional WandB, CSV dump.  
    - `checkpoint.py`: `save_checkpoint(state, path)` and `load_checkpoint(path)`.  
    - `seed.py`: deterministic seed handling.

  # ----------------------------------------------------------------------
  # SECTION 3 – Validation & Evaluation (≈2200 chars)
  # ----------------------------------------------------------------------
  validation_approach: |
    **Synthetic 2‑D experiments**  
    - Datasets: mixture of Gaussians, moons, Swiss roll (as in paper Fig. 3‑4).  
    - Train NSGF with `T=10` (10 steps) and `T=100` (100 steps).  
    - Compute 2‑Wasserstein distance between generated particles and ground‑truth target distribution every 2 k iterations. Expected: < 0.30 for `T≤10`, < 0.15 for `T=100`.  
    - Visualise trajectories (quiver plots) and compare with paper’s Figures 3‑4.

    **MNIST NSGF++**  
    - Train with config `nsgfpp_mnist.yaml`: `T=5`, `η=0.3`, `ω=0.02`, blur = 1.0, scaling = 0.85.  
    - After training, generate 10 k samples and compute:  
      - FID ≈ 3.8 (±0.2)  
      - IS ≈ 9.5 (±0.3)  
      - NFEs ≈ 60 (≤ 5 for NSGF + ~55 for NSF).  
    - Verify phase predictor outputs lie in `[0,1]` (histogram should peak near the optimal transition time reported in the paper).

    **CIFAR‑10 NSGF++**  
    - Config `nsgfpp_cifar.yaml`: `T=5`, `η=0.3`, `ω=0.02`, same blur/scaling.  
    - Generate 50 k samples; expected metrics:  
      - FID ≈ 5.5 (±0.3)  
      - IS ≈ 8.9 (±0.2)  
      - NFEs ≈ 59 (≈ 5 for NSGF, rest NSF).  
    - Compare generated image grids with paper’s Figures 9‑11.

    **Ablation checks**  
    - Remove phase predictor (run NSGF alone) → NFEs rise > 100 for comparable FID/IS.  
    - Vary `blur` (0.5 → 2.0) to confirm robustness.  
    - Replace Euler with RK4 in the NSF phase – expect negligible metric change but verify numerical stability.

    **Success criteria**  
    - All reported numbers fall within ±5 % of the paper’s tables.  
    - Visual plots (particle trajectories, generated image grids) are qualitatively indistinguishable from the paper’s figures.  
    - Training curves (loss vs. iteration) show monotonic decrease and plateau similarly to the original.

  # ----------------------------------------------------------------------
  # SECTION 4 – Environment & Dependencies (≈900 chars)
  # ----------------------------------------------------------------------
  environment_setup: |
    - **OS:** Linux (Ubuntu 20.04+ recommended)  
    - **Python:** 3.10  
    - **Core libraries:**  
      ```text
      torch==2.3.0+cu121
      torchvision==0.18.0
      geomloss==0.2.6          # Sinkhorn potentials
      torch-fidelity==0.3.0   # FID/IS
      tqdm==4.66.1
      tensorboard==2.16.2
      wandb==0.17.0           # optional logging
      matplotlib==3.8.2
      numpy==1.26.4
      pandas==2.2.2
      omegaconf==2.3.0        # config handling
      ```  
    - **Hardware:** Single NVIDIA GPU with ≥ 12 GB VRAM (RTX 3090 used in paper).  
    - **Installation steps:**  
      ```bash
      conda create -n nsgf python=3.10
      conda activate nsgf
      pip install -r requirements.txt
      ```  
    - **Special notes:** GeomLoss must be compiled with CUDA support; verify with `python -c "import geomloss; print(geomloss.__version__)"`.  
    - **Reproducibility:** Set `torch.backends.cudnn.benchmark = True`, enforce deterministic ops via `torch.use_deterministic_algorithms(True)` when needed.

  # ----------------------------------------------------------------------
  # SECTION 5 – Implementation Strategy (≈1700 chars)
  # ----------------------------------------------------------------------
  implementation_strategy: |
    **Phase 0 – Setup & Verification**  
    1. Clone the repository skeleton, create the conda environment.  
    2. Run a minimal unit test for `sinkhorn_engine`: compute potentials on a 2‑D batch (n=8) and check that gradients have the correct shape.  
    3. Verify that the UNet forward pass works for both image (CIFAR‑10, 3×32×32) and 2‑D data (shape (n,2)).

    **Phase 1 – Core NSGF (Algorithm 1)**  
    - Implement `trainer_nsgf.py` exactly as described; store trajectory pool on disk (`torch.save`).  
    - Hyper‑parameters: `T=10` for synthetic, `η=1/T`; `blur=1.0`, `scaling=0.85`.  
    - Train until the velocity‑field loss plateaus (≈ 20 k iterations).  
    - Save `v_theta.ckpt` and the final replay buffer.

    **Phase 2 – Phase‑Predictor & NSF (Algorithm 3)**  
    - Using the checkpoint from Phase 1, launch `trainer_nsgfpp.py`.  
    - Stage 2 (predictor) runs for 40 k iterations, MSE loss on uniformly sampled `t`.  
    - Stage 3 (NSF) shares the same UNet code; train with the straight‑line target `y‑x`.  
    - Validate each sub‑stage by checking loss curves; ensure `t̂` stays within `[0,1]`.  

    **Phase 3 – End‑to‑End NSGF++ Evaluation**  
    - Run `infer_nsgfpp.py` with the three checkpoints; generate the required number of samples (10 k for MNIST, 50 k for CIFAR‑10).  
    - Compute FID/IS via `metrics/frechet_inception.py`; compute 2‑W distance for synthetic data.  
    - Store generated images in `samples/` and log metrics to `metrics.json`.  

    **Phase 4 – Ablations & Hyper‑parameter Sweeps**  
    - Repeat training with `blur` set to 0.5 and 2.0 to confirm robustness.  
    - Disable the phase predictor (run only NSGF) to measure NFEs growth.  
    - Optionally replace Euler with RK4 in the NSF phase (`torchdiffeq.odeint`) to ensure numerical stability.

    **Phase 5 – Documentation & Release**  
    - Write comprehensive `README.md` covering: environment setup, command‑line usage, expected results, and how to reproduce each table/figure.  
    - Include the YAML config files, bash scripts, and a Jupyter notebook (`analysis.ipynb`) that reproduces all plots.  
    - Tag the repository with `v1.0` and push to GitHub; optionally create a Dockerfile for containerised execution.

    **Risk Mitigation**  
    - *Memory:* If the trajectory pool exceeds GPU RAM, keep it on CPU and use `torch.utils.data.DataLoader` with `pin_memory=True`.  
    - *Numerical Instability:* When `blur` is very low (0.5) gradients can explode; clip velocities to `||v||≤5`.  
    - *Reproducibility:* Fix seeds (`torch.manual_seed`, `np.random.seed`, `random.seed`) and store the seed in each checkpoint.

    By following this staged roadmap, a single developer can go from an empty repo to a fully reproducible implementation that matches all quantitative and qualitative results reported in the NSGF and NSGF++ papers.