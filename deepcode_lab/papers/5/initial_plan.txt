```yaml
complete_reproduction_plan:
  paper_info:
    title: "Neural Sinkhorn Gradient Flow (NSGF / NSGF++)"
    core_contribution: |
      The paper proposes a neural parametrisation of the time‑varying velocity
      field of the entropy‑regularised (Sinkhorn) Wasserstein gradient flow.
      Training is performed by *velocity‑field matching* using only source and
      target samples.  A two‑phase variant (NSGF++) first follows a few
      Sinkhorn‑flow steps (≤5) and then refines samples with a *Neural Straight
      Flow* (NSF) driven by a learned phase‑transition time predictor
      tₚₕᵢ(x).  The method out‑performs prior gradient‑flow and flow‑matching
      baselines on 2‑D synthetic benchmarks and image generation (MNIST,
      CIFAR‑10).

  # --------------------------------------------------------------
  # SECTION 1: File Structure Design
  # --------------------------------------------------------------
  file_structure: |
    neural_sinkhorn_gf/
    ├── configs/
    │   ├── toy.yaml                 # 2‑D synthetic experiments
    │   ├── mnist.yaml               # MNIST training/inference
    │   └── cifar10.yaml             # CIFAR‑10 training/inference
    ├── data/
    │   ├── __init__.py
    │   ├── synthetic.py             # dataset generators for 8‑gaussians, moons…
    │   ├── image.py                 # torchvision wrappers for MNIST/CIFAR‑10
    │   └── samplers.py              # generic batch sampler & replay buffer utils
    ├── utils/
    │   ├── __init__.py
    │   ├── seed.py                  # set deterministic seeds
    │   ├── logger.py                # MetricTracker + TensorBoard helper
    │   ├── metrics.py               # FID, IS, 2‑W distance helpers
    │   └── viz.py                   # trajectory / sample visualisation utilities
    ├── sinkhorn/
    │   ├── __init__.py
    │   ├── potentials.py            # thin wrapper around GeomLoss SinkhornLoss
    │   └── grad.py                  # autograd helpers for ∇f_{µ,ν}
    ├── models/
    │   ├── __init__.py
    │   ├── unet.py                  # UNet backbone (time‑conditioned & plain)
    │   ├── time_predictor.py        # CNN that predicts transition time tₚₕᵢ(x)
    │   └── mlp3.py                  # small MLP used for 2‑D experiments
    ├── train/
    │   ├── __init__.py
    │   ├── train_nsgf.py            # Algorithm 1 – velocity‑field matching
    │   ├── train_nsgfpp.py          # Algorithm 3 – two‑phase NSGF++ training
    │   └── trainer.py               # generic trainer (logging, checkpointing, EMA)
    ├── infer/
    │   ├── __init__.py
    │   ├── sample_nsgf.py           # Algorithm 2 – NSGF inference (Euler)
    │   └── sample_nsgfpp.py         # Algorithm 4 – NSGF++ inference pipeline
    ├── experiments/
    │   ├── __init__.py
    │   ├── eval_metrics.py          # compute FID/IS/2‑W; produce tables
    │   └── plot_figures.ipynb       # reproduces all paper figures
    ├── scripts/
    │   ├── run_toy.sh               # end‑to‑end 2‑D training & evaluation
    │   ├── run_mnist.sh
    │   ├── run_cifar10.sh
    │   ├── sample_mnist.sh
    │   └── sample_cifar10.sh
    ├── logs/                        # TensorBoard logs & stdout
    ├── checkpoints/                 # model weights for v_θ, t_φ, u_δ
    ├── requirements.txt
    ├── README.md                    # usage, hyper‑parameters, FAQ
    └── Dockerfile                   # optional reproducible container

  # --------------------------------------------------------------
  # SECTION 2: Implementation Components
  # --------------------------------------------------------------
  implementation_components: |
    ### 1. Core Algorithms
    * **Sinkhorn Potentials Computation** (sinkhorn/potentials.py)  
      - Wrapper around `geomloss.SamplesLoss("sinkhorn", p=2, blur=ε, scaling=σ)`.  
      - Returns dual potentials f(x) and g(y) together with autograd‑compatible
        gradients ∇f and ∇g needed for the velocity field.
    * **Velocity‑field Matching (Alg 1)** (train/train_nsgf.py)  
      - Build a *trajectory pool* by repeatedly applying the empirical Sinkhorn
        velocity `v̂_t = ∇f_{µ_t,µ*} - ∇f_{µ_t,µ_t}` (Eq. 13).  
      - Optimize a neural network `v_θ(x,t)` (UNet or MLP) to minimise  
        `L(θ)=E[‖v_θ - v̂_t‖²]` (Eq. 12) using Adam, gradient clipping, EMA.
    * **Two‑Phase NSGF++ (Alg 3)** (train/train_nsgfpp.py)  
      - Phase 1: same as Alg 1 but limited to ≤5 Euler steps (small NFE).  
      - Phase 2a: **Time‑Predictor CNN** `t_φ(x)` learns a regression onto uniformly
        sampled transition times `t∈[0,1]`.  
      - Phase 2b: **Neural Straight Flow (NSF)** network `u_δ(x)` (UNet without
        time conditioning) learns the constant displacement `Y₀−X₀` on linear
        interpolation points `X_t = (1‑t)X₀ + tY₀`.  
      - Joint optimisation of `v_θ`, `t_φ`, and `u_δ` in a multi‑loss setting.
    * **Inference Pipelines** (infer/*.py)  
      - **NSGF**: explicit Euler integration of `v_θ` over the user‑defined number
        of steps (Algorithm 2).  
      - **NSGF++**: (Algorithm 4) – run NSGF phase, predict `t̂ = t_φ(X)`,
        then integrate NSF from `t̂` to `1` using a fixed step size (Euler or a
        single closed‑form step because NSF is time‑independent).

    ### 2. Neural Architectures
    * **UNet (time‑conditioned)** – same backbone as DDPM/Stable Diffusion.
      - Image mode (CIFAR‑10): base_channels=128, depth=2, channel_mult=[1,2,2,2],
        heads=4, head_channels=64, attention at resolution 16, dropout=0.0.
      - MNIST mode: base_channels=32, depth=1, channel_mult=[1,2,2], heads=1.
      - Time embedding: 128‑dim sinusoidal → MLP (2 layers) → FiLM addition to each
        residual block.
    * **UNetStraight** – identical UNet but **without** time conditioning
      (used for NSF).
    * **Time‑Predictor CNN** – 4‑conv layers (32→64→128→256 filters, 3×3 kernels,
      ReLU), 2×2 average pooling, FC → scalar → sigmoid for `[0,1]`.
    * **MLP3** – 3 hidden layers of 256 units, ReLU, input `(x, t)`,
      output velocity vector; used for all 2‑D synthetic experiments.

    ### 3. Training Utilities
    * **TrajectoryPool** (data/samplers.py) – in‑memory list or HDF5‑backed store
      of `(x_t, v̂_t, t)` tuples; supports random sampling for experience replay.
    * **EMA** – exponential moving average of `v_θ` parameters (decay=0.999) for
      more stable sampling.
    * **Learning‑rate Schedules** – cosine annealing after an optional warm‑up of
      5 k steps; defaults are `γ=3e‑4` for `v_θ`, `γ' = γ'' = 1e‑4` for `t_φ` and
      `u_δ`.
    * **Logging** – per‑epoch loss, NFE count, FID/IS/2‑W metrics; TensorBoard
      support via `utils/logger.py`.

    ### 4. Evaluation Suite
    * **2‑Wasserstein Distance** – GeomLoss Sinkhorn with a very small blur
      (`ε≈1e‑6`) to approximate the exact 2‑W; computed on held‑out test set vs.
      generated particles.
    * **FID / IS** – `torchmetrics` or `torch_fidelity` with the official
      Inception‑v3 model; evaluate on 10 k generated images.
    * **NFE (Number of Function Evaluations)** – tracked by counting velocity
      network calls during inference; reported for every experiment.

  # --------------------------------------------------------------
  # SECTION 3: Validation & Evaluation
  # --------------------------------------------------------------
  validation_approach: |
    **Goal:** Verify that the implementation reproduces all quantitative
    results (Table 1‑2) and qualitative figures (Fig. 1‑8) of the paper.

    1. **2‑D Synthetic Benchmarks**  
       - Datasets: 8‑gaussians, moons, S‑curve, checkerboard (generated via
         `data/synthetic.py`).  
       - Train NSGF (MLP) for 10 k and 100 k Euler steps.  
       - Compute 2‑W distance against a high‑sample test set (10 k points).  
       - Expected errors: ≤ 0.03 for the 5‑step NSGF, ≤ 0.01 for NSGF++ (as in
         Table 1).

    2. **MNIST Generation**  
       - Train NSGF (UNet) with ≤ 5 steps; then train `t_φ` and NSF.  
       - Generate 10 k samples, compute FID ≤ 4.0 and IS ≈ 9.5 (paper’s values).  
       - Verify NFE ≈ 60 (5 NSGF steps + 1 NSF refinement step).  
       - Ablation: remove `t_φ` → FID should degrade by > 2.0.

    3. **CIFAR‑10 Generation**  
       - Follow the exact hyper‑parameters (blur=1.0, scaling=0.85, 5 NSGF steps).  
       - After full NSGF++ training, generate 50 k samples, compute FID ≈ 5.55,
         IS ≈ 8.86, NFE ≈ 59 (matches Table 2).  
       - Check that the trajectory visualisations (Fig. 2‑4) produced by
         `experiments/plot_figures.ipynb` align with the paper’s arrows.

    4. **Memory & Speed Checks**  
       - Ensure trajectory pool occupies ≤ 45 GB for CIFAR‑10 (as reported).  
       - Confirm total training time ≤ 2 h on an RTX 3090 for CIFAR‑10.

    5. **Reproducibility**  
       - Set random seeds (torch, numpy, python) to `2024`.  
       - Run each script twice; numeric results (FID, IS, 2‑W) must match within
         ±0.1% variance.

  # --------------------------------------------------------------
  # SECTION 4: Environment & Dependencies
  # --------------------------------------------------------------
  environment_setup: |
    - **OS**: Linux (Ubuntu 20.04 or newer) – tested on WSL2 as well.  
    - **Python**: 3.11 (>=3.10, <3.12).  
    - **Core libraries**:  
        torch==2.2.0 (CUDA 12.1), torchvision==0.17.0, geomloss==0.2.5,  
        torchmetrics==1.3.0, tqdm, numpy, scipy, h5py (for large pools).  
    - **Optional for evaluation**: torch_fidelity (for FID/IS), matplotlib,
        seaborn (for figures).  
    - **Hardware**: One GPU with ≥ 12 GB VRAM (RTX 3090 recommended), ≥ 64 GB RAM
        for the trajectory pool; CPU with AVX2 support.  
    - **Installation** (conda):  
        ```bash\nconda create -n nsgf python=3.11 -y\nconda activate nsgf\npip install -r requirements.txt\n```  
    - **CUDA**: Ensure `nvcc --version` reports 12.x; PyTorch should detect the
        correct GPU automatically.

  # --------------------------------------------------------------
  # SECTION 5: Implementation Strategy
  # --------------------------------------------------------------
  implementation_strategy: |
    **Phase 0 – Repository Setup (Day 1)**  
    - Create the directory tree exactly as described in *file_structure*.  
    - Write `README.md` with quick‑start commands and placeholder sections for
      results.  
    - Add basic utility modules (`seed.py`, `logger.py`) and install all
      dependencies in a fresh Conda environment.

    **Phase 1 – Sinkhorn Core (Day 2‑3)**  
    - Implement `sinkhorn/potentials.py` as a thin wrapper around GeomLoss.
    - Write a unit test that compares autograd gradients of the potentials with
      finite‑difference approximations on a tiny 2‑D batch.  
    - Verify that `sinkhorn_gradient` returns vectors of correct shape and dtype.

    **Phase 2 – 2‑D Prototype (Day 4‑5)**  
    - Implement `models/mlp3.py` and `train/train_nsgf.py` for the toy setting.  
    - Train on the 8‑gaussians dataset using the provided `toy.yaml`.  
    - Run `scripts/run_toy.sh`; check that the 2‑W distance matches the target
      (< 0.03).  
    - Visualise trajectories via `experiments/plot_figures.ipynb`.  
    - This phase validates the full pipeline (pool generation, matching loss,
      Euler integration).

    **Phase 3 – Image‑Scale NSGF (Day 6‑9)**  
    - Port the MLP architecture to `models/unet.py` (time‑conditioned version).  
    - Implement the trajectory pool for high‑dim data (`data/samplers.py` with
      optional HDF5 storage).  
    - Train NSGF on MNIST (`scripts/run_mnist.sh`): verify loss curves and
      checkpoint saving.  
    - Confirm that the NFE count stays ≤ 5 during inference.

    **Phase 4 – NSGF++ Components (Day 10‑13)**  
    - Implement `models/time_predictor.py` (CNN) and the plain UNet for NSF.  
    - Extend `train/train_nsgfpp.py` to jointly train the three networks:
      - First, load the previously trained `v_θ` checkpoint.  
      - Second, train `t_φ` on randomly sampled interpolation pairs.  
      - Third, train `u_δ` on the same pairs to learn the constant displacement.
    - Use the learning‑rate schedules supplied in `configs/mnist.yaml` and
      `configs/cifar10.yaml`.  
    - Perform early‑stop based on validation FID (computed every 5 k steps).

    **Phase 5 – Full CIFAR‑10 Run (Day 14‑18)**  
    - Switch to CIFAR‑10 config (larger UNet, bigger batch).  
    - Build the trajectory pool (≈ 2 500 mini‑batches, ≈ 45 GB).  
    - Run the complete NSGF++ training (`scripts/run_cifar10.sh`).  
    - Monitor GPU memory; if needed, enable the HDF5 streaming mode.  
    - After training, invoke `scripts/sample_cifar10.sh` to generate 50 k samples
      and compute FID/IS/NFE.  
    - Compare against the target numbers (FID ≈ 5.55, IS ≈ 8.86, NFE ≈ 59).

    **Phase 6 – Validation & Reporting (Day 19‑20)**  
    - Execute `experiments/eval_metrics.py` to generate the exact tables
      (Table 1‑2) and figures (Fig. 1‑8).  
    - Run the validation checklist (see section 3) automatically via a
      `bash validate.sh` script.  
    - Update `README.md` with final commands and expected run‑times.

    **Phase 7 – Packaging (Day 21)**  
    - Write a Dockerfile that installs the exact Python version and all pip
      packages, copies the repo, and sets the entrypoint to `bash run_toy.sh`.  
    - Add `requirements.txt` with pinned versions, and a `Makefile` with shortcuts
      (`make train_mnist`, `make eval_cifar10`).  
    - Push the repository to a public GitHub account and optionally upload the
      Docker image to Docker Hub.

    Throughout all phases, adopt the following *defensive programming* practices:
    - All random generators are seeded at the start of each script (`set_seed`).
    - Use `torch.backends.cudnn.deterministic = True` and
      `torch.backends.cudnn.benchmark = False` for reproducibility.
    - Wrap every heavy computation in `torch.cuda.amp.autocast()` (optional) but
      keep gradients in `float32` for stability.
    - Log hyper‑parameters, versions, and hardware details in each checkpoint’s
      metadata (JSON header) to aid future replication.

    By following this staged plan, a single developer can go from an empty
    repository to a fully reproducible implementation that matches all reported
    results of the *Neural Sinkhorn Gradient Flow* paper in roughly three weeks
    of focused development on a workstation with a modern GPU.