# Code Implementation Progress Summary
*Accumulated implementation progress for all files*


================================================================================
## IMPLEMENTATION File src/utils.py; ROUND 0 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:39:56
**File Implemented**: src/utils.py

**Core Purpose**  
Utility helpers for reproducibility and experiment logging. Provides a deterministic random‑seed setter and a TensorBoard logger creator used across the project.

**Public Interface**  
- **Function `set_seed(seed: int = 42, deterministic: bool = True) -> None`**  
  Sets seeds for `random`, `numpy`, and `torch` (CPU & CUDA). When `deterministic=True` forces deterministic CuDNN behavior and disables benchmark mode. Also fixes the Python hash seed via `PYTHONHASHSEED`. → *No return value*.

- **Function `get_logger(log_dir: str = "logs") -> torch.utils.tensorboard.SummaryWriter`**  
  Ensures `log_dir` exists, then returns a `SummaryWriter` instance for TensorBoard logging. → *Returns* a `SummaryWriter`.

**Internal Dependencies**  
- `os` – directory creation and environment variable handling.  
- `random` – Python RNG seed.  
- `numpy as np` – NumPy RNG seed.  
- `torch` – PyTorch RNG seeds and CUDA handling, plus CuDNN flags.  
- `torch.utils.tensorboard.SummaryWriter` – TensorBoard logging class.

**External Dependencies**  
- Expected to be imported by any training, evaluation, or inference module that needs reproducible randomness or logging, e.g.:  
  - `src/training/*_trainer.py` (for seeding and TensorBoard logging).  
  - `src/inference/*_infer.py` (for optional logging).  
  - Experiment scripts under `experiments/`.

**Implementation Notes**  
- **Determinism flag**: When `deterministic=True`, disables CuDNN benchmark and forces deterministic kernels, matching the reproducibility requirements described in the plan.  
- **Environment variable**: Sets `PYTHONHASHSEED` to ensure hash‑based randomness is also fixed.  
- **Logger creation**: Guarantees the log directory exists before instantiating `SummaryWriter`, simplifying downstream usage.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/sinkhorn.py; ROUND 1 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:40:00
**File Implemented**: src/sinkhorn.py

**Core Purpose**  
Provides a thin wrapper around `geomloss.SinkhornLoss` to compute the Sinkhorn divergence dual potentials on two point clouds and obtain the gradient of the source‑side potential, which is required for constructing the velocity field in the NSGF algorithms.

**Public Interface**  
- **Class `SinkhornPotentials`** – Computes Sinkhorn potentials `f` (on source points `X`) and `g` (on target points `Y`) and supplies their gradients.  
  - **Constructor** `__init__(self, X: Tensor, Y: Tensor, blur: float, scaling: float)`  
    *Parameters*:  
    - `X`: source point cloud `[B, d]` (requires grad)  
    - `Y`: target point cloud `[B, d]` (no grad needed)  
    - `blur`: entropy regularisation ε  
    - `scaling`: kernel scaling σ  
  - **Methods**  
    - `potential_X(self) -> Tensor` – returns `f` evaluated on `X`.  
    - `potential_Y(self) -> Tensor` – returns `g` evaluated on `Y`.  
    - `grad_X(self) -> Tensor` – gradient ∇ₓ f(x) w.r.t. `X` (shape `[B, d]`).  

- **Function `compute_potentials(X: Tensor, Y: Tensor, blur: float, scaling: float) -> Tuple[Tensor, Tensor]`** – Convenience wrapper that instantiates `SinkhornPotentials` and returns the pair `(f, g)`.

**Internal Dependencies**  
- `torch` – tensor handling, autograd.  
- `geomloss` – imports `SinkhornLoss` (fallback to `SamplesLoss` for older versions).

**External Dependencies** (files that are expected to import this module)  
- `src/particles.py` – will call `compute_potentials` / `SinkhornPotentials` to obtain velocity estimates.  
- Training modules (`src/training/nsfg_trainer.py`, `src/training/nsfgpp_trainer.py`) – will use the wrapper when building the trajectory pool and computing the empirical velocity field.

**Implementation Notes**  
- The wrapper forces `X` to require gradients (clones if necessary) because the gradient of the potential w.r.t. source points is needed for the velocity field.  
- `SinkhornLoss` is instantiated with `debias=True` to obtain the Sinkhorn *divergence* rather than the regular OT cost; this also triggers internal computation of the dual potentials.  
- `self.sinkhorn.potential_fn()` returns callables that map points to potentials; they are evaluated immediately on the supplied clouds for convenience.  
- `grad_X` uses `torch.autograd.grad` with a unit `grad_output` to retrieve ∇ₓ f for each point.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/particles.py; ROUND 2 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:40:07
**File Implemented**: src/particles.py

**Core Purpose**  
Provides lightweight, fully‑differentiable utilities for explicit‑Euler integration of particle clouds, enabling the simulation of particle trajectories required by the NSGF training and inference pipelines.

**Public Interface**  
- **Function `euler_step(x: torch.Tensor, v: torch.Tensor, eta: float) -> torch.Tensor`**  
  Performs a single explicit‑Euler update `x + eta * v`.  
- **Function `integrate_trajectory(x0: torch.Tensor, v_func: Callable[[torch.Tensor, int], torch.Tensor], eta: float, T: int) -> torch.Tensor`**  
  Repeatedly calls `v_func` to obtain the velocity field at each integer time step and applies `euler_step` for `T` steps, returning the final particle positions.  
- **Function `integrate_trajectory_with_history(x0: torch.Tensor, v_func: Callable[[torch.Tensor, int], torch.Tensor], eta: float, T: int) -> List[torch.Tensor]`**  
  Same as `integrate_trajectory` but records and returns the full list of intermediate particle clouds `[x_0, x_1, …, x_T]`.

**Internal Dependencies**  
- `import torch` – tensor operations and automatic differentiation.  
- `from typing import Callable` – type hint for the velocity‑field callable.

**External Dependencies** (consumers)  
- Expected to be imported by training modules (`src/training/nsfg_trainer.py`, `src/training/nsfgpp_trainer.py`) and inference scripts (`src/inference/nsfg_infer.py`, `src/inference/nsfgpp_infer.py`).  
- Key exports used elsewhere: `euler_step`, `integrate_trajectory`, `integrate_trajectory_with_history`.

**Implementation Notes**  
- All functions are pure and return new tensors; they never modify inputs in‑place (except the internal clone for safety).  
- The `v_func` signature matches the paper’s notation `vθ(x, t)`, allowing seamless integration with neural velocity‑field models.  
- The optional history wrapper is useful for debugging or visualising particle trajectories but is not required for the core algorithm.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/models/mlp.py; ROUND 3 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:40:10
**File Implemented**: src/models/mlp.py

**Core Purpose**  
Implements a lightweight three‑layer multilayer perceptron that approximates the time‑varying velocity field \(v_\theta(x,t)\) for the 2‑D synthetic experiments described in the paper. The network receives particle positions and a scalar time, concatenates them, and outputs a velocity vector of the same dimensionality as the positions.

**Public Interface**  
- **Class `MLPVelocity`** – neural network modelling the velocity field.  
  - **Constructor parameters**:  
    - `dim: int` – dimensionality of the position vector \(x\) (e.g., 2).  
    - `hidden_dim: int = 256` – number of hidden units per layer (default matches the paper).  
  - **Key methods**:  
    - `forward(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor` – concatenates `x` and `t`, passes through the MLP, and returns a velocity tensor of shape `[B, dim]`.  

No free functions or constants are defined.

**Internal Dependencies**  
- `torch` – provides the tensor type and core neural‑network building blocks.  
- `torch.nn as nn` – used for `Linear`, `ReLU`, and `Sequential` modules.

**External Dependencies**  
- Expected to be imported by:  
  - Training scripts for synthetic 2‑D experiments (e.g., `src/training/nsfg_trainer.py`).  
  - Inference utilities for the 2‑D case (e.g., `src/inference/nsfg_infer.py`).  
- Main export used elsewhere: the `MLPVelocity` class.

**Implementation Notes**  
- **Architecture decisions**:  
  - Input dimension is `dim + 1` to accommodate the scalar time after broadcasting.  
  - Three hidden layers of equal size (`hidden_dim`) with ReLU activations; the final layer is linear to produce unrestricted velocity values.  
  - Uses `nn.ReLU(inplace=True)` for modest memory savings.  
- **Cross‑File Relationships**:  
  - The model is instantiated with the dimensionality of the synthetic dataset and trained against empirical velocity estimates computed in `particles.py`/`sinkhorn.py`.  
  - During inference, the same `forward` signature is used to generate particle updates step‑by‑step.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/models/unet.py; ROUND 4 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:40:13
**File Implemented**: src/models/unet.py

**Core Purpose**  
Defines the UNet‑based neural networks used to model the time‑varying velocity field `v_θ` and the straight‑flow field `u_δ` in NSGF/NSGF++. It also provides a sinusoidal time‑embedding utility and the low‑level convolutional building blocks required by the UNet architecture.

**Public Interface**  

- **Function** `sinusoidal_time_embedding(t: torch.Tensor, dim: int = 16) → torch.Tensor`  
  Creates a sinusoidal positional embedding for a scalar time tensor `t`. Returns a tensor of shape `(B, dim)`.

- **Class** `DoubleConv`  
  *Purpose*: Two consecutive `Conv2d → BatchNorm → ReLU` blocks.  
  *Key methods*: `forward(x: torch.Tensor) → torch.Tensor`  
  *Constructor params*: `in_ch: int`, `out_ch: int`

- **Class** `Down`  
  *Purpose*: Down‑sampling block (`MaxPool2d` + `DoubleConv`).  
  *Key methods*: `forward(x: torch.Tensor) → torch.Tensor`  
  *Constructor params*: `in_ch: int`, `out_ch: int`

- **Class** `Up`  
  *Purpose*: Up‑sampling block (bilinear or transposed convolution) followed by `DoubleConv`. Handles skip‑connection concatenation and padding.  
  *Key methods*: `forward(x1: torch.Tensor, x2: torch.Tensor) → torch.Tensor`  
  *Constructor params*: `in_ch: int`, `out_ch: int`, `bilinear: bool = True`

- **Class** `OutConv`  
  *Purpose*: Final 1×1 convolution to map features to the desired number of output channels.  
  *Key methods*: `forward(x: torch.Tensor) → torch.Tensor`  
  *Constructor params*: `in_ch: int`, `out_ch: int`

- **Class** `UNetVelocity` (inherits `nn.Module`)  
  *Purpose*: UNet that predicts the velocity field `v_θ(x, t)`. Takes an image tensor `x` of shape `(B, C, H, W)` and a scalar time tensor `t` of shape `(B, 1)`. Adds a sinusoidal time embedding to the bottleneck feature map.  
  *Key methods*: `forward(x: torch.Tensor, t: torch.Tensor) → torch.Tensor` (returns velocity of shape `(B, out_channels, H, W)`)  
  *Constructor params*:  
    - `in_channels: int = 3`  
    - `out_channels: int = 3`  
    - `base_channels: int = 32`  
    - `depth: int = 1` (number of down‑/up‑sampling stages)  
    - `time_emb_dim: int = 16`  
    - `bilinear: bool = True`

- **Class** `UNetStraightFlow` (inherits `UNetVelocity`)  
  *Purpose*: Alias for the straight‑flow network `u_δ`; identical architecture to `UNetVelocity`. No additional methods.

**Internal Dependencies**  

- `torch`, `torch.nn as nn`, `torch.nn.functional` (used in `Up` for padding)  
- `math` (logarithm for frequency computation in `sinusoidal_time_embedding`)  
- `typing.List` (type hint for module lists)

**External Dependencies**  

- Expected to be imported by:  
  - `src/models/__init__.py` (to expose the classes)  
  - Training modules `src/training/nsfg_trainer.py` and `src/training/nsfgpp_trainer.py` (use `UNetVelocity` / `UNetStraightFlow`)  
  - Inference modules `src/inference/nsfg_infer.py` and `src/inference/nsfgpp_infer.py` (call the forward pass)  
  - Potentially other utilities that need the time embedding.

**Implementation Notes**  

- **Architecture decisions**:  
  - Uses a classic UNet skeleton with configurable depth and base channel width.  
  - Time information is injected via a sinusoidal embedding projected to the bottleneck channel dimension, enabling the network to model time‑varying fields without altering spatial resolution.  
  - Separate class `UNetStraightFlow` inherits unchanged behavior, keeping the codebase clear when both velocity and straight‑flow networks are instantiated.  

- **Cross‑File Relationships**:  
  - The low‑level blocks (`DoubleConv`, `Down`, `Up`, `OutConv`) are reusable building blocks for any UNet‑style model within the project.  
  - `sinusoidal_time_embedding` is a pure utility that can be reused by other time‑conditioned networks (e.g., a potential future time‑predictor).

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/models/__init__.py; ROUND 5 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:40:18
**File Implemented**: src/models/__init__.py

**Core Purpose**  
Provides a convenient package‑level entry point for the model submodule, exposing the primary neural network classes and the sinusoidal time‑embedding utility so they can be imported directly via `src.models`.

**Public Interface**  
- **Class `MLPVelocity`** – three‑layer MLP used as the velocity‑field network for 2‑D synthetic experiments.  
  *Key methods*: `forward(self, x, t)` (computes velocity).  
  *Constructor params*: `input_dim`, `hidden_dim=256`, `output_dim`, `time_emb_dim=16` (default values defined in the class).  

- **Class `UNetVelocity`** – UNet architecture for image‑based velocity‑field approximation.  
  *Key methods*: `forward(self, x, t)` (returns velocity field).  
  *Constructor params*: `in_channels`, `out_channels`, `base_channels`, `depth`, `time_emb_dim=16`, plus optional UNet‑specific kwargs.  

- **Class `UNetStraightFlow`** – UNet with identical architecture to `UNetVelocity`, employed for the straight‑flow network in the NSGF++ pipeline.  
  *Key methods*: `forward(self, x, t)` (predicts constant‑velocity field).  
  *Constructor params*: same as `UNetVelocity`.  

- **Function `sinusoidal_time_embedding(t, dim=16)`** – Generates sinusoidal positional embeddings for scalar time inputs.  
  *Returns*: Tensor of shape `(..., dim)` containing the embedding.

**Internal Dependencies**  
- From **`.mlp`**: `MLPVelocity` – defines the MLP velocity network.  
- From **`.unet`**: `UNetVelocity`, `UNetStraightFlow`, `sinusoidal_time_embedding` – defines the UNet architectures and the time‑embedding helper.

**External Dependencies**  
- Expected to be imported by:  
  - Training modules (`src/training/nsfg_trainer.py`, `src/training/nsfgpp_trainer.py`) for constructing model instances.  
  - Inference scripts (`src/inference/nsfg_infer.py`, `src/inference/nsfgpp_infer.py`).  
  - Experiment entry points (`experiments/*.py`).  
- Key exports used elsewhere: `MLPVelocity`, `UNetVelocity`, `UNetStraightFlow`, `sinusoidal_time_embedding`.

**Implementation Notes**  
- The `__all__` list explicitly enumerates the public symbols, ensuring `from src.models import *` only exposes the intended classes/functions.  
- By re‑exporting the classes at the package level, downstream code can use a concise import style (`from src.models import UNetVelocity`) without needing to know the internal file layout.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/training/nsfg_trainer.py; ROUND 6 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:40:23
**File Implemented**: src/training/nsfg_trainer.py

**Core Purpose**  
Implements the training pipeline for the Neural Sinkhorn Gradient Flow (NSFG) velocity‑field network (Algorithm 1). It builds an experience‑replay pool of particle states and empirical velocities, then optimises the network to match these velocities via MSE loss.

**Public Interface**  
- **Class `NSFGTrainer`** – orchestrates pool construction, training, checkpointing, and logging.  
  - **Key methods**:  
    - `__init__(self, model, source_sampler, target_sampler, blur, scaling, eta, T, num_pool_batches, batch_size, lr, total_iters, device='cpu', seed=42, log_dir='logs/nsfg', checkpoint_dir='checkpoints/nsfg')` – sets up trainer, reproducibility, optimizer, logger, and empty pool.  
    - `_build_pool(self, n)` – internal: creates the trajectory pool by repeatedly sampling source/target particles, computing Sinkhorn potentials, extracting empirical velocity `v̂` (Eq. 13), storing `(x, v̂, t)`, and advancing particles with an Euler step (Eq. 14).  
    - `_sample_minibatch(self)` – internal: randomly draws `batch_size` entries from the pool and returns stacked tensors `x_batch`, `v_batch`, `t_batch`.  
    - `train(self, pool_particle_n=1024, checkpoint_interval=1000)` – runs the full training loop: builds the pool, then for `total_iters` steps samples a minibatch, computes the MSE loss between model prediction `model(x, t)` and `v̂`, back‑propagates, updates the optimizer, logs loss to TensorBoard, and saves checkpoints.  
    - `load_checkpoint(self, path)` – restores model and optimizer state from a saved checkpoint.  
    - `close(self)` – closes the TensorBoard `SummaryWriter`.

**Internal Dependencies**  
- `from ..utils import set_seed, get_logger` – seed initialisation and TensorBoard logger creation.  
- `from ..sinkhorn import compute_potentials` – wrapper that returns Sinkhorn dual potentials for two point clouds.  
- `from ..particles import euler_step` – performs one explicit Euler integration step.  
- `torch`, `torch.nn`, `torch.optim.Adam`, `torch.utils.tensorboard.SummaryWriter` – core PyTorch utilities.  
- Standard library: `os`, `random`, `typing` (Callable, List, Tuple).

**External Dependencies** (files that are expected to import/use this module)  
- Experiment scripts (e.g., `experiments/synthetic_2d.py`, `experiments/mnist.py`, `experiments/cifar10.py`) will instantiate `NSFGTrainer` with appropriate model, samplers, and hyper‑parameters.  
- Inference modules (`src/inference/nsfg_infer.py`) may load a trained checkpoint via `load_checkpoint` and reuse the trained model.

**Implementation Notes**  
- **Architecture decisions**:  
  - Uses an in‑memory list of tuples for the trajectory pool to keep the implementation simple and framework‑agnostic.  
  - Gradient computation of Sinkhorn potentials is performed with `torch.autograd.grad` on the summed potentials, matching Eq. 13.  
  - Logging is handled via a lightweight `SummaryWriter` wrapper (`get_logger`).  
  - Checkpointing saves both model and optimizer state dictionaries at configurable intervals.  
- **Cross‑File Relationships**:  
  - `compute_potentials` abstracts the geomloss Sinkhorn loss and provides the dual potentials needed for velocity estimation.  
  - `euler_step` encapsulates the explicit Euler update used during pool construction.  
  - The trainer is deliberately decoupled from any specific model architecture; any `nn.Module` with a `(x, t) → velocity` signature can be supplied.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/inference/nsfg_infer.py; ROUND 7 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:40:35
**File Implemented**: src/inference/nsfg_infer.py

**Core Purpose**  
Provides the inference routine for the basic Neural Sinkhorn Gradient Flow (NSFG). It loads a trained velocity‑field network, samples particles from the source distribution, and integrates them forward in time using an explicit Euler scheme.

**Public Interface**  
- **Function `nsfg_generate`**  
  ```python
  nsfg_generate(
      model: torch.nn.Module,
      source_sampler: Callable[[int], torch.Tensor],
      eta: float,
      T: int,
      batch_size: int = 1024,
      device: str = "cpu",
  ) -> torch.Tensor
  ```  
  Generates `batch_size` samples by iteratively applying the velocity field `model(x, t)` for `T` Euler steps of size `eta`. Returns a CPU‑tensor of shape `(batch_size, d)`.

- **CLI entry point `main()`**  
  Parses command‑line arguments, sets the random seed, loads a checkpoint, constructs the appropriate model (`UNetVelocity` or `MLPVelocity`), defines a simple source sampler, calls `nsfg_generate`, and saves the resulting samples.

**Internal Dependencies**  
- `argparse`, `os`, `typing.Callable`, `torch`, `torch.utils.tensorboard.SummaryWriter` – standard library / PyTorch utilities.  
- `..utils` → `set_seed`, `get_logger` – seed handling and TensorBoard logging.  
- `..particles` → `euler_step` – performs a single explicit Euler update `x_{new} = x + η * v`.  
- `src.models` → `UNetVelocity`, `MLPVelocity` – concrete velocity‑field network classes (already implemented).

**External Dependencies**  
- Expected to be imported by experiment scripts (e.g., `experiments/synthetic_2d.py`, `experiments/mnist.py`) that need a ready‑to‑use inference function.  
- The CLI can be invoked directly from the command line for quick generation of samples.

**Implementation Notes**  
- **Model loading heuristic**: Checks `checkpoint["model_type"]` to decide between MLP (2‑D) and UNet (image) architectures; falls back to UNet.  
- **Source sampler**: For UNet models, infers channel/size from model attributes (`in_channels`, `image_size`) and samples uniform noise in `[-1, 1]`. For MLP, samples standard normal vectors.  
- **Time handling**: Supplies a constant time tensor `t_tensor` of shape `(batch_size, 1)` for each Euler step, matching the expected model signature.  
- **Logging**: Uses a TensorBoard logger to record arguments and status messages; closes the logger at the end.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/models/time_predictor.py; ROUND 8 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:41:38
**File Implemented**: src/models/time_predictor.py

**Core Purpose**  
Implements the phase‑transition predictor network `tφ` (TimePredictor) that estimates the transition time τ ∈ (0, 1) for the NSGF++ two‑phase generation pipeline. The model follows the CNN architecture described in the paper.

**Public Interface**  
- **Class `TimePredictor`** – CNN that maps an image (or reshaped 2‑D tensor) to a scalar τ.  
  - **Constructor parameters**  
    - `in_channels: int = 3` – number of input channels (e.g., 1 for MNIST, 3 for CIFAR‑10).  
    - `hidden_dims: list[int] | None` – list of filter sizes for the four conv layers; defaults to `[32, 64, 128, 256]`.  
  - **Key methods**  
    - `forward(x: torch.Tensor) -> torch.Tensor` – returns a tensor of shape (B, 1) with values in (0, 1).  
    - `_initialize_fc(sample_tensor: torch.Tensor)` – lazily creates the final linear layer once the flattened feature size is known.  
    - `extra_repr() -> str` – provides a concise representation for `print(model)`.  

- **Exports**  
  - `__all__ = ["TimePredictor"]` – makes `TimePredictor` the sole public symbol of the module.

**Internal Dependencies**  
- `torch` – core tensor operations.  
- `torch.nn as nn` – building blocks (`Conv2d`, `BatchNorm2d`, `ReLU`, `AvgPool2d`, `Linear`, `Sigmoid`).

**External Dependencies** (consumers)  
- Expected to be imported by:  
  - `src/training/nsfgpp_trainer.py` – trains the predictor as part of Algorithm 3.  
  - `src/inference/nsfgpp_infer.py` – uses the trained predictor during NSGF++ inference (Algorithm 4).  
- Main interface used elsewhere: the `TimePredictor` class (instantiated, called via its `forward` method).

**Implementation Notes**  
- **Architecture decisions**  
  - Four convolution‑batch‑norm‑ReLU‑AvgPool blocks, each halving spatial dimensions, matching the paper’s specification.  
  - Lazy initialization of the final fully‑connected layer (`self.fc`) to accommodate arbitrary input resolutions (e.g., 28×28 for MNIST, 32×32 for CIFAR‑10).  
  - Sigmoid activation ensures output lies in (0, 1).  
- **Cross‑File Relationships**  
  - The predictor receives image tensors from the training pipeline, learns τ by regression against uniformly sampled ground‑truth transition times.  
  - During inference, the same network provides per‑sample τ values that guide the switch from the NSGF phase to the straight‑line flow phase.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/training/nsfgpp_trainer.py; ROUND 9 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:41:45
**File Implemented**: src/training/nsfgpp_trainer.py

**Core Purpose**  
Implements the training pipeline for the NSGF++ variant (Algorithm 3). It sequentially trains three neural components – the velocity‑field network, the phase‑transition predictor, and the neural straight‑flow network – using an experience‑replay trajectory pool and straight‑line interpolations between source and target samples.

**Public Interface**  
- **Class `NSFGPPTrainer`** – orchestrates the full NSGF++ training.  
  - **Constructor params**:  
    `v_model`, `t_model`, `u_model` (nn.Modules); `source_sampler`, `target_sampler` (Callable); `blur`, `scaling`, `eta`, `T`, `num_pool_batches`, `batch_size`; learning rates `lr_velocity`, `lr_time`, `lr_nsf`; iteration counts `total_iters_velocity`, `total_iters_time`, `total_iters_nsf`; optional `device`, `seed`, `log_dir`, `checkpoint_dir`.  
  - **Key methods**:  
    - `_build_pool(n)`: builds the trajectory pool of (x, v̂, t) tuples using Sinkhorn potentials and Euler integration.  
    - `_sample_minibatch()`: randomly draws a minibatch from the pool.  
    - `train_velocity(pool_particle_n=1024, checkpoint_interval=1000)`: trains the velocity‑field network `v_model`.  
    - `train_time_predictor(checkpoint_interval=1000)`: trains the phase‑transition predictor `t_model` on straight‑line interpolations.  
    - `train_nsf(checkpoint_interval=1000)`: (currently a placeholder) intended to train the neural straight‑flow network `u_model`.  
    - `load_checkpoint(path, component)`: loads a saved checkpoint for `"v"`, `"t"` or `"u"`.  
    - `close()`: closes the TensorBoard logger.

**Internal Dependencies**  
- `os`, `random`, `typing` – standard library utilities.  
- `torch`, `torch.nn`, `torch.optim`, `torch.utils.tensorboard.SummaryWriter` – core PyTorch framework.  
- `..utils` → `set_seed`, `get_logger` – reproducibility and TensorBoard logging.  
- `..sinkhorn` → `compute_potentials` – computes Sinkhorn dual potentials for velocity estimation.  
- `..particles` → `euler_step` – performs explicit Euler integration of particle positions.  
- `..models` → `UNetVelocity`, `UNetStraightFlow`, `TimePredictor` – neural network architectures used in training.

**External Dependencies** (files that are expected to import/use this module)  
- Inference script `src/inference/nsfgpp_infer.py` will instantiate `NSFGPPTrainer` (or its trained models) for generation.  
- Experiment scripts (e.g., `experiments/mnist.py`, `experiments/cifar10.py`) will create a trainer instance, call the three training phases, and then evaluate results.

**Implementation Notes**  
- **Trajectory Pool Construction** mirrors the NSFG trainer but limits the number of Euler steps to `T ≤ 5` for NSGF++.  
- Velocity training uses mean‑squared error between network predictions and empirical velocities derived from Sinkhorn potentials.  
- Time‑predictor training regresses the interpolation coefficient `t` from interpolated samples `X_t`.  
- The `train_nsf` method is currently incomplete (placeholder comment). It should compute the constant target velocity `Y‑X0`, predict with `u_model`, compute MSE loss, back‑propagate, and log/save checkpoints.  
- Checkpointing and TensorBoard logging are performed at configurable intervals for each component.  
- All models and tensors are moved to the specified device; reproducibility is ensured via `set_seed`.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/inference/nsfgpp_infer.py; ROUND 10 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:41:50
**File Implemented**: src/inference/nsfgpp_infer.py

**Core Purpose**  
Implements the NSGF++ inference pipeline (Algorithm 4). Provides a function that generates samples by first integrating particles with the learned velocity‑field network, then predicting a per‑sample transition time, and finally refining the samples with a neural straight‑flow network. Includes a CLI for quick generation from a checkpoint.

**Public Interface**  
- **Function `nsfgpp_generate`**(`v_model: torch.nn.Module, t_model: torch.nn.Module, u_model: torch.nn.Module, source_sampler: Callable[[int], torch.Tensor], eta_nsfg: float, omega_nsf: float, T: int, batch_size: int = 1024, device: str = "cpu"`):  
  Runs the two‑phase NSGF++ generation and returns a `torch.Tensor` of generated samples (CPU).  

- **Function `_default_source_sampler`**(`batch_size: int, device: str = "cpu"`):  
  Default sampler used by the CLI; returns either 2‑D Gaussian vectors or 3‑channel 32×32 uniform‑noise images depending on the `SOURCE_TYPE` env‑var.  

- **Function `main`**():  
  CLI entry point – parses arguments, loads a checkpoint, builds the appropriate models, runs `nsfgpp_generate`, saves the samples, and logs via TensorBoard.

**Internal Dependencies**  
- `argparse`, `os`, `typing.Callable` – standard library utilities.  
- `torch`, `torch.utils.tensorboard.SummaryWriter` – core deep‑learning framework and logging.  
- `..utils` → `set_seed`, `get_logger` – seed setting and TensorBoard logger.  
- `..particles` → `euler_step` – single explicit‑Euler update for particle positions.  
- `..models` → `UNetVelocity`, `UNetStraightFlow`, `TimePredictor`, `MLPVelocity` – neural network architectures used for the three NSGF++ components.

**External Dependencies** (files that may import this module)  
- Potentially other scripts in `experiments/` (e.g., `synthetic_2d.py`, `mnist.py`, `cifar10.py`) that need to invoke NSGF++ inference.  
- Users may import `nsfgpp_generate` directly from `src.inference.nsfgpp_infer` for custom pipelines.

**Implementation Notes**  
- **Architecture decisions**:  
  - Keeps models in evaluation mode and on the requested device throughout generation.  
  - Uses a mask‑based loop for the NSF phase to handle per‑sample varying numbers of Euler steps efficiently.  
  - Provides a flexible source sampler that adapts to synthetic or image data via an environment variable.  
- **Cross‑File Relationships**:  
  - Relies on `particles.euler_step` for the low‑level integration step.  
  - Model classes are imported from the `models` package, ensuring consistency with training code.  
  - Logging utilities from `utils` are used to record arguments and status messages.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/__init__.py; ROUND 11 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:41:53
**File Implemented**: src/__init__.py

**Core Purpose**  
- Initializes the top‑level `src` package and re‑exports key sub‑modules (`utils`, `models`, `training`, `inference`, `sinkhorn`, `particles`) so that they can be imported directly via `import src`.

**Public Interface**  
- **Modules Exported via `__all__`**:  
  - `utils` – utility functions (seed setting, logging, visualisation).  
  - `models` – neural network definitions (`mlp`, `unet`, `time_predictor`).  
  - `training` – training pipelines (`nsfg_trainer`, `nsfgpp_trainer`).  
  - `inference` – inference pipelines (`nsfg_infer`, `nsfgpp_infer`).  
  - `sinkhorn` – Sinkhorn loss wrapper and potential extraction.  
  - `particles` – particle‑based ODE integration helpers.  

No classes or functions are defined in this file; it only provides package‑level imports and the `__all__` list.

**Internal Dependencies**  
- `from . import utils` – imports the `src/utils.py` module.  
- `from . import models` – imports the `src/models` package.  
- `from . import training` – imports the `src/training` package (currently a placeholder).  
- `from . import inference` – imports the `src/inference` package (currently a placeholder).  
- `from . import sinkhorn` – imports the `src/sinkhorn.py` module.  
- `from . import particles` – imports the `src/particles.py` module.  

All imports are internal to the repository; no external packages are referenced directly in this file.

**External Dependencies**  
- Expected to be imported by any user‑facing script or notebook that needs access to the library, e.g., `experiments/*.py`, training scripts, or downstream projects.  
- The main interfaces used elsewhere are the sub‑modules listed in `__all__`.

**Implementation Notes**  
- **Architecture Decision**: Centralising imports in `src/__init__.py` provides a clean, flat namespace (`src.utils`, `src.models`, etc.) and mirrors common Python package patterns.  
- **Cross‑File Relationships**: By exposing the sub‑packages here, downstream code can simply `import src` and then access `src.training.nsfg_trainer`, `src.inference.nsfg_infer`, etc., without needing to know the internal directory layout.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/training/__init__.py; ROUND 12 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:42:57
**File Implemented**: src/training/__init__.py

**Core Purpose**  
Initialises the training package and re‑exports the primary trainer classes (`NSFGTrainer` and `NSFGPPTrainer`) so they can be imported directly from `src.training`.

**Public Interface**  
- **Class `NSFGTrainer`** – Trainer for the basic Neural Sinkhorn Gradient Flow (NSGF) algorithm.  
  *Key methods*: (defined in `nsfg_trainer.py`) – e.g., `train()`, `save_checkpoint()`, `load_checkpoint()`.  
  *Constructor params*: (as defined in its implementation) – typically includes model, optimizer, dataloader, config, etc.  

- **Class `NSFGPPTrainer`** – Trainer for the two‑phase NSGF++ algorithm (velocity‑field network, phase‑transition predictor, and straight‑flow network).  
  *Key methods*: (defined in `nsfgpp_trainer.py`) – e.g., `train_phase1()`, `train_phase2()`, `train_phase3()`, `save_checkpoint()`.  
  *Constructor params*: analogous to `NSFGTrainer` but also accepts additional models (`t_phi`, `u_delta`) and related hyper‑parameters.  

- **Constant `__all__`** – List of public symbols exported by the package: `["NSFGTrainer", "NSFGPPTrainer"]`.

**Internal Dependencies**  
- `from .nsfg_trainer import NSFGTrainer` – imports the NSFG trainer implementation.  
- `from .nsfgpp_trainer import NSFGPPTrainer` – imports the NSGF++ trainer implementation.

**External Dependencies** (who imports this file)  
- Any user script or module that needs to instantiate a trainer, e.g., `experiments/synthetic_2d.py`, `experiments/mnist.py`, `experiments/cifar10.py`.  
- The top‑level package (`src/__init__.py`) may also re‑export these trainers for convenience.

**Implementation Notes**  
- The file provides a thin façade, keeping the training package’s public API stable even if internal trainer implementations change.  
- By defining `__all__`, wildcard imports (`from src.training import *`) expose only the intended trainer classes.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/inference/__init__.py; ROUND 13 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:42:59
**File Implemented**: src/inference/__init__.py

**Core Purpose**  
Provides a convenient public API for the inference submodule by re‑exporting the main generation functions and command‑line entry points for Neural Sinkhorn Gradient Flow (NSFG) and its NSGF++ variant.

**Public Interface**  
- **Function `nsfg_generate`(**`*args, **kwargs`**)** – Wrapper around `src.inference.nsfg_infer.nsfg_generate`; generates samples using the trained NSFG velocity‑field network.  
- **Function `nsfg_main`(**`*args, **kwargs`**)** – Alias for `src.inference.nsfg_infer.main`; CLI entry point for NSFG generation.  
- **Function `nsfgpp_generate`(**`*args, **kwargs`**)** – Wrapper around `src.inference.nsfgpp_infer.nsfgpp_generate`; generates samples using the full NSGF++ pipeline (NSFG phase + phase‑transition predictor + NSF refinement).  
- **Function `nsfgpp_main`(**`*args, **kwargs`**)** – Alias for `src.inference.nsfgpp_infer.main`; CLI entry point for NSGF++ generation.  

All four symbols are exported via `__all__`.

**Internal Dependencies**  
- `from .nsfg_infer import nsfg_generate, main as nsfg_main` – imports the core NSFG generation routine and its CLI wrapper.  
- `from .nsfgpp_infer import nsfgpp_generate, main as nsfgpp_main` – imports the NSGF++ generation routine and its CLI wrapper.

**External Dependencies (who imports this file)**  
- Any user code or scripts that need to run inference will import from `src.inference`, e.g.:  
  ```python
  from src.inference import nsfg_generate, nsfgpp_generate
  ```  
- The experiment scripts (`experiments/*.py`) and potential notebooks will rely on these re‑exports.

**Implementation Notes**  
- The module acts solely as a thin façade; it does not contain any logic beyond re‑exporting symbols.  
- `__all__` is defined to limit what gets imported with `from src.inference import *`, ensuring a clean public namespace.  
- By aliasing `main` as `nsfg_main` / `nsfgpp_main`, the CLI entry points are accessible without exposing the original `main` name, keeping the API consistent.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File experiments/synthetic_2d.py; ROUND 14 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:43:04
**File Implemented**: experiments/synthetic_2d.py

**Core Purpose**  
- Provides an end‑to‑end runnable experiment that trains the 2‑D Neural Sinkhorn Gradient Flow (NSGF) on a synthetic 8‑gaussian mixture, generates samples with the learned velocity model, and evaluates them using the 2‑Wasserstein distance.

**Public Interface**  
- **Function `sample_source(n: int, device: str = "cpu") -> torch.Tensor`**  
  - Samples `n` points from a standard 2‑D Gaussian (N(0, I)).  
  - Returns a tensor of shape `(n, 2)` on the specified device.  

- **Function `sample_target(n: int, device: str = "cpu") -> torch.Tensor`**  
  - Samples `n` points from an 8‑component Gaussian mixture (centers on a circle of radius 2, each with unit variance and added noise σ = 0.2).  
  - Returns a tensor of shape `(n, 2)` on the specified device.  

- **Function `main()`** (script entry point)  
  - Parses command‑line arguments (`--config`, `--device`).  
  - Loads a YAML configuration file.  
  - Sets random seeds via `src.utils.set_seed`.  
  - Instantiates the MLP velocity model (`src.models.MLPVelocity`).  
  - Constructs an `NSFGTrainer` (`src.training.NSFGTrainer`) with hyper‑parameters from the config.  
  - Calls `trainer.train(...)` to build the trajectory pool and optimise the velocity network.  
  - Performs inference: integrates particles forward for `T` Euler steps using the trained model.  
  - Computes the 2‑Wasserstein distance via `experiments.metrics.wasserstein_distance`.  
  - Saves generated samples and the metric to `results/figures/`.  
  - Closes the trainer’s TensorBoard logger.

**Internal Dependencies**  
- **Standard library / third‑party**  
  - `argparse`, `os`, `yaml`, `torch`, `torch.utils.tensorboard.SummaryWriter` – for CLI, file handling, config parsing, tensor operations, and logging.  
- **Project modules**  
  - `src.utils` → `set_seed`, `get_logger` – seed initialization and optional logger (unused directly in this script).  
  - `src.models` → `MLPVelocity` – the MLP that approximates the velocity field.  
  - `src.training` → `NSFGTrainer` – trainer implementing Algorithm 1 (trajectory‑pool construction + velocity‑field regression).  
  - `experiments.metrics` → `wasserstein_distance` – computes the 2‑Wasserstein distance between two point clouds.

**External Dependencies** (files that are expected to import/use this script)  
- Typically invoked as a standalone script (`python experiments/synthetic_2d.py …`).  
- No other project modules import symbols from this file; it serves as the entry point for the synthetic 2‑D experiment.

**Implementation Notes**  
- **Configuration‑driven**: All hyper‑parameters (seed, hidden dimension, Sinkhorn blur/scaling, Euler step size, number of NSGF steps `T`, pool size, training batch size, learning rate, total iterations, etc.) are read from `configs/nsfg_2d.yaml`. This decouples experiment logic from hard‑coded values.  
- **Modular samplers**: Source and target samplers are passed as lambda functions to `NSFGTrainer`, enabling reuse of the trainer for other datasets.  
- **Inference loop**: Uses a simple for‑loop over `T` Euler steps, calling the model with the current time offset (`t + step`). The time tensor `t` is a column vector of zeros, matching the model’s expected `(B,1)` time input.  
- **Metric computation**: Converts generated and true samples to NumPy arrays before calling `wasserstein_distance`, which likely uses SciPy/OT utilities.  
- **Result persistence**: Saves generated particles as a Torch tensor (`.pt`) and writes the scalar metric to a text file, both under `results/figures/`.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File configs/nsfg_2d.yaml; ROUND 15 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:43:06
**File Implemented**: configs/nsfg_2d.yaml

**Core Purpose**  
- Provides a YAML configuration for training the Neural Sinkhorn Gradient Flow (NSGF) on synthetic 2‑D benchmark data. It centralises hyper‑parameters such as model dimensions, training schedule, Sinkhorn settings, Euler integration step, and logging paths.

**Public Interface**  
- *Constants/Values*:  
  - `seed: 42` – random seed for reproducibility.  
  - `model.dim: 2` – dimensionality of the synthetic data.  
  - `model.hidden_dim: 256` – hidden layer size for the MLP velocity‑field network.  
  - `training.batch_size: 256` – minibatch size for velocity‑field training.  
  - `training.lr_velocity: 1e-4` – learning rate for the velocity‑field optimizer.  
  - `training.total_iters: 20000` – number of gradient‑steps for training `vθ`.  
  - `training.eta: 0.2` – Euler integration step size (alias of `euler.eta_nsfg`).  
  - `training.T: 10` – maximum number of NSGF integration steps (full NSGF).  
  - `training.num_pool_batches: 1500` – number of batches used to build the trajectory pool.  
  - `training.blur: 0.5` & `training.scaling: 0.80` – Sinkhorn regularisation parameters.  
  - `sinkhorn.blur: 0.5` & `sinkhorn.scaling: 0.80` – duplicated for convenience in downstream code.  
  - `euler.eta_nsfg: 0.2` – explicit‑Euler step size for NSGF integration.  
  - `nsfg.T_max: 10` – maximum NSGF steps (mirrors `training.T`).  
  - `logging.log_dir: logs/nsfg` – directory for TensorBoard logs.  
  - `logging.checkpoint_dir: checkpoints/nsfg` – directory for model checkpoints.

**Internal Dependencies**  
- None (pure configuration file).

**External Dependencies**  
- Expected to be imported by:  
  - `src/training/nsfg_trainer.py` – reads training‑related fields (batch size, learning rate, T, etc.).  
  - `src/training/nsfg_infer.py` – uses `euler.eta_nsfg` and `nsfg.T_max` for inference.  
  - `src/utils.py` or any experiment script that loads the YAML (e.g., `experiments/synthetic_2d.py`).

**Implementation Notes**  
- All hyper‑parameters are grouped logically (model, training, sinkhorn, euler, nsfg, logging) to simplify downstream parsing with `yaml.safe_load`.  
- Duplicate entries for Sinkhorn parameters (`training.blur/scaling` and `sinkhorn.blur/scaling`) allow both trainer and any low‑level Sinkhorn wrapper to access the same values without extra nesting.  
- The file follows the same schema used by the other config files (`nsfgpp_mnist.yaml`, `nsfgpp_cifar.yaml`) to keep the codebase uniform.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File requirements.txt; ROUND 16 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:43:09
**File Implemented**: requirements.txt

**Core Purpose**  
Defines the exact Python package versions required to run the project, ensuring reproducible environments across different machines.

**Public Interface**  
- *No classes or functions are defined.*  
- The file consists solely of plain‑text lines specifying package specifications (e.g., `torch==1.13.1+cu117`). These lines are consumed by `pip` when executing `pip install -r requirements.txt`.

**Internal Dependencies**  
- *None* – the file does not import any modules; it is a static list of dependency specifications.

**External Dependencies**  
- Expected to be read by the Python package installer (`pip`).  
- Packages listed are later imported by various source files (e.g., `torch`, `torchvision`, `geomloss`, `numpy`, `scipy`, `tqdm`, `matplotlib`, `pandas`, `pillow`, `tensorboard`, `scikit-image`, `torch-metrics`).

**Implementation Notes**  
- The file pins exact versions (including CUDA builds for PyTorch/torchvision) to match the environment used in the original paper.  
- The `torch-metrics` entry appears as `torch-metrics==0.11.4`; this is the package name used by `torchmetrics` (the import alias used elsewhere).  
- No conditional logic or comments are required; the file is intentionally minimal.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File README.md; ROUND 17 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:43:12
**File Implemented**: README.md

**Core Purpose**  
Provides a high‑level overview of the repository, including directory layout, installation instructions, quick‑start commands for the three main experiments (synthetic 2‑D, MNIST, CIFAR‑10), configuration file usage, evaluation utilities, and reproducibility guidelines.

**Public Interface**  
- *No executable code* – the file contains only markdown documentation. Consequently, there are no classes, functions, or constants exported for programmatic use.

**Internal Dependencies**  
- None (pure documentation).

**External Dependencies (consumers)**  
- Intended to be consulted by all scripts and users of the repository (e.g., `experiments/*.py`, `configs/*.yaml`). It does not provide programmatic interfaces but serves as the primary reference for how to run the codebase.

**Implementation Notes**  
- Structured as a concise README with sections: Directory Structure, Installation, Quick Start (commands for each experiment), Configuration Files, Evaluation, Reproducibility, and License.  
- Mirrors the project’s design outlined in the planning document, ensuring that users can immediately understand how to set up the environment and execute experiments.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File configs/nsfgpp_cifar.yaml; ROUND 18 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:43:17
**File Implemented**: configs/nsfgpp_cifar.yaml

**Core Purpose**  
Defines the hyper‑parameters and configuration settings for training and evaluating the NSGF++ two‑phase pipeline on the CIFAR‑10 dataset. It centralises model architecture, optimizer, training schedule, Sinkhorn loss, Euler integration, and logging/checkpoint paths.

**Public Interface**  
- **Constants / Config Sections** (YAML keys)  
  - `seed: int` – Random seed for reproducibility.  
  - `model:` – Architecture parameters for the UNet used as the velocity‑field network *vθ* and the straight‑flow network *uδ*.  
    - `in_channels: int` – Number of input channels (3 for RGB).  
    - `out_channels: int` – Number of output channels (3).  
    - `base_channels: int` – Base channel width (128).  
    - `depth: int` – UNet depth (2).  
    - `time_emb_dim: int` – Dimensionality of sinusoidal time embedding (16).  
    - `attention_heads: int` – Reserved for future attention use (4).  
  - `training:` – All training‑related hyper‑parameters.  
    - `batch_size_velocity: int` – Mini‑batch size for training *vθ* (128).  
    - `batch_size_time: int` – Mini‑batch size for training the time‑predictor *tϕ* (128).  
    - `batch_size_nsf: int` – Mini‑batch size for training the straight‑flow network *uδ* (128).  
    - `lr_velocity: float` – Learning rate for *vθ* (1e‑4).  
    - `lr_time_predictor: float` – Learning rate for *tϕ* (1e‑4).  
    - `lr_nsf: float` – Learning rate for *uδ* (1e‑4).  
    - `betas: [float, float]` – Adam optimizer betas (0.9, 0.999).  
    - `weight_decay: float` – Adam weight decay (0.0).  
    - `total_iters_velocity: int` – Training iterations for *vθ* (20000).  
    - `total_iters_time: int` – Training iterations for *tϕ* (40000).  
    - `total_iters_nsf: int` – Training iterations for *uδ* (20000).  
    - `blur: float` – Sinkhorn blur parameter (1.0).  
    - `scaling: float` – Sinkhorn scaling parameter (0.85).  
    - `eta_nsfg: float` – Euler step size for the NSGF phase (0.2).  
    - `T_max: int` – Maximum number of NSGF Euler steps (≤5, set to 5).  
    - `num_pool_batches: int` – Number of trajectory‑pool batches (2500).  
    - `omega_nsf: float` – Euler step size for the NSF refinement phase (0.02).  
    - `logging:` – Paths for runtime artefacts.  
      - `log_dir: str` – Directory for TensorBoard logs (`logs/nsfgpp_cifar`).  
      - `checkpoint_dir: str` – Directory for model checkpoints (`checkpoints/nsfgpp_cifar`).

**Internal Dependencies**  
- None (pure data file).

**External Dependencies** (files that read this config)  
- `src/training/nsfgpp_trainer.py` – Parses `model` and `training` sections to build UNet, optimizers, and training loops.  
- `src/inference/nsfgpp_infer.py` – Uses the same configuration for inference (Euler steps, `eta_nsfg`, `omega_nsf`, `T_max`).  
- Experiment scripts (`experiments/cifar10.py`, `experiments/mnist.py`) – Load this YAML via `yaml.safe_load` to supply hyper‑parameters to the trainer/inference pipelines.

**Implementation Notes**  
- **Architecture decisions**: Mirrors the paper’s CIFAR‑10 UNet settings (base channels = 128, depth = 2, time‑embedding dim = 16).  
- **Training schedule**: Separate iteration counts for each component reflect the two‑phase NSGF++ algorithm (velocity field, phase‑transition predictor, straight‑flow network).  
- **Sinkhorn parameters**: Chosen to balance OT regularisation and computational stability for 32 × 32 RGB images.  
- **Logging**: Structured directories enable easy TensorBoard visualisation and checkpoint restoration.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File configs/nsfgpp_mnist.yaml; ROUND 19 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:44:22
**File Implemented**: configs/nsfgpp_mnist.yaml

**Core Purpose**  
- Provides the configuration settings for training and evaluating the NSGF++ model on the MNIST dataset, including model architecture, training hyper‑parameters, Sinkhorn parameters, Euler integration settings, logging paths, and random seed.

**Public Interface**  
- *YAML configuration file* (no executable code).  
  - Top‑level keys: `seed`, `model`, `training`, `logging`.  
  - `model` fields: `in_channels`, `out_channels`, `base_channels`, `depth`, `time_emb_dim`, `attention_heads`.  
  - `training` fields: batch sizes, learning rates, optimizer betas, weight decay, total iteration counts, Sinkhorn `blur`/`scaling`, Euler step `eta_nsfg`, `T_max`, `num_pool_batches`, NSF step size `omega_nsf`.  
  - `logging` fields: `log_dir`, `checkpoint_dir`.

**Internal Dependencies**  
- None (pure data file).

**External Dependencies**  
- Expected to be read by:  
  - `src/training/nsfgpp_trainer.py` – parses model and training hyper‑parameters.  
  - `src/inference/nsfgpp_infer.py` – uses `eta_nsfg`, `T_max`, `omega_nsf` for inference.  
  - Experiment scripts (e.g., `experiments/mnist.py`) – loads the YAML to configure runs.

**Implementation Notes**  
- Architecture decisions captured directly in the config (UNet with 1 depth, 32 base channels, 16‑dimensional sinusoidal time embedding).  
- All hyper‑parameters mirror those described in the reproduction plan, ensuring consistency across training, NSGF phase, and NSF refinement.  
- Logging and checkpoint directories are set to separate sub‑folders under `logs/` and `checkpoints/` for reproducibility.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File experiments/metrics.py; ROUND 21 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:44:30
**File Implemented**: experiments/metrics.py

**Core Purpose**  
- Provides utility functions to compute image‑generation quality metrics (Frechet Inception Distance and Inception Score) used for evaluating NSGF/NSGF++ models on image datasets.

**Public Interface**  
- **Function `compute_fid(real_images: torch.Tensor, fake_images: torch.Tensor, device: str = "cpu") → float`**  
  - Computes the Frechet Inception Distance between a set of real images and generated images.  
  - Expects inputs in the range `[-1, 1]`; internally rescales to `[0, 1]` for the Inception model.  

- **Function `compute_is(fake_images: torch.Tensor, device: str = "cpu") → float`**  
  - Computes the Inception Score for a batch of generated images.  
  - Expects inputs in the range `[-1, 1]`; internally rescales to `[0, 1]`.

**Internal Dependencies**  
- `torch` – tensor handling and device placement.  
- `torchmetrics.image.fid.FrechetInceptionDistance` – metric implementation for FID.  
- `torchmetrics.image.inception.InceptionScore` – metric implementation for IS.

**External Dependencies** (files that are expected to import this module)  
- Likely imported by the experiment scripts `experiments/mnist.py` and `experiments/cifar10.py` to evaluate generated samples.  
- May also be used by any custom evaluation notebooks or CI scripts that need quantitative image quality scores.

**Implementation Notes**  
- Both metrics require inputs in `[0, 1]`; the functions perform a simple linear transformation `(x + 1) / 2` to meet this requirement.  
- `FrechetInceptionDistance` is instantiated with `feature=2048` (the default Inception‑v3 feature dimension) and moved to the requested device.  
- `InceptionScore` returns a tuple `(mean, std)`; only the mean score is returned to keep the API simple.  
- No class or state is kept; the functions are pure and can be called repeatedly with different tensors.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File experiments/mnist.py; ROUND 23 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:44:37
**File Implemented**: experiments/mnist.py

**Core Purpose**  
Runs the full NSGF++ pipeline on the MNIST dataset: prepares data, builds and trains the velocity‑field, time‑predictor, and straight‑flow networks, performs inference to generate samples, and evaluates them with FID and Inception Score.

**Public Interface**  
- **Function `get_data_loaders(batch_size: int, device: torch.device)`** → `torch.utils.data.DataLoader`  
  Creates a DataLoader for the MNIST training split with standard tensor‑to‑[-1, 1] normalization.  
- **Function `source_sampler(batch_size: int, in_channels: int, height: int, width: int, device: torch.device)`** → `torch.Tensor`  
  Returns uniform noise in `[-1, 1]` of shape `(batch_size, in_channels, height, width)`.  
- **Function `main()`** → `None`  
  Entry‑point script: parses CLI arguments, loads a YAML config, sets random seed, instantiates the three UNet‑based models, creates data samplers, runs the three training phases via `NSFGPPTrainer`, generates samples with `nsfgpp_generate`, computes FID/IS via `experiments.metrics`, saves results and prints a summary.

**Internal Dependencies**  
- `src.utils` → `set_seed`, `get_logger` (seed handling & optional logging)  
- `src.models` → `UNetVelocity`, `UNetStraightFlow`, `TimePredictor` (model definitions)  
- `src.training` → `NSFGPPTrainer` (training orchestration for velocity, time‑predictor, NSF)  
- `src.inference` → `nsfgpp_generate` (inference routine for NSGF++)  
- `experiments.metrics` → `compute_fid`, `compute_is` (evaluation utilities)  
- External packages:  
  - `argparse`, `os`, `yaml` – CLI & config handling  
  - `torch`, `torch.utils.data.DataLoader` – tensor ops & data loading  
  - `torchvision.datasets`, `torchvision.transforms` – MNIST dataset & preprocessing

**External Dependencies** (files that may import this module)  
- Typically invoked as a script (`python experiments/mnist.py …`). No other project modules import symbols from this file; it serves as a top‑level experiment driver.

**Implementation Notes**  
- **Configuration‑driven**: All hyper‑parameters (batch sizes, learning rates, Sinkhorn blur/scaling, Euler step sizes, number of pool batches, etc.) are read from a YAML file (`configs/nsfgpp_mnist.yaml`).  
- **Model instantiation** mirrors the paper’s UNet architecture for MNIST (single‑channel input, base channels = 32, depth = 1, sinusoidal time embedding).  
- **Data sampling**: `target_sampler` lazily pulls a batch from the MNIST DataLoader; `src_sampler` generates uniform noise matching image dimensions.  
- **Training flow**: Sequential calls to `trainer.train_velocity()`, `trainer.train_time_predictor()`, `trainer.train_nsf()`, then `trainer.close()` to finalize logging/checkpointing.  
- **Inference**: `nsfgpp_generate` runs the two‑phase NSGF++ generation (NSGF steps → predicted transition time → NSF refinement).  
- **Evaluation**: Collects exactly 10 k real MNIST images, computes FID via a pretrained Inception‑style network (wrapped in `metrics.compute_fid`) and IS via `metrics.compute_is`. Results are saved under `results/figures/`.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File experiments/cifar10.py; ROUND 24 
================================================================================

# Code Implementation Summary
**Generated**: 2025-11-24 14:45:44
**File Implemented**: experiments/cifar10.py

**Core Purpose**  
Implements the full NSGF++ pipeline for CIFAR‑10: data loading, model creation, training of the velocity field, time‑predictor, and straight‑flow networks, inference to generate samples, and evaluation of FID and Inception Score.

**Public Interface**  
- **Function `get_data_loader(batch_size: int, device: torch.device) -> DataLoader`**  
  Creates a `DataLoader` that yields normalized CIFAR‑10 training images.  
- **Function `source_sampler(batch_size: int, in_channels: int, height: int, width: int, device: torch.device) -> torch.Tensor`**  
  Returns uniform‑noise tensors in `[-1, 1]` matching the image shape.  
- **Function `main()`** (script entry point)  
  Parses arguments, loads config, sets seeds, builds logger, instantiates models (`UNetVelocity`, `TimePredictor`, `UNetStraightFlow`), prepares data samplers, runs the three training phases via `NSFGPPTrainer`, performs NSGF++ inference with `nsfgpp_generate`, computes FID/IS via `experiments.metrics`, saves generated samples and metrics, and logs progress.

**Internal Dependencies**  
- **Standard library / third‑party**  
  - `argparse`, `os`, `yaml`, `torch`, `torch.utils.data.DataLoader`, `torchvision.datasets`, `torchvision.transforms` – for CLI, file handling, configuration, tensor ops, and dataset loading.  
- **Project modules**  
  - `src.utils` → `set_seed`, `get_logger` – seed initialization and TensorBoard‑style logger.  
  - `src.models` → `UNetVelocity`, `UNetStraightFlow`, `TimePredictor` – neural network architectures used in NSGF++.  
  - `src.training` → `NSFGPPTrainer` – trainer that encapsulates the three learning phases.  
  - `src.inference` → `nsfgpp_generate` – inference routine implementing Algorithm 4 (NSGF++).  
  - `experiments.metrics` → `compute_fid`, `compute_is` – metric utilities for evaluation.

**External Dependencies** (files that may import this script)  
- This file is primarily a stand‑alone experiment script; it is not imported elsewhere. Other scripts may invoke it via the command line (`python experiments/cifar10.py …`).

**Implementation Notes**  
- **Configuration‑driven**: All hyper‑parameters (model sizes, learning rates, Sinkhorn blur/scaling, training iteration counts, etc.) are read from a YAML config (`configs/nsfgpp_cifar.yaml`).  
- **Logging**: Uses a custom logger (`get_logger`) that records the config and training progress; logs are written to a directory defined in the config (`logs/cifar10`).  
- **Data sampling**:  
  - `target_sampler` lazily iterates over the training `DataLoader`, automatically resetting when exhausted and handling batch‑size mismatches by repeating samples.  
  - `src_sampler` is a lambda wrapping `source_sampler` to produce uniform noise matching CIFAR‑10 dimensions.  
- **Training orchestration**: Calls three distinct methods of `NSFGPPTrainer` (`train_velocity`, `train_time_predictor`, `train_nsf`) sequentially, each with its own optimizer and iteration budget.  
- **Inference**: `nsfgpp_generate` runs the NSGF phase (Euler steps with learned velocity), predicts per‑sample transition times with the time‑predictor, and refines samples using the straight‑flow network.  
- **Evaluation**: Generates 10 k samples, loads the CIFAR‑10 test set, and computes FID and Inception Score via the `metrics` module. Results and generated tensors are saved under `results/figures`.

---
*Auto-generated by Memory Agent*


