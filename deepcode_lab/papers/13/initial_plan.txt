```yaml
complete_reproduction_plan:
  paper_info:
    title: "Neural Sinkhorn Gradient Flow"
    core_contribution: >
      Introduces NSGF, a neural network that directly parametrizes the
      time‑varying velocity field of the Sinkhorn Wasserstein gradient
      flow, plus a two‑phase variant (NSGF++) that first follows the
      Sinkhorn flow for ≤5 integration steps and then refines samples
      with a learned straight‑line flow.  The approach needs only source
      and target samples, avoids kernel‑based divergences, and enjoys a
      mean‑field convergence guarantee.

  # -----------------------------------------------------------------
  # SECTION 1 – File Structure (≈900 characters)
  # -----------------------------------------------------------------
  file_structure: |
    ├─ data/                         # synthetic 2‑D generators, MNIST, CIFAR‑10
    │   ├─ synth/                    # functions to sample 8‑gaussians, moons, etc.
    │   └─ raw/                      # raw .npz files for MNIST/CIFAR‑10
    ├─ src/
    │   ├─ __init__.py
    │   ├─ utils.py                 # logging, seed fixing, metric wrappers
    │   ├─ sinkhorn.py              # wrapper around GeomLoss Sinkhorn & gradient extraction
    │   ├─ models/
    │   │   ├─ mlp.py               # 3‑layer MLP for 2‑D experiments (v_θ)
    │   │   ├─ unet.py              # UNet backbone for image‑scale v_θ and u_δ
    │   │   ├─ time_predictor.py    # 4‑layer CNN regressor φ
    │   │   └─ nsf.py               # straight‑flow model u_δ (reuses UNet)
    │   ├─ training/
    │   │   ├─ trajectory_pool.py   # experience‑replay buffer implementation
    │   │   ├─ nsfg_train.py        # Algorithm 1: build pool + velocity‑field regression
    │   │   ├─ time_pred_train.py   # Algorithm 3 – phase‑transition predictor
    │   │   └─ nsf_train.py         # Algorithm 3 – NSF regression
    │   ├─ inference/
    │   │   ├─ nsfg_infer.py        # Algorithm 2 – plain NSGF inference
    │   │   └─ nsfgpp_infer.py      # Algorithm 4 – NSGF++ inference
    │   └─ experiments/
    │       ├─ synth_exp.py         # reproduces Table 1 (2‑D)
    │       ├─ mnist_exp.py         # reproduces Table 3
    │       └─ cifar_exp.py         # reproduces Table 2
    ├─ notebooks/                    # visualisation of trajectories & samples
    ├─ config/
    │   ├─ base.yaml                # default hyper‑parameters
    │   ├─ mnist.yaml
    │   └─ cifar.yaml
    ├─ requirements.txt
    ├─ README.md
    └─ run_all.sh                    # orchestrates data download, training, evaluation

  # -----------------------------------------------------------------
  # SECTION 2 – Implementation Components (≈3500 characters)
  # -----------------------------------------------------------------
  implementation_components: |
    ### Core Mathematical Objects
    - **Sinkhorn Divergence S_ε(μ,ν)**
      S_ε(μ,ν) = W_ε(μ,ν) – ½[W_ε(μ,μ) + W_ε(ν,ν)]   (Eq. 5)
      where W_ε is the entropy‑regularised 2‑Wasserstein distance
      (Eq. 2).  The paper uses ε>0; we set ε=0.1 (common default).
    - **W_ε‑Potentials (f_{μ,ν}, g_{μ,ν})**
      Obtained by solving the dual of (2) via Sinkhorn iterations.
      They satisfy:
        f = A(g,ν),   g = A(f,μ)                           (Eq. 18)
      with A defined in Lemma 2.  Gradients ∇f give transport
      directions.
    - **Velocity Field of the Sinkhorn Gradient Flow**
        v_Fε^μt(x) = ∇f_{μt,μ*}(x) – ∇f_{μt,μt}(x)       (Eq. 13)
      This field guarantees the descent property (Prop. 1, Eq. 11).

    ### Algorithms (full pseudocode given in the paper)
    1. **Algorithm 1 – Velocity‑field Matching (NSGF training)**
       - Build a trajectory pool:
         * Sample minibatch {X̃_i^0}∼μ₀, {Y_i}∼μ*.
         * For t=0…T:
           - Compute potentials f_{μ̃_t,μ̃_t} and f_{μ̃_t,μ*}
             on the current particle set X̃_i^t using `sinkhorn.potentials`.
           - Empirical velocity: ˆv_t(X̃_i^t)=∇f_{μ̃_t,μ̃_t} – ∇f_{μ̃_t,μ*}.
           - Euler update: X̃_i^{t+1}=X̃_i^t + η·ˆv_t.
           - Store (X̃_i^t, t, ˆv_t) in the replay buffer.
       - Regression phase:
         * Sample a minibatch from the pool.
         * Minimise L(θ)=‖v_θ(x,t) – ˆv‖²₂ (Eq. 12) with Adam.
       - Output: θ defining v_θ(x,t).

    2. **Algorithm 2 – Inference with Learned Velocity (NSGF)**
       - Initialise particles from μ₀.
       - For t=0…T−1: X̃^{t+1}=X̃^t + η·v_θ(X̃^t,t).
       - Return X̃^T.

    3. **Algorithm 3 – NSGF++ Training (Three sub‑blocks)**
       - *Phase 1*: Same as Algorithm 1 (NSGF) but with T≤5.
       - *Phase 2*: Train time‑predictor φ(x) (small CNN) by regression:
           L(φ)=E_{t∈U(0,1), X∼P_t}[(t – φ(X))²]   (Eq. loss for φ).
           Here X = (1−t)X₀ + tY, X₀∼μ₀, Y∼μ*.
       - *Phase 3*: Train Neural Straight Flow u_δ(t,X) (UNet) to predict displacement:
           L_NS​F(δ)=E_{t,X}[‖u_δ(t,X) – (Y – X₀)‖²]   (Eq. loss for NSF).
       - Output: θ, φ, δ.

    4. **Algorithm 4 – NSGF++ Inference**
       - Run NSGF phase (≤5 steps) using v_θ.
       - Predict transition time τ̂_i = φ(X̃_i^T).
       - Compute remaining integration steps: N̂ = (1−τ̂_i)/ω.
       - Integrate with explicit Euler using u_δ(t,·):
           X̂_i ← ODEsolver( X̃_i^T, u_δ, τ̂_i, 1, N̂ ).
       - Return refined samples X̂_i.

    ### Neural Architectures
    - **v_θ (velocity field)**
      * 2‑D: MLP (3 hidden layers, 256 units, ReLU). Input: concat[x, t].
      * Image‑scale: UNet (depth 2, channel 128 for CIFAR‑10, 32 for MNIST,
        channel‑multipliers [1,2,2,2] or [1,2,2]; time embedded via sinusoidal
        positional encoding added as a broadcast channel; optional multi‑head
        self‑attention at 16×16 resolution).
    - **φ (phase‑transition predictor)**
      * 4‑layer CNN: Conv(3→32)→ReLU→AvgPool; Conv(32→64)→ReLU→AvgPool;
        Conv(64→128)→ReLU→AvgPool; Conv(128→256)→ReLU→AvgPool; flatten,
        FC→Sigmoid for output in [0,1]. Designed for CIFAR‑10 (3×32×32) and
        MNIST (1×28×28).
    - **u_δ (Neural Straight Flow)**
      * Same UNet as v_θ but output is a displacement vector of shape
        (C, H, W). Trained on linear‑interpolation pairs.

    ### Supporting Modules
    - **sinkhorn.py**
      Wraps `SamplesLoss("sinkhorn", blur=…, scaling=…, cost="sqeuclidean", backend="auto")`
      from GeomLoss. Provides:
        * `potentials(x, y)` → f, g tensors.
        * `grad_f(x, y)` → ∇f via `torch.autograd.grad`.
    - **trajectory_pool.py**
      Implements a circular replay buffer storing tensors
      `(x, t, v̂)`. Supports on‑disk sharding for large pools (≈20‑45 GB).
    - **utils.py**
      Seed handling, metric wrappers (2‑Wasserstein via POT,
      FID via TorchMetrics, IS via pretrained Inception‑v3), progress
      bars, checkpointing.

    ### Hyper‑parameters (as reported / reasonable defaults)
    | Component | Value |
    |-----------|-------|
    | ε (Sinkhorn blur) | 0.5, 1.0, 2.0 (chosen per dataset; typical 1.0) |
    | scaling | 0.80‑0.95 (tuned per dataset) |
    | η (Euler step) | 0.1 (2‑D), 0.05 (MNIST/CIFAR‑10) |
    | ω (NSF step) | same as η |
    | Batch size n | 256 (MNIST), 128 (CIFAR‑10), 256 (synthetic) |
    | Trajectory pool size | 1500 batches (MNIST) → ≈20 GB; 2500 batches (CIFAR‑10) → ≈45 GB |
    | Optimiser | Adam (lr = 1e‑4, β₁=0.9, β₂=0.999, no weight decay) |
    | Training steps | Velocity field: 100 k optimizer steps; φ: 40 k steps; NSF: 100 k steps (early stopping on validation loss). |
    | Random seed | 42 (fixed across all runs). |

  # -----------------------------------------------------------------
  # SECTION 3 – Validation & Evaluation (≈2200 characters)
  # -----------------------------------------------------------------
  validation_approach: |
    **Synthetic 2‑D Benchmarks**
    - Datasets: 8‑gaussians, moons, checkerboard, S‑curve (as in
      Tong et al. 2023). 1024 test samples per distribution.
    - Procedure:
      1. Train NSGF with T∈{10,100} steps, batch = 256, η = 0.1.
      2. After training, generate 10 k particles using Algorithm 2.
      3. Compute exact 2‑Wasserstein distance via POT’s
         `emd2` (ground‑truth) and report values.
    - Expected results (Table 1): NSGF ≈0.285 (8‑gaussians, 10 steps)
      and ≈0.144 (8‑gaussians, 100 steps); all other baselines are
      larger.  Differences within ±0.02 indicate successful reproduction.

    **MNIST Generation**
    - Data: 60 k training, 10 k test, normalized to [‑1,1].
    - Train NSGF++:
      * NSGF phase: T=5, η=0.05, UNet (channels = 32, depth = 1).
      * Train φ and NSF as in Algorithm 3.
    - Sample 10 k images using Algorithm 4.
    - Metrics:
        * FID (using pretrained Inception‑v3) – target ≈3.8.
        * IS – target ≥8.5 (the paper reports only IS for CIFAR‑10; for MNIST we report
          IS for completeness, expecting ~9.0).
        * NFE – total number of ODE steps should be ≤60 (≈5 NSGF + ≈55 NSF steps).
    - Acceptance: FID ≤4.0 and NFE ≤60 matches Table 3.

    **CIFAR‑10 Generation**
    - Data: 50 k train, 10 k test, same normalization.
    - NSGF++ configuration:
      * UNet (channels = 128, depth = 2, heads=4, attention‑res=16).
      * Blur = 1.0, scaling = 0.85, η = 0.05, T=5.
    - Generate 10 k samples, compute:
        * FID – target ≤5.5 (paper reports 5.55).
        * IS – target ≥8.5 (paper reports 8.86).
        * NFE – ≤59 (paper reports 59).
    - Visual validation: reproduce Figure 5 (first row = after 5 NSGF steps,
      second row = final NSF refinement).  Trajectories should mimic Figure 1
      (fast approach to target manifold).

    **Ablation Checks**
    - Remove phase‑transition predictor (set τ̂=0.5) → expect FID increase >10% .
    - Remove NSF (output NSGF particles directly) → higher FID, confirming refinement benefit.
    - Increase T beyond 5 in NSGF++ (e.g., T=10) → similar quality but higher NFE, confirming trade‑off.

    **Success Criteria**
    - All quantitative metrics lie within ±5 % of reported values.
    - Qualitative visualizations resemble those in the paper (trajectory colour progression, sample grids).
    - Training logs show decreasing velocity‑field loss (L2) to <1e‑3, φ‑loss <1e‑4, NSF loss <1e‑3.

  # -----------------------------------------------------------------
  # SECTION 4 – Environment & Dependencies (≈900 characters)
  # -----------------------------------------------------------------
  environment_setup: |
    - **OS / Hardware**: Linux (Ubuntu 20.04) with a single NVIDIA RTX‑3090 (24 GB VRAM) or equivalent.
    - **Python**: >=3.9 (tested with 3.10).
    - **Core libraries**:
        * torch==1.13.1+cu117
        * torchvision==0.14.1
        * geomloss==0.2.6 (provides Sinkhorn loss & potentials)
        * numpy==1.24.2
        * tqdm==4.65.0
        * matplotlib==3.7.1
        * scikit-learn==1.2.2 (for 2‑Wasserstein via POT)
        * pot (POT) ==0.9.0
        * torchmetrics==0.11.4 (FID/IS)
        * wandb (optional, for experiment tracking)
    - **CUDA**: compatible with PyTorch build (CUDA 11.7 recommended).
    - **Installation**: `pip install -r requirements.txt`.  Verify GeomLoss GPU support by running `python -c "import geomloss; print(geomloss.__version__)"`.
    - **Reproducibility**: `torch.manual_seed(42); np.random.seed(42); random.seed(42); torch.backends.cudnn.deterministic=True`.

  # -----------------------------------------------------------------
  # SECTION 5 – Implementation Strategy (≈1700 characters)
  # -----------------------------------------------------------------
  implementation_strategy: |
    **Phase 0 – Project Bootstrap**
    1. Clone the repository skeleton, create virtualenv, install deps.
    2. Download MNIST and CIFAR‑10 via `torchvision.datasets` into `data/raw/`.
    3. Implement synthetic data generators (Section 3) and verify sampling.

    **Phase 1 – Sinkhorn Core**
    - Implement `sinkhorn.py`:
      * Use `SamplesLoss("sinkhorn", blur=..., scaling=..., cost="sqeuclidean")`.
      * Provide `potentials(x, y)` returning f, g tensors; compute gradients with
        `torch.autograd.grad(f.sum(), x, create_graph=True)`.
    - Write unit tests: compare potentials on a tiny 2‑point batch against
      analytical solution for ε→0.

    **Phase 2 – Velocity‑field Learning (2‑D)**
    - Build MLP (`mlp.py`) with the specified architecture.
    - Implement `trajectory_pool.py` (in‑memory buffer first, later persist to disk).
    - Code `nsfg_train.py` following Algorithm 1:
        * Loop `while building:` for a fixed number of trajectories (e.g., 1500).
        * Store `(x, t, v̂)` in the pool.
        * Switch to regression loop, sampling uniformly from pool each minibatch.
    - Validate by reproducing Table 1 (10‑step case).  Plot trajectories to ensure
      they match Figure 1.

    **Phase 3 – Image‑scale NSGF**
    - Implement UNet (`unet.py`) as described; embed scalar time via sinusoidal encoding,
      broadcast across spatial dimensions.
    - Replace MLP with UNet in `nsfg_train.py`, adjust batch size to 128/256.
    - Train on MNIST first (simpler) then CIFAR‑10.
    - Verify velocity‑field loss convergence; generate samples using `nsfg_infer.py`
      and compute 2‑Wasserstein (for MNIST) and FID/IS (for both datasets).

    **Phase 4 – Two‑Phase Extension (NSGF++)**
    - Implement `time_predictor.py` (4‑layer CNN) and `nsf.py` (UNet head for displacement).
    - Write `time_pred_train.py` and `nsf_train.py` following Algorithm 3.
    - Integrate all three sub‑models in a master script (`nsfgpp_train.py`):
        * First call `nsfg_train` to obtain v_θ and the trajectory pool.
        * Then train φ on linear interpolations sampled from μ₀ and μ*.
        * Finally train u_δ using the same interpolations but with target displacement.
    - Ensure checkpoints for each sub‑model are saved.

    **Phase 5 – Inference & Evaluation**
    - Implement `nsfgpp_infer.py` (Algorithm 4):
        * Run NSGF phase → obtain intermediate particles.
        * Predict τ̂ with φ, compute remaining steps, integrate u_δ.
        * Save generated images; compute FID/IS via TorchMetrics.
    - Run `synth_exp.py`, `mnist_exp.py`, `cifar_exp.py` to produce Tables 1‑3.
    - Generate visualisation notebooks replicating Figures 1‑7.

    **Phase 6 – Ablations & Documentation**
    - Conduct the three ablation studies (no φ, no NSF, larger T) and log metric changes.
    - Populate `README.md` with clear entry‑point commands, hyper‑parameter tables, and expected runtime estimates.
    - Provide a `run_all.sh` script that sequentially executes data preparation, training, and evaluation for the three experimental settings.

    **Risk Mitigation & Defaults**
    - If GeomLoss’s default ε leads to unstable potentials, fall back to ε=0.05 and increase number of Sinkhorn iterations (e.g., `max_iter=300`).
    - Should memory overflow when storing the trajectory pool, enable on‑disk sharding with HDF5 via `h5py`.
    - For reproducibility, lock all random seeds at the start of each script and log the exact versions of all packages in a `versions.txt` file.

```